<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN"  
  "http://www.w3.org/TR/html4/loose.dtd">  
<html > 
<head><title>Introduction to Big Data</title> 
<meta http-equiv="Content-Type" content="text/html; charset=iso-8859-1"> 
<meta name="generator" content="TeX4ht (http://www.tug.org/tex4ht/)"> 
<meta name="originator" content="TeX4ht (http://www.tug.org/tex4ht/)"> 
<!-- html --> 
<meta name="src" content="bigdata.tex"> 
<meta name="date" content="2016-05-10 11:14:00"> 
<link rel="stylesheet" type="text/css" href="bigdata.css"> 
<!-- Google tag (gtag.js) -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-57GD08QCML"></script>
<script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'G-57GD08QCML');
</script>
<style>
    body {
        font-family: "ff-meta-serif-web-pro", Georgia, serif;
        text-rendering: optimizeLegibility;
        font-size: 17px;
        margin: 0 1.0in 0 1.0in;
        line-height: 2.0em;
    }
</style>
</head>
<body>
                                                          
<p><a href="http://github.com/haifengl/bigdata"><img style="position: fixed; top: 0; right: 0; border: 0" src=https://github.blog/wp-content/uploads/2008/12/forkme_right_orange_ff7600.png alt="Fork me on GitHub"></a></p>


<!--l. 65--><p class="indent" >
                                                          
                                                          
   <div class="maketitle">
                                                          
                                                          
                                                          
                                                          
                                                          
                                                          

<h2 class="titleHead">Introduction to Big Data</h2>
       <div class="author" ><span 
class="cmr-12x-x-120">Haifeng Li</span>
<br />  <span 
  class="cmr-12x-x-120"><a href="http://haifengl.wordpress.com">http://haifengl.wordpress.com</a></span></div><br />
<div class="date" ><span 
class="cmr-12x-x-120">May 10, 2016</span></div>
                                                          
                                                          
   </div>
                                                          
                                                          
   <h2 class="likechapterHead"><a 
 id="x1-1000"></a>Contents</h2> <div class="tableofcontents">
   <span class="chapterToc" >1 <a 
href="#x1-20001" id="QQ2-1-2">Introduction</a></span>
<br />   &#x00A0;<span class="sectionToc" >1.1 <a 
href="#x1-30001.1" id="QQ2-1-3">What&#8217;s Big Data?</a></span>
<br />   &#x00A0;<span class="sectionToc" >1.2 <a 
href="#x1-40001.2" id="QQ2-1-4">Business Use Cases</a></span>
<br />   &#x00A0;&#x00A0;<span class="subsectionToc" >1.2.1 <a 
href="#x1-50001.2.1" id="QQ2-1-5">CRM</a></span>
<br />   &#x00A0;&#x00A0;<span class="subsectionToc" >1.2.2 <a 
href="#x1-60001.2.2" id="QQ2-1-6">HCM</a></span>
<br />   &#x00A0;&#x00A0;<span class="subsectionToc" >1.2.3 <a 
href="#x1-70001.2.3" id="QQ2-1-7">IoT</a></span>
<br />   &#x00A0;&#x00A0;<span class="subsectionToc" >1.2.4 <a 
href="#x1-80001.2.4" id="QQ2-1-8">Healthcare</a></span>
<br />   &#x00A0;<span class="sectionToc" >1.3 <a 
href="#x1-90001.3" id="QQ2-1-9">Audience</a></span>
<br />   &#x00A0;<span class="sectionToc" >1.4 <a 
href="#x1-100001.4" id="QQ2-1-10">Roadmap</a></span>
<br />   <span class="chapterToc" >2 <a 
href="#x1-110002" id="QQ2-1-11">Data Management</a></span>
<br />   <span class="chapterToc" >3 <a 
href="#x1-120003" id="QQ2-1-13">Hadoop</a></span>
<br />   &#x00A0;<span class="sectionToc" >3.1 <a 
href="#x1-130003.1" id="QQ2-1-15">HDFS</a></span>
<br />   &#x00A0;&#x00A0;<span class="subsectionToc" >3.1.1 <a 
href="#x1-140003.1.1" id="QQ2-1-16">Assumptions</a></span>
<br />   &#x00A0;&#x00A0;<span class="subsectionToc" >3.1.2 <a 
href="#x1-150003.1.2" id="QQ2-1-17">Architecture</a></span>
<br />   &#x00A0;&#x00A0;<span class="subsectionToc" >3.1.3 <a 
href="#x1-160003.1.3" id="QQ2-1-19">Control and Data Flow</a></span>
<br />   &#x00A0;&#x00A0;<span class="subsectionToc" >3.1.4 <a 
href="#x1-190003.1.4" id="QQ2-1-22">The Small Files Problem</a></span>
<br />   &#x00A0;&#x00A0;<span class="subsectionToc" >3.1.5 <a 
href="#x1-200003.1.5" id="QQ2-1-23">HDFS Federation</a></span>
<br />   &#x00A0;&#x00A0;<span class="subsectionToc" >3.1.6 <a 
href="#x1-210003.1.6" id="QQ2-1-24">Java API</a></span>
<br />   &#x00A0;&#x00A0;<span class="subsectionToc" >3.1.7 <a 
href="#x1-220003.1.7" id="QQ2-1-25">Data Ingestion</a></span>
<br />   &#x00A0;<span class="sectionToc" >3.2 <a 
href="#x1-260003.2" id="QQ2-1-29">MapReduce</a></span>
<br />   &#x00A0;&#x00A0;<span class="subsectionToc" >3.2.1 <a 
href="#x1-270003.2.1" id="QQ2-1-30">Overview</a></span>
<br />   &#x00A0;&#x00A0;<span class="subsectionToc" >3.2.2 <a 
href="#x1-280003.2.2" id="QQ2-1-31">Data Flow</a></span>
<br />   &#x00A0;&#x00A0;<span class="subsectionToc" >3.2.3 <a 
href="#x1-290003.2.3" id="QQ2-1-33">Secondary Sorting</a></span>
                                                          
                                                          
<br />   &#x00A0;&#x00A0;<span class="subsectionToc" >3.2.4 <a 
href="#x1-300003.2.4" id="QQ2-1-34">Examples</a></span>
<br />   &#x00A0;&#x00A0;<span class="subsectionToc" >3.2.5 <a 
href="#x1-360003.2.5" id="QQ2-1-40">Shortcomings</a></span>
<br />   &#x00A0;<span class="sectionToc" >3.3 <a 
href="#x1-400003.3" id="QQ2-1-44">Tez</a></span>
<br />   &#x00A0;<span class="sectionToc" >3.4 <a 
href="#x1-410003.4" id="QQ2-1-46">YARN</a></span>
<br />   <span class="chapterToc" >4 <a 
href="#x1-420004" id="QQ2-1-48">Spark</a></span>
<br />   &#x00A0;<span class="sectionToc" >4.1 <a 
href="#x1-430004.1" id="QQ2-1-49">RDD</a></span>
<br />   &#x00A0;<span class="sectionToc" >4.2 <a 
href="#x1-440004.2" id="QQ2-1-50">Implementation</a></span>
<br />   &#x00A0;<span class="sectionToc" >4.3 <a 
href="#x1-450004.3" id="QQ2-1-51">API</a></span>
<br />   <span class="chapterToc" >5 <a 
href="#x1-460005" id="QQ2-1-52">Analytics and Data Warehouse</a></span>
<br />   &#x00A0;<span class="sectionToc" >5.1 <a 
href="#x1-470005.1" id="QQ2-1-53">Pig</a></span>
<br />   &#x00A0;<span class="sectionToc" >5.2 <a 
href="#x1-480005.2" id="QQ2-1-54">Hive</a></span>
<br />   &#x00A0;<span class="sectionToc" >5.3 <a 
href="#x1-490005.3" id="QQ2-1-55">Impala</a></span>
<br />   &#x00A0;<span class="sectionToc" >5.4 <a 
href="#x1-500005.4" id="QQ2-1-56">Shark and Spark SQL</a></span>
<br />   <span class="chapterToc" >6 <a 
href="#x1-510006" id="QQ2-1-57">NoSQL</a></span>
<br />   &#x00A0;<span class="sectionToc" >6.1 <a 
href="#x1-520006.1" id="QQ2-1-58">The CAP Theorem</a></span>
<br />   &#x00A0;<span class="sectionToc" >6.2 <a 
href="#x1-530006.2" id="QQ2-1-59">ZooKeeper</a></span>
<br />   &#x00A0;&#x00A0;<span class="subsectionToc" >6.2.1 <a 
href="#x1-540006.2.1" id="QQ2-1-60">Data Model</a></span>
<br />   &#x00A0;&#x00A0;<span class="subsectionToc" >6.2.2 <a 
href="#x1-550006.2.2" id="QQ2-1-61">Atomic Broadcast</a></span>
<br />   &#x00A0;<span class="sectionToc" >6.3 <a 
href="#x1-560006.3" id="QQ2-1-63">HBase</a></span>
<br />   &#x00A0;&#x00A0;<span class="subsectionToc" >6.3.1 <a 
href="#x1-570006.3.1" id="QQ2-1-64">Data Model</a></span>
<br />   &#x00A0;&#x00A0;<span class="subsectionToc" >6.3.2 <a 
href="#x1-580006.3.2" id="QQ2-1-65">Storage</a></span>
<br />   &#x00A0;&#x00A0;<span class="subsectionToc" >6.3.3 <a 
href="#x1-590006.3.3" id="QQ2-1-66">Architecture</a></span>
<br />   &#x00A0;&#x00A0;<span class="subsectionToc" >6.3.4 <a 
href="#x1-650006.3.4" id="QQ2-1-73">Security</a></span>
<br />   &#x00A0;&#x00A0;<span class="subsectionToc" >6.3.5 <a 
href="#x1-670006.3.5" id="QQ2-1-75">Coprocessor</a></span>
<br />   &#x00A0;&#x00A0;<span class="subsectionToc" >6.3.6 <a 
href="#x1-680006.3.6" id="QQ2-1-76">Summary</a></span>
<br />   &#x00A0;<span class="sectionToc" >6.4 <a 
href="#x1-690006.4" id="QQ2-1-77">Riak</a></span>
<br />   &#x00A0;&#x00A0;<span class="subsectionToc" >6.4.1 <a 
href="#x1-700006.4.1" id="QQ2-1-78">Data Model</a></span>
<br />   &#x00A0;&#x00A0;<span class="subsectionToc" >6.4.2 <a 
href="#x1-710006.4.2" id="QQ2-1-79">Storage</a></span>
<br />   &#x00A0;&#x00A0;<span class="subsectionToc" >6.4.3 <a 
href="#x1-750006.4.3" id="QQ2-1-83">Architecture</a></span>
<br />   &#x00A0;&#x00A0;<span class="subsectionToc" >6.4.4 <a 
href="#x1-760006.4.4" id="QQ2-1-86">Consistency</a></span>
<br />   &#x00A0;&#x00A0;<span class="subsectionToc" >6.4.5 <a 
href="#x1-800006.4.5" id="QQ2-1-90">Summary</a></span>
                                                          
                                                          
<br />   &#x00A0;<span class="sectionToc" >6.5 <a 
href="#x1-810006.5" id="QQ2-1-91">Cassandra</a></span>
<br />   &#x00A0;&#x00A0;<span class="subsectionToc" >6.5.1 <a 
href="#x1-820006.5.1" id="QQ2-1-92">Data Model</a></span>
<br />   &#x00A0;&#x00A0;<span class="subsectionToc" >6.5.2 <a 
href="#x1-830006.5.2" id="QQ2-1-93">Storage</a></span>
<br />   &#x00A0;&#x00A0;<span class="subsectionToc" >6.5.3 <a 
href="#x1-840006.5.3" id="QQ2-1-94">Architecture</a></span>
<br />   &#x00A0;&#x00A0;<span class="subsectionToc" >6.5.4 <a 
href="#x1-850006.5.4" id="QQ2-1-95">CQL</a></span>
<br />   &#x00A0;&#x00A0;<span class="subsectionToc" >6.5.5 <a 
href="#x1-860006.5.5" id="QQ2-1-96">Consistency</a></span>
<br />   &#x00A0;&#x00A0;<span class="subsectionToc" >6.5.6 <a 
href="#x1-870006.5.6" id="QQ2-1-97">Summary</a></span>
<br />   &#x00A0;<span class="sectionToc" >6.6 <a 
href="#x1-880006.6" id="QQ2-1-98">MongoDB</a></span>
<br />   &#x00A0;&#x00A0;<span class="subsectionToc" >6.6.1 <a 
href="#x1-890006.6.1" id="QQ2-1-99">Data Model</a></span>
<br />   &#x00A0;&#x00A0;<span class="subsectionToc" >6.6.2 <a 
href="#x1-900006.6.2" id="QQ2-1-100">Storage</a></span>
<br />   &#x00A0;&#x00A0;<span class="subsectionToc" >6.6.3 <a 
href="#x1-930006.6.3" id="QQ2-1-104">Cluster Architecture</a></span>
<br />   &#x00A0;&#x00A0;<span class="subsectionToc" >6.6.4 <a 
href="#x1-940006.6.4" id="QQ2-1-105">Replic Set</a></span>
<br />   &#x00A0;&#x00A0;<span class="subsectionToc" >6.6.5 <a 
href="#x1-950006.6.5" id="QQ2-1-107">Sharding</a></span>
<br />   &#x00A0;&#x00A0;<span class="subsectionToc" >6.6.6 <a 
href="#x1-960006.6.6" id="QQ2-1-109">Summary</a></span>
   </div>
                                                          
                                                          
                                                          
                                                          
                                                          
                                                          
<!--l. 71--><p class="indent" >
                                                          
                                                          
   <h2 class="chapterHead"><span class="titlemark">Chapter&#x00A0;1</span><br /><a 
 id="x1-20001"></a>Introduction</h2> Just like Internet, Big Data<a 
 id="dx1-2001"></a> is part
of our lives today. From search, online shopping, video on
demand, to e-dating, Big Data always plays an important role
behind the scene. Some people claim that Internet of things
(IoT)<a 
 id="dx1-2002"></a> will take over big data as the most hyped technology <span class="cite">[<a 
href="#XGartner2014">35</a>]</span>.
It may become true. But IoT<a 
 id="dx1-2003"></a> cannot come alive without
big data. In this book, we will dive deeply into big data
technologies. But we need to understand what is Big Data
first.
   <h3 class="sectionHead"><span class="titlemark">1.1   </span> <a 
 id="x1-30001.1"></a>What&#8217;s Big Data?</h3>
<!--l. 78--><p class="noindent" >Gartner, and now much of the industry, use the &#8220;3Vs&#8221;<a 
 id="dx1-3001"></a> model
<span class="cite">[<a 
href="#XLaney2012">55</a>]</span> for describing big data:
      <div class="quote">
      <!--l. 80--><p class="noindent" >Big data is high volume, high velocity, and/or
      high  variety  information  assets  that  require
      new  forms  of  processing  to  enable  enhanced
      decision making, insight discovery and process
      optimization.</div>
<!--l. 82--><p class="noindent" >It is no doubt that today&#8217;s systems are processing huge amount of
data every day. For example, Facebook&#8217;s Hive data warehouse
holds 300 PB data with an incoming daily rate of about 600 TB
in April, 2014 <span class="cite">[<a 
href="#XVagateWilfong2014">78</a>]</span>! This example also shows us that big data is
fast data, too. Without high speed data generation and capture,
we won&#8217;t quickly accumulate a large amount of data to process.
According to IBM, 90% of the data in the world today
has been created over the last two years alone <span class="cite">[<a 
href="#XIBM2013">48</a>]</span>. High
variety (i.e. unstructured data) is another important aspect
                                                          
                                                          
of big data. It refers to information that either does not
have a pre-defined data model or format. Traditional data
processing systems (e.g. relational data warehouse) may
handle large volume of rigid relational data but they are not
flexible to process semi-structure or unstructured data. New
technologies have to be developed to handle data from
various sources, e.g. texts, social networks, image data,
etc.
<!--l. 84--><p class="indent" >   The 3Vs model nicely describe several major aspects of big
data. Since then, people added more Vs (e.g. Variability,
Veracity) to the list. However, do 3Vs (or 4Vs, 5Vs, &#x2026;) really
capture the core characteristics of big data? Probably not. We
are processing data in the scale of petabyte or even exabyte
today. But big is always relative, right? Although 1TB data is
not that big today, it was big and very challenging to process 20
years ago. Recall the fastest supercomputer in 1994, Fujitsu
Numerical Wind Tunnel, had the peak speed of 170 GFLOPS
<span class="cite">[<a 
href="#XTop500">76</a>]</span>. Well, a Nvidia K40 GPU in a PC has the power of 1430
GFLOPS today <span class="cite">[<a 
href="#XNvidia2014">80</a>]</span>. Besides software innovations (e.g. GFS
and MapReduce) also helped a lot to process bigger and bigger
data. With the advances of technologies, today&#8217;s big data will
quickly become small in tomorrow&#8217;s standard. The same thing
holds for &#8220;high velocity&#8221;. So high volume and high velocity are
not the core of big data movement even though they are the
driving force of technology advancement. How about &#8220;high
variety&#8221;? Many people read it as unstructured data which can
not be well handled by RDBMS. But unstructured data
have always been there no matter how they are stored,
processed, and analyzed. We do handle text, voice, images
and videos better today with the advances in NoSQL,
natural language processing, information retrieval, computer
                                                          
                                                          
vision, and pattern recognition. But it is still about the
technology advancement rather than intrinsic value of big
data.
<!--l. 86--><p class="indent" >   From the business point of view, we may understand big
data better. Although data is a valuable corporate asset, it is
just soil, not oil. Without analysis, they are pretty much
useless. But extremely valuable knowledge and insights can be
discovered from data. No matter how you call this analytic
process (data science, business intelligence, machine learning,
data mining, or information retrieval), the business goal is the
same: higher competency gained from the discovered knowledge
and insights. But wait a second. does not the idea of data
analytics exist for a long time? So what&#8217;re the real differences
between today&#8217;s &#8220;big data&#8221; analytics and traditional data
analytics? Looking back to web data analysis, the origin of
big data, we will find that big data means proactively
learning and understanding the customers, their needs,
behaviors, experience, and trends in near real-time and
24<span 
class="cmsy-10x-x-120">&#x00D7;</span>7. On the other hand, traditional data analytics is
passive/reactive, treats customers as a whole or segments
rather than individuals, and there is significant time lag.
Check out the applications of big data, a lot of them is
about
      <ul class="itemize1">
      <li class="itemize">User Experience and Behavior Analysis
      </li>
      <li class="itemize">Personalization
      </li>
      <li class="itemize">Recommendation</li></ul>
                                                          
                                                          
<!--l. 92--><p class="noindent" >which you rarely find in business intelligence applications
<span class="footnote-mark"><a 
href="bigdata2.html#fn1x2"><sup class="textsuperscript">1</sup></a></span><a 
 id="x1-3002f1"></a>.
New applications, e.g. smart grid and Internet of things, are
pushing this real-time proactive analysis forward to the whole
environment and context. Therefore, the fundamental objective
of big data is to help the organizations turn data into
actionable information for identifying new opportunities,
recognizing operational issues and problems, and better
decision-making, etc. This is the driving force for corporations
to embrace big data.
<!--l. 94--><p class="indent" >   How did this shift happen? The data have been changing.
Traditionally, our databases are just the systems of records,
which are manually input by people. In contrast, a large part of
big data is log data, which are generated by applications and
record every interaction between users and systems. Some
people call them machine generated data to emphasize the
speed of data generation and the size of data. But the truth is
that they are triggered by human actions (event is probably a
better name of these data). The Internet of things will help
us even to understand the environment and context of
user actions. The analysis on events results in a better
understanding of every single user and thus yield improved user
experience and bigger revenue, a lovely win-win for both
customers and business.
   <h3 class="sectionHead"><span class="titlemark">1.2   </span> <a 
 id="x1-40001.2"></a>Business Use Cases</h3>
                                                          
                                                          
<!--l. 98--><p class="noindent" >Big data is not just a hype but can bring great values to
business. In what follows, we will discuss some use case of big
data in different areas and industries. The list can go very long
but we will focus on several important cases to show how big
data can help solve business challenges.
<!--l. 100--><p class="noindent" >
   <h4 class="subsectionHead"><span class="titlemark">1.2.1   </span> <a 
 id="x1-50001.2.1"></a>CRM</h4>
<a 
 id="dx1-5001"></a>
<!--l. 101--><p class="noindent" >Customer relationship management (CRM) is for managing a
company&#8217;s interactions with current and future customers. By
integrating big data into a CRM solution, companies can learn
customer behavior, identify sales opportunities, analyze
customers&#8217; sentiment, and improve customer experience to
increase customer engagement and bring greater profits.
<!--l. 103--><p class="indent" >   Using big data, organizations can collect more accurate and
detailed information to gain the 360 view of customers. The
analysis of all the customers&#8217; touch points, such as browsing history
<span class="footnote-mark"><a 
href="bigdata3.html#fn2x2"><sup class="textsuperscript">2</sup></a></span><a 
 id="x1-5002f2"></a>,
social media, email, and call center, enable companies to gain a
much more complete and deeper understanding of customer
behavior &#8211; what ads attract them, why they buy, how they
shop, what they buy together, what they&#8217;ll buy next, why
they switch, how they recommend a product/service in
their social network, etc. Once actionable insights are
                                                          
                                                          
discovered, companies will more likely rise above industry
standards.
<!--l. 105--><p class="indent" >   Big data also enable comprehensive benchmarking over the
time. For example, banks, telephone service companies, Internet
service providers, pay TV companies, insurance firms, and
alarm monitoring services, often use customer attrition
analysis and customer attrition rates as one of their key
business metrics because the cost of retaining an existing
customer is far less than acquiring a new one <span class="cite">[<a 
href="#XReichheldSasser1990">68</a>]</span>. Moreover,
big data enables service providers to move from reactive
churn management to proactive customer retention with
predictive modeling before customers explicitly start the
switch.
   <h4 class="subsectionHead"><span class="titlemark">1.2.2   </span> <a 
 id="x1-60001.2.2"></a>HCM</h4>
<a 
 id="dx1-6001"></a>
<!--l. 109--><p class="noindent" >Human capital management (HCM) supposes to maximize
employee performance in service of their employer&#8217;s strategic
objectives. However, current HCM systems are mostly
bookkeeping. For example, many HCM softwares/services
provide <span class="cite">[<a 
href="#XAdpHcm">3</a>]</span>
      <ul class="itemize1">
      <li class="itemize">Enrolling or changing benefits information
      </li>
      <li class="itemize">Reporting life events such as moving or having a baby
      </li>
      <li class="itemize">Acknowledging company policies
                                                          
                                                          
      </li>
      <li class="itemize">Viewing pay statements and W-2 information
      </li>
      <li class="itemize">Changing W-4 tax information
      </li>
      <li class="itemize">Managing a 401(k) account
      </li>
      <li class="itemize">Viewing the company directory
      </li>
      <li class="itemize">Submitting requisition requests
      </li>
      <li class="itemize">Approving leave requests
      </li>
      <li class="itemize">Managing performance and goals
      </li>
      <li class="itemize">Viewing team calendars</li></ul>
<!--l. 123--><p class="noindent" >These are all important HR tasks. However, they are hardly
associated to &#8220;maximize employee performance&#8221;. Even worse,
current HCM systems are passive. Taking performance and
goals management as an example, one and his/her manager
enter the goals at the beginning of years and input the
performance evaluations and feedbacks at the end of year. So
what? If low performance happened, it has already happened
for most of the year!
                                                          
                                                          
<!--l. 125--><p class="indent" >   With big data, HCM systems can help HR practitioners and
managers to actively measure, monitor and improve employee
performance. Although it is pretty hard to measure employee
performance in real time, especially for long term projects,
studies show a clear correlation between engagement and
performance &#8211; and most importantly between improving
engagement and improving performance <span class="cite">[<a 
href="#XMacLeodClarke2012">57</a>]</span>. That is,
organizations with a highly engaged workforce significantly
outperform those without.
<!--l. 127--><p class="indent" >   Engagement analytics has been an active research area in
CRM and many technologies can be borrowed to HCM. For
example, churn analysis can be used to understand the
underlying patterns of employee turnover. With big data, HCM
systems can predict which high-performing employees are
likely to leave a company in the next year and then offers
possible actions (higher compensation and/or new job)
that might make them stay. For corporations, they simply
want to know their employees as well as they know their
customers. From this point of view, it does make a lot of
sense to connect HCM and CRM together with big data to
shorten the communication paths between inside and outside
world.
<!--l. 129--><p class="noindent" >
   <h4 class="subsectionHead"><span class="titlemark">1.2.3   </span> <a 
 id="x1-70001.2.3"></a>IoT</h4>
<a 
 id="dx1-7001"></a>
<!--l. 130--><p class="noindent" >The Internet of Things is the interconnection of uniquely
identifiable embedded computing devices within the Internet
infrastructure. IoT is representing the next big wave in the
evolution of the Internet. The combination of big data and IoT
                                                          
                                                          
is producing huge opportunities for companies in all industries.
Industries such as manufacturing, mobility and retail have
already been leveraging the data generated by billions of
devices to provide new levels of operational and business
insights.
<!--l. 132--><p class="indent" >   Industrial companies are progressing in creating financial
value by gathering and analyzing vast volumes of machine
sensor data <span class="cite">[<a 
href="#XIndustrialInternetReport2014">37</a>]</span>. Additionally, some companies are progressing
to leverage insights from machine asset data to create
efficiencies in operations and drive market advantages with
greater confidence. For example, Thames Water Utilities
Limited, the largest provider of water and wastewater services
in the UK, is using sensors, analytics and real-time data to help
the utility respond more quickly to critical situations such as
leaks or adverse weather events <span class="cite">[<a 
href="#XAccenture14SmartGrid">2</a>]</span>.
<!--l. 134--><p class="indent" >   Smart grid, an advanced application of IoT, is profoundly
changing the fundamentals of urban areas throughout the
world. Multiple cities around the world are conducting the so
called smart city trials. For example, the city of Seattle is
applying analytics to optimize energy usage by identifying
equipment and system inefficiencies, and alerting building
managers to areas of wasted energy. Elements in each room of a
building &#8211; such as lighting, temperature and the position of
window shades &#8211; can then be adjusted, depending on data
readings, to maximize efficiency <span class="cite">[<a 
href="#XAccenture13Seattle">1</a>]</span>.
<!--l. 136--><p class="noindent" >
   <h4 class="subsectionHead"><span class="titlemark">1.2.4   </span> <a 
 id="x1-80001.2.4"></a>Healthcare</h4>
<!--l. 137--><p class="noindent" >Healthcare is a big industry and contribute to a significant
part of a country&#8217;s economy (in fact 17.7% of GDP in
                                                          
                                                          
USA). Big data can improve our ability to treat illnesses,
e.g. recognizing individuals who are at risk for serious
health problems. It can also identify waste in the healthcare
system and thus lower the cost of healthcare across the
board.
<!--l. 139--><p class="indent" >   A recent exciting advance in applying big data to healthcare
is IBM Watson. IBM Watson is an artificially intelligent
computer system capable of answering questions posed
in natural language <span class="cite">[<a 
href="#XWatson2014">49</a>]</span>. Watson may work as a clinical
decision support system for medical professionals based on its
natural language, hypothesis generation, and evidence-based
learning capabilities <span class="cite">[<a 
href="#XWatson2013Healthcare">47</a>,&#x00A0;<a 
href="#XWatson2013Cancer">46</a>]</span>. When a doctor asks Watson
about symptoms and other related factors, Watson first
parses the input to identify the most important pieces of
information; then mines patient data to find facts relevant to
the patient&#8217;s medical and hereditary history; then examines
available data sources to form and test hypotheses; and
finally provides a list of individualized, confidence-scored
recommendations. The sources of data that Watson uses
for analysis can include treatment guidelines, electronic
medical record data, notes from doctors and nurses, research
materials, clinical studies, journal articles, and patient
information.
<!--l. 142--><p class="noindent" >
   <h3 class="sectionHead"><span class="titlemark">1.3   </span> <a 
 id="x1-90001.3"></a>Audience</h3>
<!--l. 144--><p class="noindent" >This book is created as an overview of the Big Data
technologies, geared toward software architects and advanced
developers. Prior experience with Big Data, as either a user or a
                                                          
                                                          
developer, is not necessary. As this young area is evolving at
an amazing speed, we do not intend to cover how to use
software tools or their APIs in details, which will become
obsolete very soon. Instead, we focus how these systems are
designed and why in this way. We hope that you get a better
understanding of Big Data and thus make the best use of
it.
<!--l. 146--><p class="noindent" >
   <h3 class="sectionHead"><span class="titlemark">1.4   </span> <a 
 id="x1-100001.4"></a>Roadmap</h3>
<!--l. 148--><p class="noindent" >Although the book is made for technologists, we start
with a brief discussion of data management. Frequently,
technologists are lost in the trees of technical details without
seeing the whole forest. As we discussed earlier, Big Data is
meant to meet business needs in a data-driven approach. To
make Big Data a success, executives and managers need all
the disciplines to manage data as a valuable resource.
Chapter 2 brings up a framework to define a successful data
strategy.
<!--l. 150--><p class="indent" >   Chapter 3 is a deep diving into Apache Hadoop, the de facto
Big Data platform. Apache Hadoop is an open-source software
framework for distributed storage and distributed processing of
big data on clusters of commodity hardware. Especially, we
discuss HDFS, MapReduce, Tez and YARN.
<!--l. 152--><p class="indent" >   Chapter 4 is a discussion on Apache Spark, the new hot
buzzword in Big Data. Although MapReduce is great for large
scale data processing, it is not friendly for iterative algorithms
or interactive analytics. Apache Spark is designed to solve this
problem by reusing the working dataset.
                                                          
                                                          
<!--l. 154--><p class="indent" >   MapReduce and Spark enable us to crunch numbers in a
massive parallel way. However, they provide relatively low level
APIs. To quickly obtain actionable insights from data, we
would like to employ some data warehouse built on top of them.
In Chapter 5, we cover Pig and Hive that translate high level
DSL or SQL to native MapReduce/Tez code. Similarly, Shark
and Spark SQL bring SQL on top of Spark. Moreover, we
discuss Cloudera Impala and Apache Drill that are native
massively parallel processing query engines for interactive
analysis of web-scale datasets.
<!--l. 156--><p class="indent" >   In Chapter 6, we discuss several operational NoSQL
databases that are designed for horizontal scaling and high
availability.
<!--l. 158--><p class="indent" >   Although the book can be read sequentially straight
through, you can comfortably break between the chapters. For
example, you may jump directly into the NoSQL chapter while
skipping Hadoop and Spark.
                                                          
                                                          
   <h2 class="chapterHead"><span class="titlemark">Chapter&#x00A0;2</span><br /><a 
 id="x1-110002"></a>Data Management</h2> <hr class="figure"><div class="figure" 
>
                                                          
                                                          
<a 
 id="x1-110011"></a>
                                                          
                                                          

<!--l. 162--><p class="noindent" ><img 
src="images/data-management.png" alt="PIC"  
>
<br /> <div class="caption" 
><span class="id">Figure&#x00A0;2.1: </span><span  
class="content">Data Management</span></div><!--tex4ht:label?: x1-110011 -->
                                                          
                                                          
<!--l. 165--><p class="noindent" ></div><hr class="endfigure">
<!--l. 167--><p class="indent" >   Big Data is to solve complex enterprise optimization
problems. To make the best use of Big Data, we have to
recognize that data is a vital corporate asset as data is the
lifeblood of the Internet economy. Today organizations rely on
data science to make more informed and more effective
decisions, which create competitive advantages through
innovative products and operational efficiencies.
<!--l. 169--><p class="indent" >   However, data is firstly a debt. The costs of data acquisition,
hardware, software, operation, and talents are very high.
Without the right management, it is unlikely for us to
effectively extract values from data. To make big data a
success, we must have all the disciplines to manage data as
a valuable resource. Data management<a 
 id="dx1-11002"></a> is much broader
than database management. It is a systematic process of
capturing, delivering, operating, protecting, enhancing,
and disposing of the data cost-effectively, which needs the
ever-going reinforcement of plans, policies, programs and
practices.
<!--l. 171--><p class="indent" >   The ultimate goal of data management is to increase the
value proposition of the data. It requires serious and careful
consideration and should start with a data strategy that defines
a roadmap to meet the business needs in a data-driven
approach. To create a data strategy<a 
 id="dx1-11003"></a>, think carefully of the
following questions:
      <ul class="itemize1">
      <li class="itemize">What problem do we try to solve? What value can
      big  data  bring  in?  Big  data  is  hot  and  thus  many
      corporations are hugging it. However, big data for the
      sake of big data is apparently wrong. Other&#8217;s use cases
      do not have to be yours. To glean the value of big data,
                                                          
                                                          
      a deep understanding of your business and problems
      to solve is essential.
      </li>
      <li class="itemize">Who holds the data, who owns the data, and who
      can  access  the  data?  Data  governance<a 
 id="dx1-11004"></a>  is  a  set  of
      processes that ensures that important data assets are
      formally managed throughout the enterprise. Through
      data governance, we expect data stewards and data
      custodians to exercise positive control over the data.
      Data custodians are responsible for the safe custody,
      transport, and storage of the data while data stewards
      are responsible for the management of data elements
      &#8211; both the content and metadata.
      </li>
      <li class="itemize">What data do we need? It may seem obvious, but
      it is often simply answered with &#8220;I do not know&#8221; or
      &#8220;Everything&#8221;, which indicates a lack of understanding
      business practices. Whenever this happens, we should
      go back to answer the first question again. How to
      acquire the data? Data may be collected from internal
      system of records, log files, surveys, or third parties.
      The transactional systems may be revised to collect
      necessary data for analytics.
      </li>
      <li class="itemize">Where to store the data and how long to keep them?
      Due  to  the  variety  of  data,  today&#8217;s  data  may  be
      stored  in  various  databases  (relational  or  NoSQL),
      data   warehouses,   Hadoop,   etc.   Today,   database
      management   is   way   beyond   relational   database
                                                          
                                                          
      administration. Because big data is also fast data, it
      is impractical to keep all of the data forever. Careful
      thoughts are needed to determine the lifespan of data.
      </li>
      <li class="itemize">How to ensure the data quality? Junk in, Junk out.
      Without  ensuring  the  data  quality,  big  data  won&#8217;t
      bring  any  values  to  the  business.  With  the  advent
      of big data, data quality management is both more
      important and more challenging than ever.
      </li>
      <li class="itemize">How  to  analyze  and  visualize  the  data?  A  large
      number  of  mathematical  models  are  available  for
      analyzing data. Simply applying mathematical models
      does  not  necessarily  result  in  actionable  insights.
      Before talking about your mathematical models, go
      understand  your  business  and  problems.  Lead  the
      model  with  your  insights  (or  <span 
class="cmti-12">a  priori  </span>in  terms
      of  machine  learning)  rather  than  be  lead  by  the
      uninterpretable numbers of black box models. Besides,
      visualization is extremely helpful to explore data and
      present the analytic results as a picture is worth a
      thousand words.
      </li>
      <li class="itemize">How to manage the complexity? Big data is extremely
      complicated. To manage the complexity and improve
      the data management practices, we need to develop
      the accountability framework to encourage desirable
      behavior,  which  is  tailored  to  the  organization&#8217;s
      business strategies, strengths and priorities.</li></ul>
                                                          
                                                          
<!--l. 182--><p class="indent" >   We believe that a good data strategy will emerge after
thinking through and answer the above questions.
                                                          
                                                          
<!--l. 184--><p class="indent" >
                                                          
                                                          
   <h2 class="chapterHead"><span class="titlemark">Chapter&#x00A0;3</span><br /><a 
 id="x1-120003"></a>Hadoop</h2> Big data unavoidably needs distributed
parallel computing on a cluster of computers. Therefore, we
need a distributed data operating system to manage a variety of
resources, data, and computing tasks. Today, Apache Hadoop
<span class="cite">[<a 
href="#XHadoop">10</a>]</span><a 
 id="dx1-12001"></a> is the de facto distributed data operating system. Apache
Hadoop is an open-source software framework for distributed
storage and distributed processing of big data on clusters of
commodity hardware. Essentially, Hadoop consists of three
parts:
      <ul class="itemize1">
      <li class="itemize">HDFS is a distributed high-throughput file system
      </li>
      <li class="itemize">MapReduce   for   job   framework   of   parallel   data
      processing
      </li>
      <li class="itemize">YARN   for   job   scheduling   and   cluster   resource
      management</li></ul>
<!--l. 192--><p class="noindent" >The HDFS splits files into large blocks that are distributed (and
replicated) among the nodes in the cluster. For processing the
data, MapReduce takes advantage of data locality by shipping
code to the nodes that have the required data and processing
the data in parallel.
<!--l. 194--><p class="indent" >   <hr class="figure"><div class="figure" 
>
                                                          
                                                          
<a 
 id="x1-120021"></a>
                                                          
                                                          

<!--l. 195--><p class="noindent" ><img 
src="images/hadoop.png" alt="PIC"  
>
<br /> <div class="caption" 
><span class="id">Figure&#x00A0;3.1: </span><span  
class="content">Hadoop</span></div><!--tex4ht:label?: x1-120021 -->
                                                          
                                                          
<!--l. 199--><p class="indent" >   </div><hr class="endfigure">
<!--l. 201--><p class="indent" >   Originally Hadoop cluster resource management was part of
MapReduce because it was the main computing paradigm.
Today the Hadoop ecosystem goes beyond MapReduce and
includes many additional parallel computing framework, such as
Apache Spark, Apache Tez, Apache Storm, etc. So the
resource manager, referred to as YARN, was striped out from
MapReduce and improved to support other computing
framework in Hadoop v2. Now MapReduce is one kind
of applications running in a YARN container and other
types of applications can be written generically to run on
YARN.
   <h3 class="sectionHead"><span class="titlemark">3.1   </span> <a 
 id="x1-130003.1"></a>HDFS</h3>
<!--l. 205--><p class="noindent" >Hadoop Distributed File System (HDFS) <span class="cite">[<a 
href="#XHDFS">44</a>]</span><a 
 id="dx1-13001"></a> is a multi-machine
file system that runs on top of machines&#8217; local file system but
appears as a single namespace, accessible through <span 
class="pcrr7t-x-x-120">hdfs://</span>
URIs. It is designed to reliably store very large files across
machines in a large cluster of inexpensive commodity hardware.
HDFS closely follows the design of the Google File System
(GFS)<a 
 id="dx1-13002"></a> <span class="cite">[<a 
href="#XGhemawat:2003:GFS">38</a>,&#x00A0;<a 
href="#XMcKusick:2009:GEF">59</a>]</span>.
<!--l. 207--><p class="noindent" >
   <h4 class="subsectionHead"><span class="titlemark">3.1.1   </span> <a 
 id="x1-140003.1.1"></a>Assumptions</h4>
<!--l. 208--><p class="noindent" >An HDFS instance may consist of hundreds or thousands of
nodes, which are made of inexpensive commodity components
that often fail. It implies that some components are virtually
not functional at any given time and some will not recover from
                                                          
                                                          
their current failures. Therefore, constant monitoring, error
detection, fault tolerance, and automatic recovery would have
to be an integral part of the file system.
<!--l. 210--><p class="indent" >   HDFS is tuned to support a modest number (tens of
millions) of large files, which are typically gigabytes to terabytes
in size. Initially, HDFS assumes a write-once-read-many access
model for files. A file once created, written, and closed need not
be changed. This assumption simplifies the data coherency
problem and enables high throughput data access. The
append operation was added later (single appender only)
<span class="cite">[<a 
href="#XHDFS2010:265">42</a>]</span>.
<!--l. 213--><p class="indent" >   HDFS applications typically have large streaming access to
their datasets. HDFS is mainly designed for batch processing
rather than interactive use. The emphasis is on high throughput
of data access rather than low latency.
<!--l. 215--><p class="noindent" >
   <h4 class="subsectionHead"><span class="titlemark">3.1.2   </span> <a 
 id="x1-150003.1.2"></a>Architecture</h4>
<!--l. 216--><p class="noindent" ><hr class="figure"><div class="figure" 
>
                                                          
                                                          
<a 
 id="x1-150012"></a>
                                                          
                                                          

<!--l. 217--><p class="noindent" ><img 
src="images/hdfs-architecture.png" alt="PIC"  
>
<br /> <div class="caption" 
><span class="id">Figure&#x00A0;3.2: </span><span  
class="content">HDFS Architecture</span></div><!--tex4ht:label?: x1-150012 -->
                                                          
                                                          
<!--l. 221--><p class="noindent" ></div><hr class="endfigure">
<!--l. 223--><p class="indent" >   HDFS has a master/slave architecture. An HDFS
cluster consists of a single NameNode<a 
 id="dx1-15002"></a>, a master server
that manages the file system namespace and regulates
access to files by clients. In addition, there are a number of
DataNodes<a 
 id="dx1-15003"></a> that manage storage attached to the nodes
that they run on. A typical deployment has a dedicated
machine that runs only the NameNode. Each of the other
machines in the cluster runs one instance of the DataNode
<span class="footnote-mark"><a 
href="bigdata4.html#fn1x4"><sup class="textsuperscript">1</sup></a></span><a 
 id="x1-15004f1"></a>.
<!--l. 225--><p class="indent" >   HDFS supports a traditional hierarchical file organization
that consists of directories and files. In HDFS, each file is stored
as a sequence of blocks (identified by 64 bit unique id); all
blocks in a file except the last one are the same size (typically
64 MB). DataNodes store each block in a separate file on local
file system and provide read/write access. When a DataNode
starts up, it scans through its local file system and sends
the list of hosted data blocks (called Blockreport) to the
NameNode.
<!--l. 227--><p class="indent" >   For reliability, each block is replicated on multiple
DataNodes (three replicas by default). The placement of
replicas is critical to HDFS reliability and performance. HDFS
employs a rack-aware replica placement policy to improve data
reliability, availability, and network bandwidth utilization.
When the replication factor is three, HDFS puts one replica on
one node in the local rack, another on a different node
in the same rack, and the last on a node in a different
rack. This policy reduces the inter-rack write traffic which
                                                          
                                                          
generally improves write performance. Since the chance
of rack failure is far less than that of node failure, this
policy does not impact data reliability and availability
notably.
<!--l. 230--><p class="indent" >   The NameNode is the arbitrator and repository for all
HDFS metadata. The NameNode executes common namespace
operations such as create, delete, modify and list files
and directories. The NameNode also performs the block
management including mapping files to blocks, creating
and deleting blocks, and managing replica placement and
re-replication. Besides, the NameNode provides DataNode
cluster membership by handling registrations and periodic
heart beats. But the user data never flows through the
NameNode.
<!--l. 232--><p class="indent" >   To achieve high performance, the NameNode keeps all
metadata in main memory including the file and block
namespace, the mapping from files to blocks, and the locations
of each block&#8217;s replicas. The namespace and file-to-block
mapping are also kept persistent into the files EditLog<a 
 id="dx1-15005"></a> and
FsImage<a 
 id="dx1-15006"></a> in the local file system of the NameNode. The
file FsImage stores the entire file system namespace and
file-to-block map. The EditLog is a transaction log to record
every change that occurs to file system metadata, e.g. creating a
new file and changing the replication factor of a file. When the
NameNode starts up, it reads the FsImage and EditLog
from disk, applies all the transactions from the EditLog to
the in-memory representation of the FsImage, flushes out
the new version of FsImage to disk, and truncates the
EditLog.
<!--l. 235--><p class="indent" >   Because the NameNode replays the EditLog and updates
the FsImage only during start up, the EditLog could get very
                                                          
                                                          
large over time and the next restart of NameNode takes longer.
To avoid this problem, HDFS has a secondary NameNode that
updates the FsImage with the EditLog periodically and
keeps the EditLog within a limit. Note that the secondary
NameNode is not a standby NameNode. It usually runs on a
different machine from the primary NameNode since its
memory requirements are on the same order as the primary
NameNode.
<!--l. 237--><p class="indent" >   The NameNode does not store block location information
persistently. On startup, the NameNode enters a special
state called Safemode<a 
 id="dx1-15007"></a> and receives Blockreport messages
from the DataNodes. Each block has a specified minimum
number of replicas. A block is considered safely replicated
when the minimum number of replicas has checked in with
the NameNode. After a configurable percentage of safely
replicated data blocks checks in with the NameNode (plus an
additional 30 seconds), the NameNode exits the Safemode
state.
   <h4 class="subsectionHead"><span class="titlemark">3.1.3   </span> <a 
 id="x1-160003.1.3"></a>Control and Data Flow</h4>
<!--l. 240--><p class="noindent" >HDFS is designed such that clients never read and write file
data through the NameNode. Instead, a client asks the
NameNode which DataNodes it should contact using the class
ClientProtocol through an RPC connection. Then the client
communicates with a DataNode directly to transfer data using
the DataTransferProtocol, which is a streaming protocol for
performance reasons. Besides, all communication between
Namenode and Datanode, e.g. DataNode registration,
heartbeat, Blockreport, is initiated by the Datanode, and
responded to by the Namenode.
                                                          
                                                          
<!--l. 242--><p class="noindent" >
   <h5 class="subsubsectionHead"><span class="titlemark">3.1.3.1   </span> <a 
 id="x1-170003.1.3.1"></a>Read</h5>
<!--l. 243--><p class="noindent" >First, the client queries the NameNode with the file name, read
range start offset, and the range length. The NameNode returns
the locations of the blocks of the specified file within the
specified range. Especially, DataNode locations for each block
are sorted by the proximity to the client. The client then sends
a request to one of the DataNodes, most likely the closest
one.
<!--l. 245--><p class="noindent" >
   <h5 class="subsubsectionHead"><span class="titlemark">3.1.3.2   </span> <a 
 id="x1-180003.1.3.2"></a>Write</h5>
<!--l. 246--><p class="noindent" >A client request to create a file does not reach the NameNode
immediately. Instead, the client caches the file data into a
temporary local file. Once the local file accumulates data worth
over one block size, the client contacts the NameNode, which
updates the file system namespace and returns the allocated
data block location. Then the client flushes the block from the
local temporary file to the specified DataNode. When a file is
closed, the remaining last block data is transferred to the
DataNodes.
<!--l. 248--><p class="noindent" >
   <h4 class="subsectionHead"><span class="titlemark">3.1.4   </span> <a 
 id="x1-190003.1.4"></a>The Small Files Problem</h4>
<!--l. 249--><p class="noindent" >Big data but small files (significantly smaller than the block
size) implies a lot of files, which creates a big problem for the
                                                          
                                                          
NameNode <span class="cite">[<a 
href="#XSmallFiles">79</a>]</span>. Recall that the NameNode holds all the
metadata of files and blocks in main memory. Given that
each of the metadata object occupies about 150 bytes, the
NameNode may host about 10 million files, each using a block,
with 3 gigabytes of memory. Although larger memory can push
the upper limit higher, large heap is a big challenge for JVM
garbage collector. Furthermore, HDFS is not efficient to read
small files because of the overhead of client-NameNode
communication, too much disk seeks, and lots of hopping from
DataNode to DataNode to retrieve each small file.
<!--l. 252--><p class="indent" >   In order to reduce the number of files and thus the pressure
on the NameNode&#8217;s memory, Hadoop Archives (HAR files) were
introduced. HAR files<a 
 id="dx1-19001"></a>, created by <span 
class="pcrr7t-x-x-120">hadoop archive</span>
<span class="footnote-mark"><a 
href="bigdata5.html#fn2x4"><sup class="textsuperscript">2</sup></a></span><a 
 id="x1-19002f2"></a>
command, are special format archives that contain metadata
and data files. The archive exposes itself as a file system layer.
All of the original files are visible and accessible through a
<span 
class="pcrr7t-x-x-120">har:// </span>URI. It is also easy to use HAR files as input file
system in MapReduce. Note that it is actually slower to read
through files in a HAR because of the extra access to
metadata.
<!--l. 254--><p class="indent" >   The SequenceFile<a 
 id="dx1-19003"></a>, consisting of binary key-value pairs, can
also be used to handle the small files problem, by using the
filename as the key and the file contents as the value. This
works very well in practice for MapReduce jobs. Besides, the
SequenceFile supports compression, which reduces disk
usage and speeds up data loading in MapReduce. Open
source tools exist to convert tar files to SequenceFiles
                                                          
                                                          
<span class="cite">[<a 
href="#XTar2Seq">74</a>]</span>.
<!--l. 256--><p class="indent" >   The key-value stores, e.g. HBase<a 
 id="dx1-19004"></a> and Accumulo<a 
 id="dx1-19005"></a>, may also be
used to reduce file count although they are designed for much
more complicated use cases. Compared to SequenceFile, they
support random access by keys.
   <h4 class="subsectionHead"><span class="titlemark">3.1.5   </span> <a 
 id="x1-200003.1.5"></a>HDFS Federation</h4>
<!--l. 259--><p class="noindent" >The existence of a single NameNode in a cluster greatly
simplifies the architecture of the system. However, it also
introduces problems. The file count problem, due to the limited
memory of NameNode, is an example. A more serious problem
is that it proved to be a bottleneck for the clients <span class="cite">[<a 
href="#XMcKusick:2009:GEF">59</a>]</span>. Even
though the clients issue few metadata operations to the
NameNode, there may be thousands of clients all talking to the
NameNode at the same time. With multiple MapReduce jobs,
we might suddenly have thousands of tasks in a large
cluster, each trying to open a number of files. Given that the
NameNode is capable of doing only a few thousand operations
a second, it would take a long time to handle all those
requests.
<!--l. 261--><p class="indent" >   Since Hadoop 2.0, we can have two redundant NameNodes
in the same cluster in an active/passive configuration with
a hot standby. Although this allows a fast failover to a
new NameNode for fault tolerance, it does not solve the
the performance issue. To partially resolve the scalability
problem, the concept of HDFS Federation<a 
 id="dx1-20001"></a>, was introduced
to allow multiple namespaces within a HDFS cluster. In
the future, it may also support the cooperation across
clusters.
<!--l. 263--><p class="indent" >   In HDFS Federation, there are multiple independent
                                                          
                                                          
NameNodes (and thus multiple namespaces). The NameNodes
do not require coordination with each other. The DataNodes
are used as the common storage by all the NameNodes
by registering with and handles commands from all the
NameNodes in the cluster. The failure of a NameNode does not
prevent the DataNode from serving other NameNodes in the
cluster.
<!--l. 265--><p class="indent" >   Because multiple NameNodes run independently, there
may be conflicts of 64 bit block ids generated by different
NameNodes. To avoid this problem, a namespace uses one or
more block pools, identified by a unique id in a cluster. A block
pool belongs to a single namespace and does not cross
namespace boundary. The extended block id, a tuple of (Block
Pool ID, Block ID), is used for block identification in HDFS
Federation.
<!--l. 267--><p class="noindent" >
   <h4 class="subsectionHead"><span class="titlemark">3.1.6   </span> <a 
 id="x1-210003.1.6"></a>Java API</h4>
<!--l. 268--><p class="noindent" >HDFS is implemented in Java and provides a native Java API.
To access HDFS in other programming languages, Thrift<a 
 id="dx1-21001"></a>
<span class="footnote-mark"><a 
href="bigdata6.html#fn3x4"><sup class="textsuperscript">3</sup></a></span><a 
 id="x1-21002f3"></a>
bindings are provided for Perl, Python, Ruby and PHP <span class="cite">[<a 
href="#XHdfsThrift">12</a>]</span>. In
what follows, we will discuss how to work with HDFS Java API
                                                          
                                                          
with a couple of small examples. First of all, we need to add the
following dependencies to the project&#8217;s Maven POM file <span class="cite">[<a 
href="#XMaven">15</a>]</span>.
<!--l. 269-->
<div class="lstlisting" id="listing-1"><span class="label"><a 
 id="x1-21003r1"></a></span><span 
class="pcrr7t-x-x-109">&#x003C;</span><span 
class="pcrr7t-x-x-109">dependency</span><span 
class="pcrr7t-x-x-109">&#x003E;</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><br /><span class="label"><a 
 id="x1-21004r2"></a></span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><span 
class="pcrr7t-x-x-109">&#x003C;</span><span 
class="pcrr7t-x-x-109">groupId</span><span 
class="pcrr7t-x-x-109">&#x003E;</span><span 
class="pcrr7t-x-x-109">org</span><span 
class="pcrr7t-x-x-109">.</span><span 
class="pcrr7t-x-x-109">apache</span><span 
class="pcrr7t-x-x-109">.</span><span 
class="pcrr7t-x-x-109">hadoop</span><span 
class="pcrr7t-x-x-109">&#x003C;/</span><span 
class="pcrr7t-x-x-109">groupId</span><span 
class="pcrr7t-x-x-109">&#x003E;</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><br /><span class="label"><a 
 id="x1-21005r3"></a></span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><span 
class="pcrr7t-x-x-109">&#x003C;</span><span 
class="pcrr7t-x-x-109">artifactId</span><span 
class="pcrr7t-x-x-109">&#x003E;</span><span 
class="pcrr7t-x-x-109">hadoop</span><span 
class="pcrr7t-x-x-109">-</span><span 
class="pcrr7t-x-x-109">common</span><span 
class="pcrr7t-x-x-109">&#x003C;/</span><span 
class="pcrr7t-x-x-109">artifactId</span><span 
class="pcrr7t-x-x-109">&#x003E;</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><br /><span class="label"><a 
 id="x1-21006r4"></a></span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><span 
class="pcrr7t-x-x-109">&#x003C;</span><span 
class="pcrr7t-x-x-109">version</span><span 
class="pcrr7t-x-x-109">&#x003E;2.6.0&#x003C;/</span><span 
class="pcrr7t-x-x-109">version</span><span 
class="pcrr7t-x-x-109">&#x003E;</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><br /><span class="label"><a 
 id="x1-21007r5"></a></span><span 
class="pcrr7t-x-x-109">&#x003C;/</span><span 
class="pcrr7t-x-x-109">dependency</span><span 
class="pcrr7t-x-x-109">&#x003E;</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><br /><span class="label"><a 
 id="x1-21008r6"></a></span><span 
class="pcrr7t-x-x-109">&#x003C;</span><span 
class="pcrr7t-x-x-109">dependency</span><span 
class="pcrr7t-x-x-109">&#x003E;</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><br /><span class="label"><a 
 id="x1-21009r7"></a></span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><span 
class="pcrr7t-x-x-109">&#x003C;</span><span 
class="pcrr7t-x-x-109">groupId</span><span 
class="pcrr7t-x-x-109">&#x003E;</span><span 
class="pcrr7t-x-x-109">org</span><span 
class="pcrr7t-x-x-109">.</span><span 
class="pcrr7t-x-x-109">apache</span><span 
class="pcrr7t-x-x-109">.</span><span 
class="pcrr7t-x-x-109">hadoop</span><span 
class="pcrr7t-x-x-109">&#x003C;/</span><span 
class="pcrr7t-x-x-109">groupId</span><span 
class="pcrr7t-x-x-109">&#x003E;</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><br /><span class="label"><a 
 id="x1-21010r8"></a></span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><span 
class="pcrr7t-x-x-109">&#x003C;</span><span 
class="pcrr7t-x-x-109">artifactId</span><span 
class="pcrr7t-x-x-109">&#x003E;</span><span 
class="pcrr7t-x-x-109">hadoop</span><span 
class="pcrr7t-x-x-109">-</span><span 
class="pcrr7t-x-x-109">hdfs</span><span 
class="pcrr7t-x-x-109">&#x003C;/</span><span 
class="pcrr7t-x-x-109">artifactId</span><span 
class="pcrr7t-x-x-109">&#x003E;</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><br /><span class="label"><a 
 id="x1-21011r9"></a></span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><span 
class="pcrr7t-x-x-109">&#x003C;</span><span 
class="pcrr7t-x-x-109">version</span><span 
class="pcrr7t-x-x-109">&#x003E;2.6.0&#x003C;/</span><span 
class="pcrr7t-x-x-109">version</span><span 
class="pcrr7t-x-x-109">&#x003E;</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><br /><span class="label"><a 
 id="x1-21012r10"></a></span><span 
class="pcrr7t-x-x-109">&#x003C;/</span><span 
class="pcrr7t-x-x-109">dependency</span><span 
class="pcrr7t-x-x-109">&#x003E;</span>
   </div>
<!--l. 282--><p class="indent" >   The main entry point of HDFS Java API is the abstract class
<span 
class="pcrr7t-x-x-120">FileSystem </span>in the package <span 
class="pcrr7t-x-x-120">org.apache.hadoop.fs </span>that
serves as a generic file system representation. <span 
class="pcrr7t-x-x-120">FileSystem </span>has
various implementations:
      <dl class="description"><dt class="description">
<span 
class="cmbx-12">DistributedFileSystem</span> </dt><dd 
class="description">The
      implementation of distributed file system. This object
      is the way end-user code interacts with an HDFS.
      </dd><dt class="description">
<span 
class="cmbx-12">LocalFileSystem</span> </dt><dd 
class="description">The  local  implementation  for  small
      Hadoop instances and for testing.
      </dd><dt class="description">
<span 
class="cmbx-12">FTPFileSystem</span> </dt><dd 
class="description">A FileSystem backed by an FTP client.
      </dd><dt class="description">
<span 
class="cmbx-12">S3FileSystem</span> </dt><dd 
class="description">A   block-based   FileSystem   backed   by
      Amazon S3.</dd></dl>
<!--l. 290--><p class="indent" >   The <span 
class="pcrr7t-x-x-120">FileSystem </span>class also serves as a factory for concrete
implementations: <!--l. 291-->
<div class="lstlisting" id="listing-2"><span class="label"><a 
 id="x1-21013r1"></a></span><span 
class="pcrr7t-x-x-109">Configuration</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><span 
class="pcrr7t-x-x-109">conf</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><span 
class="pcrr7t-x-x-109">=</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><span 
class="pcrr7t-x-x-109">new</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><span 
class="pcrr7t-x-x-109">Configuration</span><span 
class="pcrr7t-x-x-109">()</span><span 
class="pcrr7t-x-x-109">;</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><br /><span class="label"><a 
 id="x1-21014r2"></a></span><span 
class="pcrr7t-x-x-109">FileSystem</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><span 
class="pcrr7t-x-x-109">fs</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><span 
class="pcrr7t-x-x-109">=</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><span 
class="pcrr7t-x-x-109">FileSystem</span><span 
class="pcrr7t-x-x-109">.</span><span 
class="pcrr7t-x-x-109">get</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><span 
class="pcrr7t-x-x-109">(</span><span 
class="pcrr7t-x-x-109">conf</span><span 
class="pcrr7t-x-x-109">)</span><span 
class="pcrr7t-x-x-109">;</span>
                                                          
                                                          
   </div>
<!--l. 295--><p class="indent" >   where the <span 
class="pcrr7t-x-x-120">Configuration </span>class passes the Hadoop
configuration information such as scheme, authority, NameNode
host and port, etc. Unless explicitly turned off, Hadoop by
default specifies two resources, loaded in-order from the
classpath:
      <dl class="description"><dt class="description">
<span 
class="cmbx-12">core-default.xml</span> </dt><dd 
class="description">Read-only defaults for Hadoop.
      </dd><dt class="description">
<span 
class="cmbx-12">core-site.xml</span> </dt><dd 
class="description">Site-specific   configuration   for   a   given
      Hadoop installation.</dd></dl>
<!--l. 300--><p class="noindent" >Applications may add additional resources, which are loaded
subsequent to these resources in the order they are added. With
<span 
class="pcrr7t-x-x-120">FileSystem</span>, one can do common namespace operations,
e.g. creating, deleting, and renaming files. We can also
query the status of a file such as the length, block size,
block locations, permission, etc. To read or write files,
we need to use the classes <span 
class="pcrr7t-x-x-120">FSDataInputStream </span>and
<span 
class="pcrr7t-x-x-120">FSDataOutputStream</span>. In the following example, we develop
two simple functions to copy a local file into/from HDFS. For
simplicity, we do not check the file existence or any I/O
errors. Note that <span 
class="pcrr7t-x-x-120">FileSystem </span>does provide several utility
functions for copying files between local and distributed file
systems.
   <!--l. 302-->
<div class="lstlisting" id="listing-3"><span class="label"><a 
 id="x1-21015r1"></a></span><span 
class="pcrr7t-x-x-109">/&#x22C6;</span><span 
class="pcrr7t-x-x-109">&#x22C6;</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><span 
class="pcrr7t-x-x-109">Copy</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><span 
class="pcrr7t-x-x-109">a</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><span 
class="pcrr7t-x-x-109">local</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><span 
class="pcrr7t-x-x-109">file</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><span 
class="pcrr7t-x-x-109">to</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><span 
class="pcrr7t-x-x-109">HDFS</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><span 
class="pcrr7t-x-x-109">&#x22C6;/</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><br /><span class="label"><a 
 id="x1-21016r2"></a></span><span 
class="pcrr7t-x-x-109">public</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><span 
class="pcrr7t-x-x-109">void</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><span 
class="pcrr7t-x-x-109">copyFromLocal</span><span 
class="pcrr7t-x-x-109">(</span><span 
class="pcrr7t-x-x-109">String</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><span 
class="pcrr7t-x-x-109">src</span><span 
class="pcrr7t-x-x-109">,</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><span 
class="pcrr7t-x-x-109">String</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><span 
class="pcrr7t-x-x-109">dst</span><span 
class="pcrr7t-x-x-109">)</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><span 
class="pcrr7t-x-x-109">throws</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><span 
class="pcrr7t-x-x-109">IOException</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><span 
class="pcrr7t-x-x-109">{</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><br /><span class="label"><a 
 id="x1-21017r3"></a></span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><br /><span class="label"><a 
 id="x1-21018r4"></a></span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><span 
class="pcrr7t-x-x-109">Configuration</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><span 
class="pcrr7t-x-x-109">conf</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><span 
class="pcrr7t-x-x-109">=</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><span 
class="pcrr7t-x-x-109">new</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><span 
class="pcrr7t-x-x-109">Configuration</span><span 
class="pcrr7t-x-x-109">()</span><span 
class="pcrr7t-x-x-109">;</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><br /><span class="label"><a 
 id="x1-21019r5"></a></span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><span 
class="pcrr7t-x-x-109">FileSystem</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><span 
class="pcrr7t-x-x-109">fs</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><span 
class="pcrr7t-x-x-109">=</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><span 
class="pcrr7t-x-x-109">FileSystem</span><span 
class="pcrr7t-x-x-109">.</span><span 
class="pcrr7t-x-x-109">get</span><span 
class="pcrr7t-x-x-109">(</span><span 
class="pcrr7t-x-x-109">conf</span><span 
class="pcrr7t-x-x-109">)</span><span 
class="pcrr7t-x-x-109">;</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><br /><span class="label"><a 
 id="x1-21020r6"></a></span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><br /><span class="label"><a 
 id="x1-21021r7"></a></span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><span 
class="pcrr7t-x-x-109">//</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><span 
class="pcrr7t-x-x-109">The</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><span 
class="pcrr7t-x-x-109">Path</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><span 
class="pcrr7t-x-x-109">object</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><span 
class="pcrr7t-x-x-109">represents</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><span 
class="pcrr7t-x-x-109">a</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><span 
class="pcrr7t-x-x-109">file</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><span 
class="pcrr7t-x-x-109">or</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><span 
class="pcrr7t-x-x-109">directory</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><span 
class="pcrr7t-x-x-109">in</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><span 
class="pcrr7t-x-x-109">HDFS</span><span 
class="pcrr7t-x-x-109">.</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><br /><span class="label"><a 
 id="x1-21022r8"></a></span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><span 
class="pcrr7t-x-x-109">FSDataOutputStream</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><span 
class="pcrr7t-x-x-109">out</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><span 
class="pcrr7t-x-x-109">=</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><span 
class="pcrr7t-x-x-109">fs</span><span 
class="pcrr7t-x-x-109">.</span><span 
class="pcrr7t-x-x-109">create</span><span 
class="pcrr7t-x-x-109">(</span><span 
class="pcrr7t-x-x-109">new</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><span 
class="pcrr7t-x-x-109">Path</span><span 
class="pcrr7t-x-x-109">(</span><span 
class="pcrr7t-x-x-109">dst</span><span 
class="pcrr7t-x-x-109">)</span><span 
class="pcrr7t-x-x-109">)</span><span 
class="pcrr7t-x-x-109">;</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><br /><span class="label"><a 
 id="x1-21023r9"></a></span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><span 
class="pcrr7t-x-x-109">InputStream</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><span 
class="pcrr7t-x-x-109">in</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><span 
class="pcrr7t-x-x-109">=</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><span 
class="pcrr7t-x-x-109">new</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><span 
class="pcrr7t-x-x-109">BufferedInputStream</span><span 
class="pcrr7t-x-x-109">(</span><span 
class="pcrr7t-x-x-109">new</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><span 
class="pcrr7t-x-x-109">FileInputStream</span><span 
class="pcrr7t-x-x-109">(</span><span 
class="pcrr7t-x-x-109">new</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><span 
class="pcrr7t-x-x-109">File</span><span 
class="pcrr7t-x-x-109">(</span><span 
class="pcrr7t-x-x-109">src</span><span 
class="pcrr7t-x-x-109">)</span><span 
class="pcrr7t-x-x-109">)</span><span 
class="pcrr7t-x-x-109">)</span><span 
class="pcrr7t-x-x-109">;</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><br /><span class="label"><a 
 id="x1-21024r10"></a></span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><br /><span class="label"><a 
 id="x1-21025r11"></a></span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><span 
class="pcrr7t-x-x-109">byte</span><span 
class="pcrr7t-x-x-109">[]</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><span 
class="pcrr7t-x-x-109">b</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><span 
class="pcrr7t-x-x-109">=</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><span 
class="pcrr7t-x-x-109">new</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><span 
class="pcrr7t-x-x-109">byte</span><span 
class="pcrr7t-x-x-109">[1024];</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><br /><span class="label"><a 
 id="x1-21026r12"></a></span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><span 
class="pcrr7t-x-x-109">int</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><span 
class="pcrr7t-x-x-109">numBytes</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><span 
class="pcrr7t-x-x-109">=</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><span 
class="pcrr7t-x-x-109">0;</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><br /><span class="label"><a 
 id="x1-21027r13"></a></span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><span 
class="pcrr7t-x-x-109">while</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><span 
class="pcrr7t-x-x-109">((</span><span 
class="pcrr7t-x-x-109">numBytes</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><span 
class="pcrr7t-x-x-109">=</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><span 
class="pcrr7t-x-x-109">in</span><span 
class="pcrr7t-x-x-109">.</span><span 
class="pcrr7t-x-x-109">read</span><span 
class="pcrr7t-x-x-109">(</span><span 
class="pcrr7t-x-x-109">b</span><span 
class="pcrr7t-x-x-109">)</span><span 
class="pcrr7t-x-x-109">)</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><span 
class="pcrr7t-x-x-109">&#x003E;</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><span 
class="pcrr7t-x-x-109">0)</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><span 
class="pcrr7t-x-x-109">{</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><br /><span class="label"><a 
 id="x1-21028r14"></a></span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><span 
class="pcrr7t-x-x-109">out</span><span 
class="pcrr7t-x-x-109">.</span><span 
class="pcrr7t-x-x-109">write</span><span 
class="pcrr7t-x-x-109">(</span><span 
class="pcrr7t-x-x-109">b</span><span 
class="pcrr7t-x-x-109">,</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><span 
class="pcrr7t-x-x-109">0,</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><span 
class="pcrr7t-x-x-109">numBytes</span><span 
class="pcrr7t-x-x-109">)</span><span 
class="pcrr7t-x-x-109">;</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><br /><span class="label"><a 
 id="x1-21029r15"></a></span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><span 
class="pcrr7t-x-x-109">}</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><br /><span class="label"><a 
 id="x1-21030r16"></a></span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><br /><span class="label"><a 
 id="x1-21031r17"></a></span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><span 
class="pcrr7t-x-x-109">in</span><span 
class="pcrr7t-x-x-109">.</span><span 
class="pcrr7t-x-x-109">close</span><span 
class="pcrr7t-x-x-109">()</span><span 
class="pcrr7t-x-x-109">;</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><br /><span class="label"><a 
 id="x1-21032r18"></a></span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><span 
class="pcrr7t-x-x-109">out</span><span 
class="pcrr7t-x-x-109">.</span><span 
class="pcrr7t-x-x-109">close</span><span 
class="pcrr7t-x-x-109">()</span><span 
class="pcrr7t-x-x-109">;</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><br /><span class="label"><a 
 id="x1-21033r19"></a></span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><span 
class="pcrr7t-x-x-109">fs</span><span 
class="pcrr7t-x-x-109">.</span><span 
class="pcrr7t-x-x-109">close</span><span 
class="pcrr7t-x-x-109">()</span><span 
class="pcrr7t-x-x-109">;</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><br /><span class="label"><a 
 id="x1-21034r20"></a></span><span 
class="pcrr7t-x-x-109">}</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><br /><span class="label"><a 
 id="x1-21035r21"></a></span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><br /><span class="label"><a 
 id="x1-21036r22"></a></span><span 
class="pcrr7t-x-x-109">/&#x22C6;</span><span 
class="pcrr7t-x-x-109">&#x22C6;</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><span 
class="pcrr7t-x-x-109">Copy</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><span 
class="pcrr7t-x-x-109">an</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><span 
class="pcrr7t-x-x-109">HDFS</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><span 
class="pcrr7t-x-x-109">file</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><span 
class="pcrr7t-x-x-109">to</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><span 
class="pcrr7t-x-x-109">local</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><span 
class="pcrr7t-x-x-109">file</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><span 
class="pcrr7t-x-x-109">system</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><span 
class="pcrr7t-x-x-109">&#x22C6;/</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><br /><span class="label"><a 
 id="x1-21037r23"></a></span><span 
class="pcrr7t-x-x-109">public</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><span 
class="pcrr7t-x-x-109">void</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><span 
class="pcrr7t-x-x-109">copyToLocal</span><span 
class="pcrr7t-x-x-109">(</span><span 
class="pcrr7t-x-x-109">String</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><span 
class="pcrr7t-x-x-109">src</span><span 
class="pcrr7t-x-x-109">,</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><span 
class="pcrr7t-x-x-109">String</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><span 
class="pcrr7t-x-x-109">dst</span><span 
class="pcrr7t-x-x-109">)</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><span 
class="pcrr7t-x-x-109">throws</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><span 
class="pcrr7t-x-x-109">IOException</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><span 
class="pcrr7t-x-x-109">{</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><br /><span class="label"><a 
 id="x1-21038r24"></a></span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><br /><span class="label"><a 
 id="x1-21039r25"></a></span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><span 
class="pcrr7t-x-x-109">Configuration</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><span 
class="pcrr7t-x-x-109">conf</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><span 
class="pcrr7t-x-x-109">=</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><span 
class="pcrr7t-x-x-109">new</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><span 
class="pcrr7t-x-x-109">Configuration</span><span 
class="pcrr7t-x-x-109">()</span><span 
class="pcrr7t-x-x-109">;</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><br /><span class="label"><a 
 id="x1-21040r26"></a></span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><span 
class="pcrr7t-x-x-109">FileSystem</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><span 
class="pcrr7t-x-x-109">fs</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><span 
class="pcrr7t-x-x-109">=</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><span 
class="pcrr7t-x-x-109">FileSystem</span><span 
class="pcrr7t-x-x-109">.</span><span 
class="pcrr7t-x-x-109">get</span><span 
class="pcrr7t-x-x-109">(</span><span 
class="pcrr7t-x-x-109">conf</span><span 
class="pcrr7t-x-x-109">)</span><span 
class="pcrr7t-x-x-109">;</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><br /><span class="label"><a 
 id="x1-21041r27"></a></span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><br /><span class="label"><a 
 id="x1-21042r28"></a></span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><span 
class="pcrr7t-x-x-109">FSDataInputStream</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><span 
class="pcrr7t-x-x-109">in</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><span 
class="pcrr7t-x-x-109">=</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><span 
class="pcrr7t-x-x-109">fs</span><span 
class="pcrr7t-x-x-109">.</span><span 
class="pcrr7t-x-x-109">open</span><span 
class="pcrr7t-x-x-109">(</span><span 
class="pcrr7t-x-x-109">new</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><span 
class="pcrr7t-x-x-109">Path</span><span 
class="pcrr7t-x-x-109">(</span><span 
class="pcrr7t-x-x-109">src</span><span 
class="pcrr7t-x-x-109">)</span><span 
class="pcrr7t-x-x-109">)</span><span 
class="pcrr7t-x-x-109">;</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><br /><span class="label"><a 
 id="x1-21043r29"></a></span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><span 
class="pcrr7t-x-x-109">OutputStream</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><span 
class="pcrr7t-x-x-109">out</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><span 
class="pcrr7t-x-x-109">=</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><span 
class="pcrr7t-x-x-109">new</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><span 
class="pcrr7t-x-x-109">BufferedOutputStream</span><span 
class="pcrr7t-x-x-109">(</span><span 
class="pcrr7t-x-x-109">new</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><span 
class="pcrr7t-x-x-109">FileOutputStream</span><span 
class="pcrr7t-x-x-109">(</span><span 
class="pcrr7t-x-x-109">new</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><span 
class="pcrr7t-x-x-109">File</span><span 
class="pcrr7t-x-x-109">(</span><span 
class="pcrr7t-x-x-109">dst</span><span 
class="pcrr7t-x-x-109">)</span><span 
class="pcrr7t-x-x-109">)</span><span 
class="pcrr7t-x-x-109">)</span><span 
class="pcrr7t-x-x-109">;</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><br /><span class="label"><a 
 id="x1-21044r30"></a></span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><span 
class="pcrr7t-x-x-109">byte</span><span 
class="pcrr7t-x-x-109">[]</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><span 
class="pcrr7t-x-x-109">b</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><span 
class="pcrr7t-x-x-109">=</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><span 
class="pcrr7t-x-x-109">new</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><span 
class="pcrr7t-x-x-109">byte</span><span 
class="pcrr7t-x-x-109">[1024];</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><br /><span class="label"><a 
 id="x1-21045r31"></a></span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><span 
class="pcrr7t-x-x-109">int</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><span 
class="pcrr7t-x-x-109">numBytes</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><span 
class="pcrr7t-x-x-109">=</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><span 
class="pcrr7t-x-x-109">0;</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><br /><span class="label"><a 
 id="x1-21046r32"></a></span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><span 
class="pcrr7t-x-x-109">while</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><span 
class="pcrr7t-x-x-109">((</span><span 
class="pcrr7t-x-x-109">numBytes</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><span 
class="pcrr7t-x-x-109">=</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><span 
class="pcrr7t-x-x-109">in</span><span 
class="pcrr7t-x-x-109">.</span><span 
class="pcrr7t-x-x-109">read</span><span 
class="pcrr7t-x-x-109">(</span><span 
class="pcrr7t-x-x-109">b</span><span 
class="pcrr7t-x-x-109">)</span><span 
class="pcrr7t-x-x-109">)</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><span 
class="pcrr7t-x-x-109">&#x003E;</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><span 
class="pcrr7t-x-x-109">0)</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><span 
class="pcrr7t-x-x-109">{</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><br /><span class="label"><a 
 id="x1-21047r33"></a></span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><span 
class="pcrr7t-x-x-109">out</span><span 
class="pcrr7t-x-x-109">.</span><span 
class="pcrr7t-x-x-109">write</span><span 
class="pcrr7t-x-x-109">(</span><span 
class="pcrr7t-x-x-109">b</span><span 
class="pcrr7t-x-x-109">,</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><span 
class="pcrr7t-x-x-109">0,</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><span 
class="pcrr7t-x-x-109">numBytes</span><span 
class="pcrr7t-x-x-109">)</span><span 
class="pcrr7t-x-x-109">;</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><br /><span class="label"><a 
 id="x1-21048r34"></a></span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><span 
class="pcrr7t-x-x-109">}</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><br /><span class="label"><a 
 id="x1-21049r35"></a></span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><br /><span class="label"><a 
 id="x1-21050r36"></a></span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><span 
class="pcrr7t-x-x-109">in</span><span 
class="pcrr7t-x-x-109">.</span><span 
class="pcrr7t-x-x-109">close</span><span 
class="pcrr7t-x-x-109">()</span><span 
class="pcrr7t-x-x-109">;</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><br /><span class="label"><a 
 id="x1-21051r37"></a></span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><span 
class="pcrr7t-x-x-109">out</span><span 
class="pcrr7t-x-x-109">.</span><span 
class="pcrr7t-x-x-109">close</span><span 
class="pcrr7t-x-x-109">()</span><span 
class="pcrr7t-x-x-109">;</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><br /><span class="label"><a 
 id="x1-21052r38"></a></span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><span 
class="pcrr7t-x-x-109">fs</span><span 
class="pcrr7t-x-x-109">.</span><span 
class="pcrr7t-x-x-109">close</span><span 
class="pcrr7t-x-x-109">()</span><span 
class="pcrr7t-x-x-109">;</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><br /><span class="label"><a 
 id="x1-21053r39"></a></span><span 
class="pcrr7t-x-x-109">}</span>
   </div>
                                                          
                                                          
<!--l. 344--><p class="indent" >   In the example, we use the method <span 
class="pcrr7t-x-x-120">FileSystem.create</span>
to create an <span 
class="pcrr7t-x-x-120">FSDataOutputStream </span>at the indicated <span 
class="pcrr7t-x-x-120">Path</span>.
If the file exists, it will be overwritten by default. The
<span 
class="pcrr7t-x-x-120">Path </span>object is used to locate a file or directory in HDFS.
<span 
class="pcrr7t-x-x-120">Path </span>is really a URI. For HDFS, it takes the format of
<span 
class="pcrr7t-x-x-120">hdfs://host: port/location</span>. To read an HDFS file,
we use the method <span 
class="pcrr7t-x-x-120">FileSystem.open </span>that returns an
<span 
class="pcrr7t-x-x-120">FSDataInputStream </span>object. The rest of example is just as
the regular Java I/O stream operations.
   <h4 class="subsectionHead"><span class="titlemark">3.1.7   </span> <a 
 id="x1-220003.1.7"></a>Data Ingestion</h4>
<!--l. 349--><p class="noindent" >Today, most data are generated and stored out of Hadoop, e.g.
relational databases, plain files, etc. Therefore, data ingestion is
the first step to utilize the power of Hadoop. To move the data
into HDFS, we do not have to do the low level programming as
the previous example. Various utilities have been developed to
move data into Hadoop.
<!--l. 351--><p class="noindent" >
   <h5 class="subsubsectionHead"><span class="titlemark">3.1.7.1   </span> <a 
 id="x1-230003.1.7.1"></a>Batch Data Ingestion</h5>
<!--l. 352--><p class="noindent" >The File System Shell<a 
 id="dx1-23001"></a> <span class="cite">[<a 
href="#XHdfsShell">8</a>]</span> includes various shell-like commands,
including <span 
class="pcrr7t-x-x-120">copyFromLocal </span>and <span 
class="pcrr7t-x-x-120">copyToLocal</span>, that directly
interact with the HDFS as well as other file systems that
Hadoop supports. Most of the commands in File System Shell
behave like corresponding Unix commands. When the data
files are ready in local file system, the shell is a great tool
to ingest data into HDFS in batch. In order to stream
data into Hadoop for real time analytics, however, we
need more advanced tools, e.g. Apache Flume and Apache
                                                          
                                                          
Chukwa.
<!--l. 354--><p class="noindent" >
   <h5 class="subsubsectionHead"><span class="titlemark">3.1.7.2   </span> <a 
 id="x1-240003.1.7.2"></a>Streaming Data Ingestion</h5>
<!--l. 355--><p class="noindent" >Apache Flume<a 
 id="dx1-24001"></a> <span class="cite">[<a 
href="#XFlume">9</a>]</span> is a distributed, reliable, and available service
for efficiently collecting, aggregating, and moving large amounts
of log data into HDFS. It has a simple and flexible architecture
based on streaming data flows; and robust and fault tolerant
with tunable reliability mechanisms and many failover and
recovery mechanisms. It uses a simple extensible data
model that allows for online analytic application. Flume
employs the familiar producer-consumer model. <span 
class="pcrr7t-x-x-120">Source </span>is
the entity through which data enters into Flume. Sources
either actively poll for data or passively wait for data to be
delivered to them. On the other hand, <span 
class="pcrr7t-x-x-120">Sink </span>is the entity that
delivers the data to the destination. Flume has many built-in
sources (e.g. log4j and syslogs) and sinks (e.g. HDFS and
HBase). <span 
class="pcrr7t-x-x-120">Channel </span>is the conduit between the Source and
the Sink. Sources ingest events into the channel and the
sinks drain the channel. Channels allow decoupling of
ingestion rate from drain rate. When data are generated faster
than what the destination can handle, the channel size
increases.
<!--l. 357--><p class="indent" >   Apache Chukwa<a 
 id="dx1-24002"></a> <span class="cite">[<a 
href="#XChukwa">7</a>]</span> is devoted to large-scale log collection
and analysis, built on top of MapReduce framework. Beyond
data ingestion, Chukwa also includes a flexible and powerful
toolkit for displaying monitoring and analyzing results.
Different from Flume, Chukwa is not a a continuous stream
processing system but a mini-batch system.
<!--l. 359--><p class="indent" >   Apache Kafka<a 
 id="dx1-24003"></a> <span class="cite">[<a 
href="#XKafka">13</a>]</span> and Apache Storm<a 
 id="dx1-24004"></a> <span class="cite">[<a 
href="#XStorm">18</a>]</span> may also be used
                                                          
                                                          
to ingest streaming data into Hadoop although they are mainly
designed to solve different problems. Kafka is a distributed
publish-subscribe messaging system. It is designed to provide
high throughput persistent messaging that&#8217;s scalable and allows
for parallel data loads into Hadoop. Storm is a distributed
realtime computation system for use cases such as realtime
analytics, online machine learning, continuous computation,
etc.
<!--l. 361--><p class="noindent" >
   <h5 class="subsubsectionHead"><span class="titlemark">3.1.7.3   </span> <a 
 id="x1-250003.1.7.3"></a>Structured Data Ingestion</h5>
<!--l. 362--><p class="noindent" >Apache Sqoop<a 
 id="dx1-25001"></a> <span class="cite">[<a 
href="#XSqoop">17</a>]</span> is a tool designed to efficiently transfer data
between Hadoop and relational databases. We can use Sqoop to
import data from a relational database table into HDFS. The
import process is performed in parallel and thus generates
multiple files in the format of delimited text, Avro, or
SequenceFile. Besides, Sqoop generates a Java class that
encapsulates one row of the imported table, which can be used
in subsequent MapReduce processing of the data. Moreover,
Sqoop can export the data (e.g. the results of MapReduce
processing) back to the relational database for consumption by
external applications or users.
<!--l. 365--><p class="noindent" >
   <h3 class="sectionHead"><span class="titlemark">3.2   </span> <a 
 id="x1-260003.2"></a>MapReduce</h3>
<a 
 id="dx1-26001"></a>
<!--l. 366--><p class="noindent" >Distributed parallel computing is not new. Supercomputers
have been using MPI<a 
 id="dx1-26002"></a> <span class="cite">[<a 
href="#XForum:1994:MMI">34</a>]</span> for years for complex numerical
                                                          
                                                          
computing. Although MPI provides a comprehensive API for
data transfer and synchronization, it is not very suitable for
big data. Due to the large data size and shared-nothing
architecture for scalability, data distribution and I/O are
critical to big data analytics while MPI almost ignores it
<span class="footnote-mark"><a 
href="bigdata7.html#fn4x4"><sup class="textsuperscript">4</sup></a></span><a 
 id="x1-26003f4"></a>.
On the other hand, many big data analytics are conceptually
straightforward and does not need very complicated
communication and synchronization mechanism. Based on these
observations, Google invented MapReduce <span class="cite">[<a 
href="#XDean:2008:MSD">30</a>]</span> to deal the
issues of how to parallelize the computation, distribute the
data, and handle failures.
   <h4 class="subsectionHead"><span class="titlemark">3.2.1   </span> <a 
 id="x1-270003.2.1"></a>Overview</h4>
<!--l. 369--><p class="noindent" >In a shared-nothing distributed computing environment, a
computation is much more efficient if it is executed near the
data it operates on. This is especially true when the size of the
data set is huge as it minimizes network traffic and increases
the overall throughput of the system. Therefore, it is often
better to migrate the computation closer to where the data is
located rather than moving the data to where the application is
running. With GFS/HDFS, MapReduce provides such a parallel
programming framework.
<!--l. 371--><p class="indent" >   Inspired by the <span 
class="pcrr7t-x-x-120">map </span>and
                                                          
                                                          
<span 
class="pcrr7t-x-x-120">reduce</span><span class="footnote-mark"><a 
href="bigdata8.html#fn5x4"><sup class="textsuperscript">5</sup></a></span><a 
 id="x1-27001f5"></a> 
functions commonly used in functional programming, a
MapReduce program is composed of a Map()<a 
 id="dx1-27002"></a> procedure that
performs transformation and a Reduce()<a 
 id="dx1-27003"></a> procedure that
takes the shuffled output of Map as input and performs a
summarization operation. More specifically, the user-defined
Map function processes a key-value pair to generate a set of
intermediate key-value pairs, and the Reduce function
aggregates all intermediate values associated with the same
intermediate key.
<!--l. 373--><p class="indent" >   MapReduce applications are automatically parallelized and
executed on a large cluster of commodity machines. During the
execution, the Map invocations are distributed across multiple
machines by automatically partitioning the input data into
a set of M splits. The input splits can be processed in
parallel by different machines. Reduce invocations are
distributed by partitioning the intermediate key space into
R pieces using a partitioning function. The number of
partitions and the partitioning function are specified by the
user. Besides partitioning the input data and running the
various tasks in parallel, the framework also manages all
communications and data transfers, load balance, and fault
tolerance.
<!--l. 375--><p class="indent" >   MapReduce provides programmers a really simple parallel
computing paradigm. Because of automatic parallelization, no
explicit handling of data transfer and synchronization in
programs, and no deadlock, this model is very attractive.
MapReduce is also designed to process very large data that is
too big to fit into the memory (combined from all nodes). To
                                                          
                                                          
achieve that, MapReduce employs a data flow model, which also
provides a simple I/O interface to access large amount of data
in distributed file system. It also exploits data locality for
efficiency. In most cases, we do not need to worry about I/O at
all.
   <h4 class="subsectionHead"><span class="titlemark">3.2.2   </span> <a 
 id="x1-280003.2.2"></a>Data Flow</h4>
<!--l. 378--><p class="noindent" ><hr class="figure"><div class="figure" 
>
                                                          
                                                          
<a 
 id="x1-280013"></a>
                                                          
                                                          

<!--l. 379--><p class="noindent" ><img 
src="images/MapReduce.png" alt="PIC"  
>
<br /> <div class="caption" 
><span class="id">Figure&#x00A0;3.3: </span><span  
class="content">MapReduce Data Flow</span></div><!--tex4ht:label?: x1-280013 -->
                                                          
                                                          
<!--l. 383--><p class="noindent" ></div><hr class="endfigure">
<!--l. 385--><p class="indent" >   For a given task, the MapReduce system runs as follows
      <dl class="description"><dt class="description">
<span 
class="cmbx-12">Prepare the Map() input</span> </dt><dd 
class="description">The  system  splits  the  input
      files into M pieces and then starts up M Map workers
      on a cluster of machines.
      </dd><dt class="description">
<span 
class="cmbx-12">Run the user-defined Map() code</span> </dt><dd 
class="description">The   Map   worker
      parses  key-value  pairs  out  of  the  assigned  split
      and   passes   each   pair   to   the   user-defined   Map
      function. The intermediate key-value pairs produced
      by   the   Map   function   are   buffered   in   memory.
      Periodically,  the  buffered  pairs  are  written  to  local
      disk, partitioned into R regions for sharding purposes
      by the partitioning function (called partitioner<a 
 id="dx1-28002"></a>) that
      is given the key and the number of reducers R and
      returns the index of the desired reducer.
      </dd><dt class="description">
<span 
class="cmbx-12">Shuffle the Map output to the Reduce processors</span> </dt><dd 
class="description">
      <a 
 id="dx1-28003"></a>When  ready,  a  reduce  worker  reads  remotely  the
      buffered data from the local disks of the map workers.
      When a reduce worker has read all intermediate data,
      it  sorts  the  data  by  the  intermediate  keys  so  that
      all occurrences of the same key are grouped together.
      Typically many different keys map to the same reduce
      task.
      </dd><dt class="description">
<span 
class="cmbx-12">Run the user-defined Reduce() code</span> </dt><dd 
class="description">The        reduce
      worker  iterates  over  the  sorted  intermediate  data
                                                          
                                                          
      and  for  each  unique  intermediate  key  encountered,
      it  passes  the  key  and  the  corresponding  set  of
      intermediate values to the user&#8217;s Reduce function.
      </dd><dt class="description">
<span 
class="cmbx-12">Produce the final output</span> </dt><dd 
class="description">The final output is available
      in the R output files (one per reduce task).</dd></dl>
<!--l. 398--><p class="noindent" >Optionally, a combiner<a 
 id="dx1-28004"></a> can be used between map and reduce
as an optimization. The combiner function runs on the
output of the map phase and is used as a filtering or an
aggregating step to lessen the data that are being passed to the
reducer. In most of the cases the reducer class is set to be the
combiner class so that we can save network time. Note that
this works only if reduce function is commutative and
associative.
<!--l. 400--><p class="indent" >   In practice, one should pay attention to the task granularity,
i.e. the number of map tasks M and the number of reduce tasks
R. In general, M should be much larger than the number of
nodes in cluster, which improves load balancing and speeds
recovery from worker failure. The right level of parallelism for
maps seems to be around 10-100 maps per node (maybe more
for very cpu light map tasks). Besides, the task setup takes
awhile. On a Hadoop cluster of 100 nodes, it takes 25 seconds
until all nodes are executing the job. So it is best if the maps
take at least a minute to execute. In Hadoop, one can call
<span 
class="pcrr7t-x-x-120">JobConf.setNumMapTasks(int) </span>to set the number
of map tasks. Note that it only provides a hint to the
framework.
<!--l. 402--><p class="indent" >   The number of reducers is usually a small multiple
of the number of nodes. The right factor number seems
to be 0.95 for well-balanced data (per intermediate key)
                                                          
                                                          
or 1.75 otherwise for better load balancing. Note that
we reserve a few reduce slots for speculative tasks and
failed tasks. We can set the number of reduce tasks by
<span 
class="pcrr7t-x-x-120">JobConf.setNumReduceTasks(int) </span>in Hadoop and the
framework will honor it. It is fine to set R to zero if no
reduction is desired.
   <h4 class="subsectionHead"><span class="titlemark">3.2.3   </span> <a 
 id="x1-290003.2.3"></a>Secondary Sorting</h4>
<!--l. 405--><p class="noindent" >The output of Mappers is firstly sorted by the intermediate
keys. However, we do want to sort the intermediate values (or
some fields of intermediate values) sometimes, e.g. calculating
the stock price moving average where the key is the stock ticker
and the value is a pair of timestamp and stock price. If the
values of a given key are sorted by the timestamp, we
can easily calculate the moving average with a sliding
window over the values. This problem is called secondary
sorting.
<!--l. 407--><p class="indent" >   A direct approach to secondary sorting is for the reducer to
buffer all of the values for a given key and do an in-memory
sort. Unfortunately, it may cause the reducer to run out of
memory.
<!--l. 409--><p class="indent" >   Alternatively, we may use a composite key that has
multiple parts. In the case of calculating moving average, we
may create a composite key of (ticker, timestamp) and
also provide a customized sort comparator (subclass of
<span 
class="pcrr7t-x-x-120">WritableComparator</span>) that compares ticker and then
timestamp. To ensure only the ticker (referred as natural
key) is considered when determining which reducer to
send the data to, we need to write a custom partitioner
(subclass of <span 
class="pcrr7t-x-x-120">Partitioner</span>) that is solely based on the
                                                          
                                                          
natural key. Once the data reaches a reducer, all data is
grouped by key. Since we have a composite key, we need to
make sure records are grouped solely by the natural key by
implementing a group comparator (another subclass of
<span 
class="pcrr7t-x-x-120">WritableComparator</span>) that considers only the natural
key.
<!--l. 413--><p class="noindent" >
   <h4 class="subsectionHead"><span class="titlemark">3.2.4   </span> <a 
 id="x1-300003.2.4"></a>Examples</h4>
<!--l. 414--><p class="noindent" >Hadoop implements MapReduce in Java. To create a MapReduce
program, please add the following dependencies to the project&#8217;s
Maven POM file.
   <!--l. 416-->
<div class="lstlisting" id="listing-4"><span class="label"><a 
 id="x1-30001r1"></a></span><span 
class="pcrr7t-x-x-109">&#x003C;</span><span 
class="pcrr7t-x-x-109">dependency</span><span 
class="pcrr7t-x-x-109">&#x003E;</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><br /><span class="label"><a 
 id="x1-30002r2"></a></span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><span 
class="pcrr7t-x-x-109">&#x003C;</span><span 
class="pcrr7t-x-x-109">groupId</span><span 
class="pcrr7t-x-x-109">&#x003E;</span><span 
class="pcrr7t-x-x-109">org</span><span 
class="pcrr7t-x-x-109">.</span><span 
class="pcrr7t-x-x-109">apache</span><span 
class="pcrr7t-x-x-109">.</span><span 
class="pcrr7t-x-x-109">hadoop</span><span 
class="pcrr7t-x-x-109">&#x003C;/</span><span 
class="pcrr7t-x-x-109">groupId</span><span 
class="pcrr7t-x-x-109">&#x003E;</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><br /><span class="label"><a 
 id="x1-30003r3"></a></span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><span 
class="pcrr7t-x-x-109">&#x003C;</span><span 
class="pcrr7t-x-x-109">artifactId</span><span 
class="pcrr7t-x-x-109">&#x003E;</span><span 
class="pcrr7t-x-x-109">hadoop</span><span 
class="pcrr7t-x-x-109">-</span><span 
class="pcrr7t-x-x-109">common</span><span 
class="pcrr7t-x-x-109">&#x003C;/</span><span 
class="pcrr7t-x-x-109">artifactId</span><span 
class="pcrr7t-x-x-109">&#x003E;</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><br /><span class="label"><a 
 id="x1-30004r4"></a></span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><span 
class="pcrr7t-x-x-109">&#x003C;</span><span 
class="pcrr7t-x-x-109">version</span><span 
class="pcrr7t-x-x-109">&#x003E;2.6.0&#x003C;/</span><span 
class="pcrr7t-x-x-109">version</span><span 
class="pcrr7t-x-x-109">&#x003E;</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><br /><span class="label"><a 
 id="x1-30005r5"></a></span><span 
class="pcrr7t-x-x-109">&#x003C;/</span><span 
class="pcrr7t-x-x-109">dependency</span><span 
class="pcrr7t-x-x-109">&#x003E;</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><br /><span class="label"><a 
 id="x1-30006r6"></a></span><span 
class="pcrr7t-x-x-109">&#x003C;</span><span 
class="pcrr7t-x-x-109">dependency</span><span 
class="pcrr7t-x-x-109">&#x003E;</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><br /><span class="label"><a 
 id="x1-30007r7"></a></span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><span 
class="pcrr7t-x-x-109">&#x003C;</span><span 
class="pcrr7t-x-x-109">groupId</span><span 
class="pcrr7t-x-x-109">&#x003E;</span><span 
class="pcrr7t-x-x-109">org</span><span 
class="pcrr7t-x-x-109">.</span><span 
class="pcrr7t-x-x-109">apache</span><span 
class="pcrr7t-x-x-109">.</span><span 
class="pcrr7t-x-x-109">hadoop</span><span 
class="pcrr7t-x-x-109">&#x003C;/</span><span 
class="pcrr7t-x-x-109">groupId</span><span 
class="pcrr7t-x-x-109">&#x003E;</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><br /><span class="label"><a 
 id="x1-30008r8"></a></span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><span 
class="pcrr7t-x-x-109">&#x003C;</span><span 
class="pcrr7t-x-x-109">artifactId</span><span 
class="pcrr7t-x-x-109">&#x003E;</span><span 
class="pcrr7t-x-x-109">hadoop</span><span 
class="pcrr7t-x-x-109">-</span><span 
class="pcrr7t-x-x-109">mapreduce</span><span 
class="pcrr7t-x-x-109">-</span><span 
class="pcrr7t-x-x-109">client</span><span 
class="pcrr7t-x-x-109">-</span><span 
class="pcrr7t-x-x-109">core</span><span 
class="pcrr7t-x-x-109">&#x003C;/</span><span 
class="pcrr7t-x-x-109">artifactId</span><span 
class="pcrr7t-x-x-109">&#x003E;</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><br /><span class="label"><a 
 id="x1-30009r9"></a></span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><span 
class="pcrr7t-x-x-109">&#x003C;</span><span 
class="pcrr7t-x-x-109">version</span><span 
class="pcrr7t-x-x-109">&#x003E;2.6.0&#x003C;/</span><span 
class="pcrr7t-x-x-109">version</span><span 
class="pcrr7t-x-x-109">&#x003E;</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><br /><span class="label"><a 
 id="x1-30010r10"></a></span><span 
class="pcrr7t-x-x-109">&#x003C;/</span><span 
class="pcrr7t-x-x-109">dependency</span><span 
class="pcrr7t-x-x-109">&#x003E;</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><br /><span class="label"><a 
 id="x1-30011r11"></a></span><span 
class="pcrr7t-x-x-109">&#x003C;</span><span 
class="pcrr7t-x-x-109">dependency</span><span 
class="pcrr7t-x-x-109">&#x003E;</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><br /><span class="label"><a 
 id="x1-30012r12"></a></span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><span 
class="pcrr7t-x-x-109">&#x003C;</span><span 
class="pcrr7t-x-x-109">groupId</span><span 
class="pcrr7t-x-x-109">&#x003E;</span><span 
class="pcrr7t-x-x-109">org</span><span 
class="pcrr7t-x-x-109">.</span><span 
class="pcrr7t-x-x-109">apache</span><span 
class="pcrr7t-x-x-109">.</span><span 
class="pcrr7t-x-x-109">hadoop</span><span 
class="pcrr7t-x-x-109">&#x003C;/</span><span 
class="pcrr7t-x-x-109">groupId</span><span 
class="pcrr7t-x-x-109">&#x003E;</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><br /><span class="label"><a 
 id="x1-30013r13"></a></span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><span 
class="pcrr7t-x-x-109">&#x003C;</span><span 
class="pcrr7t-x-x-109">artifactId</span><span 
class="pcrr7t-x-x-109">&#x003E;</span><span 
class="pcrr7t-x-x-109">hadoop</span><span 
class="pcrr7t-x-x-109">-</span><span 
class="pcrr7t-x-x-109">hdfs</span><span 
class="pcrr7t-x-x-109">&#x003C;/</span><span 
class="pcrr7t-x-x-109">artifactId</span><span 
class="pcrr7t-x-x-109">&#x003E;</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><br /><span class="label"><a 
 id="x1-30014r14"></a></span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><span 
class="pcrr7t-x-x-109">&#x003C;</span><span 
class="pcrr7t-x-x-109">version</span><span 
class="pcrr7t-x-x-109">&#x003E;2.6.0&#x003C;/</span><span 
class="pcrr7t-x-x-109">version</span><span 
class="pcrr7t-x-x-109">&#x003E;</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><br /><span class="label"><a 
 id="x1-30015r15"></a></span><span 
class="pcrr7t-x-x-109">&#x003C;/</span><span 
class="pcrr7t-x-x-109">dependency</span><span 
class="pcrr7t-x-x-109">&#x003E;</span>
   </div>
<!--l. 434--><p class="noindent" >
   <h5 class="subsubsectionHead"><span class="titlemark">3.2.4.1   </span> <a 
 id="x1-310003.2.4.1"></a>Sort</h5>
<!--l. 436--><p class="noindent" >The essential part of the MapReduce framework is a large
distributed sort. So we just let the framework do the job in this
case while the map is as simple as emitting the sort key and
original input. In the below example, we just assume the input
key is the sort key. The reduce operator is an identity
function.
   <!--l. 438-->
<div class="lstlisting" id="listing-5"><span class="label"><a 
 id="x1-31001r1"></a></span><span 
class="pcrr7t-x-x-109">public</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><span 
class="pcrr7t-x-x-109">class</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><span 
class="pcrr7t-x-x-109">SortMapper</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><span 
class="pcrr7t-x-x-109">extends</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><span 
class="pcrr7t-x-x-109">Mapper</span><span 
class="pcrr7t-x-x-109">&#x003C;</span><span 
class="pcrr7t-x-x-109">IntWritable</span><span 
class="pcrr7t-x-x-109">,</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><span 
class="pcrr7t-x-x-109">Text</span><span 
class="pcrr7t-x-x-109">,</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><span 
class="pcrr7t-x-x-109">IntWritable</span><span 
class="pcrr7t-x-x-109">,</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><span 
class="pcrr7t-x-x-109">Text</span><span 
class="pcrr7t-x-x-109">&#x003E;</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><span 
class="pcrr7t-x-x-109">{</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><br /><span class="label"><a 
 id="x1-31002r2"></a></span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><br /><span class="label"><a 
 id="x1-31003r3"></a></span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><span 
class="pcrr7t-x-x-109">public</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><span 
class="pcrr7t-x-x-109">void</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><span 
class="pcrr7t-x-x-109">map</span><span 
class="pcrr7t-x-x-109">(</span><span 
class="pcrr7t-x-x-109">IntWritable</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><span 
class="pcrr7t-x-x-109">key</span><span 
class="pcrr7t-x-x-109">,</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><span 
class="pcrr7t-x-x-109">Text</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><span 
class="pcrr7t-x-x-109">value</span><span 
class="pcrr7t-x-x-109">,</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><span 
class="pcrr7t-x-x-109">Context</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><span 
class="pcrr7t-x-x-109">context</span><span 
class="pcrr7t-x-x-109">)</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><br /><span class="label"><a 
 id="x1-31004r4"></a></span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><span 
class="pcrr7t-x-x-109">throws</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><span 
class="pcrr7t-x-x-109">IOException</span><span 
class="pcrr7t-x-x-109">,</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><span 
class="pcrr7t-x-x-109">InterruptedException</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><span 
class="pcrr7t-x-x-109">{</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><br /><span class="label"><a 
 id="x1-31005r5"></a></span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><span 
class="pcrr7t-x-x-109">context</span><span 
class="pcrr7t-x-x-109">.</span><span 
class="pcrr7t-x-x-109">write</span><span 
class="pcrr7t-x-x-109">(</span><span 
class="pcrr7t-x-x-109">key</span><span 
class="pcrr7t-x-x-109">,</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><span 
class="pcrr7t-x-x-109">value</span><span 
class="pcrr7t-x-x-109">)</span><span 
class="pcrr7t-x-x-109">;</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><br /><span class="label"><a 
 id="x1-31006r6"></a></span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><span 
class="pcrr7t-x-x-109">}</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><br /><span class="label"><a 
 id="x1-31007r7"></a></span><span 
class="pcrr7t-x-x-109">}</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><br /><span class="label"><a 
 id="x1-31008r8"></a></span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><br /><span class="label"><a 
 id="x1-31009r9"></a></span><span 
class="pcrr7t-x-x-109">public</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><span 
class="pcrr7t-x-x-109">class</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><span 
class="pcrr7t-x-x-109">SortReducer</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><span 
class="pcrr7t-x-x-109">extends</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><span 
class="pcrr7t-x-x-109">Reducer</span><span 
class="pcrr7t-x-x-109">&#x003C;</span><span 
class="pcrr7t-x-x-109">IntWritable</span><span 
class="pcrr7t-x-x-109">,</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><span 
class="pcrr7t-x-x-109">Text</span><span 
class="pcrr7t-x-x-109">,</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><span 
class="pcrr7t-x-x-109">IntWritable</span><span 
class="pcrr7t-x-x-109">,</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><span 
class="pcrr7t-x-x-109">Text</span><span 
class="pcrr7t-x-x-109">&#x003E;</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><span 
class="pcrr7t-x-x-109">{</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><br /><span class="label"><a 
 id="x1-31010r10"></a></span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><br /><span class="label"><a 
 id="x1-31011r11"></a></span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><span 
class="pcrr7t-x-x-109">public</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><span 
class="pcrr7t-x-x-109">void</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><span 
class="pcrr7t-x-x-109">reduce</span><span 
class="pcrr7t-x-x-109">(</span><span 
class="pcrr7t-x-x-109">IntWritable</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><span 
class="pcrr7t-x-x-109">key</span><span 
class="pcrr7t-x-x-109">,</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><span 
class="pcrr7t-x-x-109">Iterable</span><span 
class="pcrr7t-x-x-109">&#x003C;</span><span 
class="pcrr7t-x-x-109">Text</span><span 
class="pcrr7t-x-x-109">&#x003E;</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><span 
class="pcrr7t-x-x-109">values</span><span 
class="pcrr7t-x-x-109">,</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><span 
class="pcrr7t-x-x-109">Context</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><span 
class="pcrr7t-x-x-109">context</span><span 
class="pcrr7t-x-x-109">)</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><br /><span class="label"><a 
 id="x1-31012r12"></a></span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><span 
class="pcrr7t-x-x-109">throws</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><span 
class="pcrr7t-x-x-109">IOException</span><span 
class="pcrr7t-x-x-109">,</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><span 
class="pcrr7t-x-x-109">InterruptedException</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><span 
class="pcrr7t-x-x-109">{</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><br /><span class="label"><a 
 id="x1-31013r13"></a></span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><span 
class="pcrr7t-x-x-109">for</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><span 
class="pcrr7t-x-x-109">(</span><span 
class="pcrr7t-x-x-109">Text</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><span 
class="pcrr7t-x-x-109">value</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><span 
class="pcrr7t-x-x-109">:</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><span 
class="pcrr7t-x-x-109">values</span><span 
class="pcrr7t-x-x-109">)</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><span 
class="pcrr7t-x-x-109">{</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><br /><span class="label"><a 
 id="x1-31014r14"></a></span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><span 
class="pcrr7t-x-x-109">context</span><span 
class="pcrr7t-x-x-109">.</span><span 
class="pcrr7t-x-x-109">write</span><span 
class="pcrr7t-x-x-109">(</span><span 
class="pcrr7t-x-x-109">key</span><span 
class="pcrr7t-x-x-109">,</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><span 
class="pcrr7t-x-x-109">value</span><span 
class="pcrr7t-x-x-109">)</span><span 
class="pcrr7t-x-x-109">;</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><br /><span class="label"><a 
 id="x1-31015r15"></a></span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><span 
class="pcrr7t-x-x-109">}</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><br /><span class="label"><a 
 id="x1-31016r16"></a></span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><span 
class="pcrr7t-x-x-109">}</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><br /><span class="label"><a 
 id="x1-31017r17"></a></span><span 
class="pcrr7t-x-x-109">}</span>
                                                          
                                                          
   </div>
<!--l. 458--><p class="indent" >   Although this example is extremely simple, there are
many important classes to understand. The MapReduce
framework takes key-value pairs as the input and produces
a new set of key-value pairs (maybe of different types).
The key and value classes have to be serializable by the
framework and hence need to implement the <span 
class="pcrr7t-x-x-120">Writable</span>
interface. Additionally, the key classes have to implement the
<span 
class="pcrr7t-x-x-120">WritableComparable </span>interface to facilitate sorting by the
framework.
<!--l. 461--><p class="indent" >   The <span 
class="pcrr7t-x-x-120">map </span>method of <span 
class="pcrr7t-x-x-120">Mapper </span>implementation processes one
key-value pair in the input split at a time. The <span 
class="pcrr7t-x-x-120">reduce</span>
method of <span 
class="pcrr7t-x-x-120">Reducer </span>implementation is called once for each
intermediate key and associate group of values. In this case, we
do not have to override the <span 
class="pcrr7t-x-x-120">map </span>and <span 
class="pcrr7t-x-x-120">reduce </span>methods because
the default implementation is actually an identity function. The
sample code is mainly to show the interface. Both <span 
class="pcrr7t-x-x-120">Mapper </span>and
<span 
class="pcrr7t-x-x-120">Reducer </span>emit their output through the <span 
class="pcrr7t-x-x-120">Context </span>object
provided by the framework.
<!--l. 463--><p class="indent" >   To submit a MapReduce job to Hadoop, we need to do the
below steps. First, the application describes various facets of
the job via <span 
class="pcrr7t-x-x-120">Job </span>object. <span 
class="pcrr7t-x-x-120">Job </span>is typically used to specify the
<span 
class="pcrr7t-x-x-120">Mapper</span>, <span 
class="pcrr7t-x-x-120">Reducer</span>, <span 
class="pcrr7t-x-x-120">InputFormat</span>, <span 
class="pcrr7t-x-x-120">OutputFormat</span>
implementations, the directories of input files and the location
of output files. Optionally, one may specify advanced facets of
the job such as the Combiner, Partitioner, Comparator, and
DistributedCache, etc. Then the application submits the job to
the cluster by the method <span 
class="pcrr7t-x-x-120">waitForCompletion(boolean</span>
<span 
class="pcrr7t-x-x-120">verbose) </span>and wait for it to finish. <span 
class="pcrr7t-x-x-120">Job </span>also allows the user to
                                                          
                                                          
control the execution and query the state. <!--l. 466-->
<div class="lstlisting" id="listing-6"><span class="label"><a 
 id="x1-31018r1"></a></span><span 
class="pcrr7t-x-x-109">public</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><span 
class="pcrr7t-x-x-109">class</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><span 
class="pcrr7t-x-x-109">MapReduceSort</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><span 
class="pcrr7t-x-x-109">{</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><br /><span class="label"><a 
 id="x1-31019r2"></a></span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><span 
class="pcrr7t-x-x-109">public</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><span 
class="pcrr7t-x-x-109">static</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><span 
class="pcrr7t-x-x-109">void</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><span 
class="pcrr7t-x-x-109">main</span><span 
class="pcrr7t-x-x-109">(</span><span 
class="pcrr7t-x-x-109">String</span><span 
class="pcrr7t-x-x-109">[]</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><span 
class="pcrr7t-x-x-109">args</span><span 
class="pcrr7t-x-x-109">)</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><span 
class="pcrr7t-x-x-109">throws</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><span 
class="pcrr7t-x-x-109">Exception</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><span 
class="pcrr7t-x-x-109">{</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><br /><span class="label"><a 
 id="x1-31020r3"></a></span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><span 
class="pcrr7t-x-x-109">Configuration</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><span 
class="pcrr7t-x-x-109">conf</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><span 
class="pcrr7t-x-x-109">=</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><span 
class="pcrr7t-x-x-109">new</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><span 
class="pcrr7t-x-x-109">Configuration</span><span 
class="pcrr7t-x-x-109">()</span><span 
class="pcrr7t-x-x-109">;</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><br /><span class="label"><a 
 id="x1-31021r4"></a></span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><span 
class="pcrr7t-x-x-109">Job</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><span 
class="pcrr7t-x-x-109">job</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><span 
class="pcrr7t-x-x-109">=</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><span 
class="pcrr7t-x-x-109">Job</span><span 
class="pcrr7t-x-x-109">.</span><span 
class="pcrr7t-x-x-109">getInstance</span><span 
class="pcrr7t-x-x-109">(</span><span 
class="pcrr7t-x-x-109">conf</span><span 
class="pcrr7t-x-x-109">,</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><span 
class="pcrr7t-x-x-109">"</span><span 
class="pcrr7t-x-x-109">sort</span><span 
class="pcrr7t-x-x-109">"</span><span 
class="pcrr7t-x-x-109">)</span><span 
class="pcrr7t-x-x-109">;</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><br /><span class="label"><a 
 id="x1-31022r5"></a></span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><span 
class="pcrr7t-x-x-109">job</span><span 
class="pcrr7t-x-x-109">.</span><span 
class="pcrr7t-x-x-109">setJarByClass</span><span 
class="pcrr7t-x-x-109">(</span><span 
class="pcrr7t-x-x-109">MapReduceSort</span><span 
class="pcrr7t-x-x-109">.</span><span 
class="pcrr7t-x-x-109">class</span><span 
class="pcrr7t-x-x-109">)</span><span 
class="pcrr7t-x-x-109">;</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><br /><span class="label"><a 
 id="x1-31023r6"></a></span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><span 
class="pcrr7t-x-x-109">job</span><span 
class="pcrr7t-x-x-109">.</span><span 
class="pcrr7t-x-x-109">setMapperClass</span><span 
class="pcrr7t-x-x-109">(</span><span 
class="pcrr7t-x-x-109">SortMapper</span><span 
class="pcrr7t-x-x-109">.</span><span 
class="pcrr7t-x-x-109">class</span><span 
class="pcrr7t-x-x-109">)</span><span 
class="pcrr7t-x-x-109">;</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><br /><span class="label"><a 
 id="x1-31024r7"></a></span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><span 
class="pcrr7t-x-x-109">job</span><span 
class="pcrr7t-x-x-109">.</span><span 
class="pcrr7t-x-x-109">setReducerClass</span><span 
class="pcrr7t-x-x-109">(</span><span 
class="pcrr7t-x-x-109">SortReducer</span><span 
class="pcrr7t-x-x-109">.</span><span 
class="pcrr7t-x-x-109">class</span><span 
class="pcrr7t-x-x-109">)</span><span 
class="pcrr7t-x-x-109">;</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><br /><span class="label"><a 
 id="x1-31025r8"></a></span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><span 
class="pcrr7t-x-x-109">job</span><span 
class="pcrr7t-x-x-109">.</span><span 
class="pcrr7t-x-x-109">setOutputKeyClass</span><span 
class="pcrr7t-x-x-109">(</span><span 
class="pcrr7t-x-x-109">IntWritable</span><span 
class="pcrr7t-x-x-109">.</span><span 
class="pcrr7t-x-x-109">class</span><span 
class="pcrr7t-x-x-109">)</span><span 
class="pcrr7t-x-x-109">;</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><br /><span class="label"><a 
 id="x1-31026r9"></a></span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><span 
class="pcrr7t-x-x-109">job</span><span 
class="pcrr7t-x-x-109">.</span><span 
class="pcrr7t-x-x-109">setOutputValueClass</span><span 
class="pcrr7t-x-x-109">(</span><span 
class="pcrr7t-x-x-109">Text</span><span 
class="pcrr7t-x-x-109">.</span><span 
class="pcrr7t-x-x-109">class</span><span 
class="pcrr7t-x-x-109">)</span><span 
class="pcrr7t-x-x-109">;</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><br /><span class="label"><a 
 id="x1-31027r10"></a></span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><br /><span class="label"><a 
 id="x1-31028r11"></a></span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><span 
class="pcrr7t-x-x-109">FileInputFormat</span><span 
class="pcrr7t-x-x-109">.</span><span 
class="pcrr7t-x-x-109">addInputPath</span><span 
class="pcrr7t-x-x-109">(</span><span 
class="pcrr7t-x-x-109">job</span><span 
class="pcrr7t-x-x-109">,</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><span 
class="pcrr7t-x-x-109">new</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><span 
class="pcrr7t-x-x-109">Path</span><span 
class="pcrr7t-x-x-109">(</span><span 
class="pcrr7t-x-x-109">args</span><span 
class="pcrr7t-x-x-109">[0])</span><span 
class="pcrr7t-x-x-109">)</span><span 
class="pcrr7t-x-x-109">;</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><br /><span class="label"><a 
 id="x1-31029r12"></a></span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><span 
class="pcrr7t-x-x-109">FileOutputFormat</span><span 
class="pcrr7t-x-x-109">.</span><span 
class="pcrr7t-x-x-109">setOutputPath</span><span 
class="pcrr7t-x-x-109">(</span><span 
class="pcrr7t-x-x-109">job</span><span 
class="pcrr7t-x-x-109">,</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><span 
class="pcrr7t-x-x-109">new</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><span 
class="pcrr7t-x-x-109">Path</span><span 
class="pcrr7t-x-x-109">(</span><span 
class="pcrr7t-x-x-109">args</span><span 
class="pcrr7t-x-x-109">[1])</span><span 
class="pcrr7t-x-x-109">)</span><span 
class="pcrr7t-x-x-109">;</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><br /><span class="label"><a 
 id="x1-31030r13"></a></span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><br /><span class="label"><a 
 id="x1-31031r14"></a></span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><span 
class="pcrr7t-x-x-109">System</span><span 
class="pcrr7t-x-x-109">.</span><span 
class="pcrr7t-x-x-109">exit</span><span 
class="pcrr7t-x-x-109">(</span><span 
class="pcrr7t-x-x-109">job</span><span 
class="pcrr7t-x-x-109">.</span><span 
class="pcrr7t-x-x-109">waitForCompletion</span><span 
class="pcrr7t-x-x-109">(</span><span 
class="pcrr7t-x-x-109">true</span><span 
class="pcrr7t-x-x-109">)</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><span 
class="pcrr7t-x-x-109">?</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><span 
class="pcrr7t-x-x-109">0</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><span 
class="pcrr7t-x-x-109">:</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><span 
class="pcrr7t-x-x-109">1)</span><span 
class="pcrr7t-x-x-109">;</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><br /><span class="label"><a 
 id="x1-31032r15"></a></span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><span 
class="pcrr7t-x-x-109">}</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><br /><span class="label"><a 
 id="x1-31033r16"></a></span><span 
class="pcrr7t-x-x-109">}</span>
   </div>
<!--l. 484--><p class="noindent" >
   <h5 class="subsubsectionHead"><span class="titlemark">3.2.4.2   </span> <a 
 id="x1-320003.2.4.2"></a>Grep</h5>
<!--l. 486--><p class="noindent" >The map function emits a line if it matches a given
pattern. The reduce part is not necessary in this case
and we can simply set the number of reduce tasks zero
(<span 
class="pcrr7t-x-x-120">job.setNumReduceTasks(0)</span>). Note that the <span 
class="pcrr7t-x-x-120">Mapper</span>
implementation also overrides the <span 
class="pcrr7t-x-x-120">setup </span>method, which will be
called once at the beginning of the task. In this case, we use it
to set the search pattern from the job configuration. This is also
a good example of passing small configuration data to
MapReduce tasks. To pass large amount of read-only data to
tasks, DistributedCache is preferred and will be discussed later
in the case of Inner Join. Similar to <span 
class="pcrr7t-x-x-120">setup</span>, one may also
overrides the <span 
class="pcrr7t-x-x-120">cleanup </span>method, which will be called once at
the end of the task.
   <!--l. 488-->
<div class="lstlisting" id="listing-7"><span class="label"><a 
 id="x1-32001r1"></a></span><span 
class="pcrr7t-x-x-109">public</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><span 
class="pcrr7t-x-x-109">class</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><span 
class="pcrr7t-x-x-109">GrepMapper</span><span 
class="pcrr7t-x-x-109">&#x003C;</span><span 
class="pcrr7t-x-x-109">K</span><span 
class="pcrr7t-x-x-109">&#x003E;</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><span 
class="pcrr7t-x-x-109">extends</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><span 
class="pcrr7t-x-x-109">Mapper</span><span 
class="pcrr7t-x-x-109">&#x003C;</span><span 
class="pcrr7t-x-x-109">K</span><span 
class="pcrr7t-x-x-109">,</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><span 
class="pcrr7t-x-x-109">Text</span><span 
class="pcrr7t-x-x-109">,</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><span 
class="pcrr7t-x-x-109">K</span><span 
class="pcrr7t-x-x-109">,</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><span 
class="pcrr7t-x-x-109">Text</span><span 
class="pcrr7t-x-x-109">&#x003E;</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><span 
class="pcrr7t-x-x-109">{</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><br /><span class="label"><a 
 id="x1-32002r2"></a></span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><br /><span class="label"><a 
 id="x1-32003r3"></a></span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><span 
class="pcrr7t-x-x-109">public</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><span 
class="pcrr7t-x-x-109">static</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><span 
class="pcrr7t-x-x-109">String</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><span 
class="pcrr7t-x-x-109">PATTERN</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><span 
class="pcrr7t-x-x-109">=</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><span 
class="pcrr7t-x-x-109">"</span><span 
class="pcrr7t-x-x-109">mapper</span><span 
class="pcrr7t-x-x-109">.</span><span 
class="pcrr7t-x-x-109">pattern</span><span 
class="pcrr7t-x-x-109">"</span><span 
class="pcrr7t-x-x-109">;</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><br /><span class="label"><a 
 id="x1-32004r4"></a></span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><span 
class="pcrr7t-x-x-109">private</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><span 
class="pcrr7t-x-x-109">Pattern</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><span 
class="pcrr7t-x-x-109">pattern</span><span 
class="pcrr7t-x-x-109">;</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><br /><span class="label"><a 
 id="x1-32005r5"></a></span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><br /><span class="label"><a 
 id="x1-32006r6"></a></span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><span 
class="pcrr7t-x-x-109">//</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><span 
class="pcrr7t-x-x-109">Setup</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><span 
class="pcrr7t-x-x-109">the</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><span 
class="pcrr7t-x-x-109">match</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><span 
class="pcrr7t-x-x-109">pattern</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><span 
class="pcrr7t-x-x-109">from</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><span 
class="pcrr7t-x-x-109">job</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><span 
class="pcrr7t-x-x-109">context</span><span 
class="pcrr7t-x-x-109">.</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><br /><span class="label"><a 
 id="x1-32007r7"></a></span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><span 
class="pcrr7t-x-x-109">//</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><span 
class="pcrr7t-x-x-109">Called</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><span 
class="pcrr7t-x-x-109">once</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><span 
class="pcrr7t-x-x-109">at</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><span 
class="pcrr7t-x-x-109">the</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><span 
class="pcrr7t-x-x-109">beginning</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><span 
class="pcrr7t-x-x-109">of</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><span 
class="pcrr7t-x-x-109">the</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><span 
class="pcrr7t-x-x-109">task</span><span 
class="pcrr7t-x-x-109">.</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><br /><span class="label"><a 
 id="x1-32008r8"></a></span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><span 
class="pcrr7t-x-x-109">public</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><span 
class="pcrr7t-x-x-109">void</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><span 
class="pcrr7t-x-x-109">setup</span><span 
class="pcrr7t-x-x-109">(</span><span 
class="pcrr7t-x-x-109">Context</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><span 
class="pcrr7t-x-x-109">context</span><span 
class="pcrr7t-x-x-109">)</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><span 
class="pcrr7t-x-x-109">{</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><br /><span class="label"><a 
 id="x1-32009r9"></a></span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><span 
class="pcrr7t-x-x-109">Configuration</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><span 
class="pcrr7t-x-x-109">conf</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><span 
class="pcrr7t-x-x-109">=</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><span 
class="pcrr7t-x-x-109">context</span><span 
class="pcrr7t-x-x-109">.</span><span 
class="pcrr7t-x-x-109">getConfiguration</span><span 
class="pcrr7t-x-x-109">()</span><span 
class="pcrr7t-x-x-109">;</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><br /><span class="label"><a 
 id="x1-32010r10"></a></span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><span 
class="pcrr7t-x-x-109">pattern</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><span 
class="pcrr7t-x-x-109">=</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><span 
class="pcrr7t-x-x-109">Pattern</span><span 
class="pcrr7t-x-x-109">.</span><span 
class="pcrr7t-x-x-109">compile</span><span 
class="pcrr7t-x-x-109">(</span><span 
class="pcrr7t-x-x-109">conf</span><span 
class="pcrr7t-x-x-109">.</span><span 
class="pcrr7t-x-x-109">get</span><span 
class="pcrr7t-x-x-109">(</span><span 
class="pcrr7t-x-x-109">PATTERN</span><span 
class="pcrr7t-x-x-109">)</span><span 
class="pcrr7t-x-x-109">)</span><span 
class="pcrr7t-x-x-109">;</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><br /><span class="label"><a 
 id="x1-32011r11"></a></span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><span 
class="pcrr7t-x-x-109">}</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><br /><span class="label"><a 
 id="x1-32012r12"></a></span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><br /><span class="label"><a 
 id="x1-32013r13"></a></span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><span 
class="pcrr7t-x-x-109">public</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><span 
class="pcrr7t-x-x-109">void</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><span 
class="pcrr7t-x-x-109">map</span><span 
class="pcrr7t-x-x-109">(</span><span 
class="pcrr7t-x-x-109">K</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><span 
class="pcrr7t-x-x-109">key</span><span 
class="pcrr7t-x-x-109">,</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><span 
class="pcrr7t-x-x-109">Text</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><span 
class="pcrr7t-x-x-109">value</span><span 
class="pcrr7t-x-x-109">,</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><span 
class="pcrr7t-x-x-109">Context</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><span 
class="pcrr7t-x-x-109">context</span><span 
class="pcrr7t-x-x-109">)</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><br /><span class="label"><a 
 id="x1-32014r14"></a></span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><span 
class="pcrr7t-x-x-109">throws</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><span 
class="pcrr7t-x-x-109">IOException</span><span 
class="pcrr7t-x-x-109">,</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><span 
class="pcrr7t-x-x-109">InterruptedException</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><span 
class="pcrr7t-x-x-109">{</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><br /><span class="label"><a 
 id="x1-32015r15"></a></span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><span 
class="pcrr7t-x-x-109">if</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><span 
class="pcrr7t-x-x-109">(</span><span 
class="pcrr7t-x-x-109">pattern</span><span 
class="pcrr7t-x-x-109">.</span><span 
class="pcrr7t-x-x-109">matcher</span><span 
class="pcrr7t-x-x-109">(</span><span 
class="pcrr7t-x-x-109">value</span><span 
class="pcrr7t-x-x-109">.</span><span 
class="pcrr7t-x-x-109">toString</span><span 
class="pcrr7t-x-x-109">()</span><span 
class="pcrr7t-x-x-109">)</span><span 
class="pcrr7t-x-x-109">.</span><span 
class="pcrr7t-x-x-109">find</span><span 
class="pcrr7t-x-x-109">()</span><span 
class="pcrr7t-x-x-109">)</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><span 
class="pcrr7t-x-x-109">{</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><br /><span class="label"><a 
 id="x1-32016r16"></a></span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><span 
class="pcrr7t-x-x-109">context</span><span 
class="pcrr7t-x-x-109">.</span><span 
class="pcrr7t-x-x-109">write</span><span 
class="pcrr7t-x-x-109">(</span><span 
class="pcrr7t-x-x-109">key</span><span 
class="pcrr7t-x-x-109">,</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><span 
class="pcrr7t-x-x-109">value</span><span 
class="pcrr7t-x-x-109">)</span><span 
class="pcrr7t-x-x-109">;</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><br /><span class="label"><a 
 id="x1-32017r17"></a></span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><span 
class="pcrr7t-x-x-109">}</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><br /><span class="label"><a 
 id="x1-32018r18"></a></span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><span 
class="pcrr7t-x-x-109">}</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><br /><span class="label"><a 
 id="x1-32019r19"></a></span><span 
class="pcrr7t-x-x-109">}</span>
   </div>
<!--l. 509--><p class="indent" >   In a relational database, one can achieve this by the
following simple query in SQL.
   <!--l. 512-->
                                                          
                                                          
<div class="lstlisting" id="listing-8"><span class="label"><a 
 id="x1-32020r1"></a></span><span 
class="pcrr7t-x-x-109">SELECT</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><span 
class="pcrr7t-x-x-109">&#x22C6;</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><span 
class="pcrr7t-x-x-109">FROM</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><span 
class="pcrr7t-x-x-109">T_KV</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><span 
class="pcrr7t-x-x-109">WHERE</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><span 
class="pcrr7t-x-x-109">value</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><span 
class="pcrr7t-x-x-109">LIKE</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><span 
class="pcrr7t-x-x-109">'</span><span 
class="pcrr7t-x-x-109">%</span><span 
class="pcrr7t-x-x-109">XYZ</span><span 
class="pcrr7t-x-x-109">%</span><span 
class="pcrr7t-x-x-109">'</span><span 
class="pcrr7t-x-x-109">;</span>
   </div>
<!--l. 515--><p class="indent" >   Although this query requires a full table scan, a parallel
DMBS can easily outperformance MapReduce in this case. It is
because the setup cost of MapReduce is high. The performance
gap will be much larger in case that an index can be used such
as
   <!--l. 518-->
<div class="lstlisting" id="listing-9"><span class="label"><a 
 id="x1-32021r1"></a></span><span 
class="pcrr7t-x-x-109">SELECT</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><span 
class="pcrr7t-x-x-109">&#x22C6;</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><span 
class="pcrr7t-x-x-109">FROM</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><span 
class="pcrr7t-x-x-109">T_PERSON</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><span 
class="pcrr7t-x-x-109">WHERE</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><span 
class="pcrr7t-x-x-109">age</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><span 
class="pcrr7t-x-x-109">&#x003E;</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><span 
class="pcrr7t-x-x-109">30;</span>
   </div>
<!--l. 523--><p class="noindent" >
   <h5 class="subsubsectionHead"><span class="titlemark">3.2.4.3   </span> <a 
 id="x1-330003.2.4.3"></a>Aggregation</h5>
<!--l. 525--><p class="noindent" >Aggregation is a simple analytic calculation such as counting
the number of access or users from different countries.
WordCount, the &#8220;hello world&#8221; program in the MapReduce
world, is an example of aggregation. WordCount simply counts
the number of occurrences of each word in a given input set.
The Mapper splits the input line into words and emits a
key-value pair of <span 
class="cmmi-12">&#x003C;</span>word, 1<span 
class="cmmi-12">&#x003E;</span>. The Reducer just sums up the
values. For the sample code, please refer Hadoop&#8217;s MapReduce
Tutorial <span class="cite">[<a 
href="#XMapReduceTutorial">14</a>]</span>.
<!--l. 527--><p class="indent" >   For SQL, aggregation simply means GROUP BY such as the
following example: <!--l. 528-->
<div class="lstlisting" id="listing-10"><span class="label"><a 
 id="x1-33001r1"></a></span><span 
class="pcrr7t-x-x-109">SELECT</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><span 
class="pcrr7t-x-x-109">country</span><span 
class="pcrr7t-x-x-109">,</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><span 
class="pcrr7t-x-x-109">count</span><span 
class="pcrr7t-x-x-109">(&#x22C6;)</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><span 
class="pcrr7t-x-x-109">FROM</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><span 
class="pcrr7t-x-x-109">T_WEB_LOG</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><span 
class="pcrr7t-x-x-109">GROUP</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><span 
class="pcrr7t-x-x-109">BY</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><span 
class="pcrr7t-x-x-109">country</span><span 
class="pcrr7t-x-x-109">;</span>
                                                          
                                                          
   </div>
<!--l. 531--><p class="indent" >   With a combiner, the aggregation in MapReduce works
pretty much same as in a parallel DBMS. Of course, a
DBMS can still benefit a lot from an index on the group by
field.
<!--l. 533--><p class="noindent" >
   <h5 class="subsubsectionHead"><span class="titlemark">3.2.4.4   </span> <a 
 id="x1-340003.2.4.4"></a>Inner Join</h5>
<!--l. 535--><p class="noindent" >An inner join operation combines two data sets, A and B, to
produce a third one containing all record pairs from A
and B with matching attribute value. The sort-merge join
algorithm and hash-join algorithm are two common alternatives
to implement the join operation in a parallel data flow
environment <span class="cite">[<a 
href="#XDeWitt:1992:PDS">32</a>]</span>. In sort-merge join, both A and B are sorted
by the join attribute and then compared in sorted order. The
matching pairs are inserted into the output stream. The
hash-join first prepares a hash table of the smaller data set with
the join attribute as the hash key. Then we scan the larger
dataset and find the relevant rows from the smaller dataset by
searching the hash table.
<!--l. 537--><p class="indent" >   There are several ways to implement join in MapReduce,
e.g. reduce-side join and map-side join. The reduce-side join is a
straightforward approach that takes advantage of that
identical keys are sent to the same reducer. In the reduce-side
join, the output key of Mapper has to be the join key so
that they reach the same reducer. The Mapper also tags
each dataset with an identity to differentiate them in the
reducer. With secondary sorting on the dataset identity,
we ensure the order of values sent to the reducer, which
generates the matched pairs for each join key. Because two
                                                          
                                                          
datasets are usually in different formats, we can use the class
<span 
class="pcrr7t-x-x-120">MultipleInputs </span>to setup different <span 
class="pcrr7t-x-x-120">InputFormat </span>and
<span 
class="pcrr7t-x-x-120">Mapper </span>for each input path. The reduce-side join belongs to
the sort-merge join family and scales very well for large
datasets. However, it may be less efficient in the case of
data skew where a dataset is significantly smaller than the
other.
<!--l. 540--><p class="indent" >   If one dataset is small enough to fit into the memory, we
may use the memory-based map-side join. In this approach, the
Mappers side-load the smaller dataset and build a hash table of
it during the setup, and process the rows of the larger
dataset one-by-one in the map function. To efficiently
load the smaller dataset in every Mapper, we should use
the <span 
class="pcrr7t-x-x-120">DistributedCache</span><a 
 id="dx1-34001"></a>. The <span 
class="pcrr7t-x-x-120">DistributedCache </span>is
a facility to cache application-specific large, read-only
files. An application specifies the files to be cached by
<span 
class="pcrr7t-x-x-120">Job.addCacheFile(URI)</span>. The MapReduce framework will
copy the necessary files on to the slave node before any tasks
for the job are executed on that node. This is much more
efficient than that copying the files for each Mapper. Besides,
we can declare the hash table as a static field so that the tasks
running successively in a JVM will share the data using the
task JVM reuse feature. Thus, we only need to load the data
only once for each JVM.
<!--l. 544--><p class="indent" >   The above map-side join is fast but only works when the
smaller dataset fits in the memory. To avoid this pitfall, we can
use the multi-phrase map-side join. First we run a MapReduce
job on each dataset that uses the join attribute as the Mapper&#8217;s
and Reducer&#8217;s output key and have the same number of
reducers for all datasets. In this way, all datasets are sorted by
the join attribute and have the same number of partitions. In
                                                          
                                                          
second phrase, we use <span 
class="pcrr7t-x-x-120">CompositeInputFormat </span>as the input
format. The <span 
class="pcrr7t-x-x-120">CompositeInputFormat </span>performs joins over a
set of data sources sorted and partitioned the same way, which
is guaranteed by the first phrase. So the records are already
merged before they reach the Mapper, which simplify outputs
the joins to the stream.
<!--l. 546--><p class="indent" >   Because the join implementation is fairly complicated, we
will not show the sample code here. In practice, one should use
higher level tools such as Hive or Pig to join data sets rather
than reinventing the wheel.
<!--l. 548--><p class="indent" >   In practice, join, aggregation, and sort are frequently used
together, e.g. finding the client of the ad that generates the
most revenue (or clicks) during a period. In MapReduce, this
has to be done in multiple phases. The first phrase filters the
data base on the click timestamp and joins the client and click
log datasets. The second phrase does the aggregation on the
output of join and the third one finishes the task by sorting the
output of aggregation.
<!--l. 550--><p class="indent" >   Various benchmarks shows that parallel DBMSs are way
faster than MapReduce for joins <span class="cite">[<a 
href="#XPavlo:2009:CAL">65</a>]</span>. Again an index on the
join key is very helpful. But more importantly, joins can be
done locally on each node if both tables are partitioned by
the join key so that no data transfer is needed before the
join.
<!--l. 552--><p class="noindent" >
   <h5 class="subsubsectionHead"><span class="titlemark">3.2.4.5   </span> <a 
 id="x1-350003.2.4.5"></a>K-Means Clustering</h5>
<!--l. 554--><p class="noindent" >The k-means clustering is a simple and widely used method
that partitions data into k clusters in which each record belongs
to the cluster with the nearest center, serving as a prototype of
                                                          
                                                          
the cluster <span class="cite">[<a 
href="#XJain:1988:ACD">51</a>]</span>. The most common algorithm for k-means
clustering is Lloyd&#8217;s algorithm that iteratively proceeds by
alternating between two steps. The assignment step assigns
each sample to the cluster of nearest mean. The update step
calculates the new means to be the centroids of the samples
in the new clusters. The algorithm converges when the
assignments no longer change. The algorithm can be naturally
implemented in the MapReduce framework where each iteration
will be a MapReduce job.
      <dl class="description"><dt class="description">
<span 
class="cmbx-12">Input</span> </dt><dd 
class="description">The  data  files  as  regular  MapReduce  input  and
      cluster center files side-loaded by DistributedCache.
      Initially, the cluster centers may be random selected.
      </dd><dt class="description">
<span 
class="cmbx-12">Map</span> </dt><dd 
class="description">With side-loaded cluster centers, each sample input
      is mapped to a cluster of nearest mean. The emitted
      key-value pair is <span 
class="cmmi-12">&#x003C;</span>cluster id, sample vector<span 
class="cmmi-12">&#x003E;</span>.
      </dd><dt class="description">
<span 
class="cmbx-12">Combine</span> </dt><dd 
class="description">In order to reduce the data passed to the reducer,
      we  may  have  a  combiner  that  aggregates  samples
      belonging to the same cluster.
      </dd><dt class="description">
<span 
class="cmbx-12">Reduce</span> </dt><dd 
class="description">The  reduce  tasks  recalculate  the  new  means  of
      clusters as the centroids of samples in the new clusters.
      The output of new cluster means will be used as the
      input to next iteration.
      </dd><dt class="description">
<span 
class="cmbx-12">Iterate</span> </dt><dd 
class="description">This  process  is  repeated  until  the  algorithm
                                                          
                                                          
      converges   or   reaches   the   maximum   number   of
      iterations.
      </dd><dt class="description">
<span 
class="cmbx-12">Output</span> </dt><dd 
class="description">Runs  a  map  only  job  to  output  the  cluster
      assignment.</dd></dl>
<!--l. 564--><p class="indent" >   Such an implementation is very scalable. it can handle
very large data size, which may be even larger than the
combined memory of the cluster. On the other hand, it is
not very efficient because the input data have to been
read again and again for each iteration. This is a general
performance issue for MapReduce to implement iterative
algorithms.
<!--l. 566--><p class="noindent" >
   <h4 class="subsectionHead"><span class="titlemark">3.2.5   </span> <a 
 id="x1-360003.2.5"></a>Shortcomings</h4>
<!--l. 567--><p class="noindent" >The above examples show that MapReduce is capable of a
variety of tasks. On the other hand, they also demonstrate
several drawbacks of MapReduce.
   <h5 class="subsubsectionHead"><span class="titlemark">3.2.5.1   </span> <a 
 id="x1-370003.2.5.1"></a>Performance</h5>
<!--l. 569--><p class="noindent" >MapReduce provides a scalable programming model on large
clusters. However, it is not guaranteed to be fast due to many
reasons:
      <ul class="itemize1">
      <li class="itemize">Even  though  Hadoop  now  reuses  JVM  instances
      for  map  and  reduce  tasks,  the  startup  time  is  still
      significant  on  large  clusters.  The  high  startup  cost
                                                          
                                                          
      means that MapReduce is mainly suitable for long run
      batch jobs.
      </li>
      <li class="itemize">The communication between map and reduce tasks
      always are done by remote file access, which actually
      often dominates the computation cost. Such a pulling
      strategy is great for fault tolerance, but it results in
      low performance compared to the push mechanism.
      Besides  there  could  be  M  *  R  intermediate  files.
      Given  large  M  and  R,  it  is  certainly  a  challenge
      for  underlying  file  system.  With  multiple  reducers
      running simultaneously, it is highly likely that some of
      them will attempt to read from the same map node at
      the same time, inducing a large number of disk seeks
      and slowing the effective disk transfer rate.
      </li>
      <li class="itemize">Iterative  algorithms  perform  poorly  on  MapReduce
      because of reading input data again and again. Data
      also  must  be  materialized  and  replicated  on  the
      distributed file system between successive jobs.</li></ul>
<!--l. 575--><p class="noindent" >
   <h5 class="subsubsectionHead"><span class="titlemark">3.2.5.2   </span> <a 
 id="x1-380003.2.5.2"></a>Low Level Programming Interface</h5>
<!--l. 576--><p class="noindent" >A major goal of MapReduce is to provide a simple programming
model that application developers need only to write the
map and reduce parts of the program. However, practical
programmers have to take care of a lot things such as
input/output format, partition functions, comparison functions,
combiners, and job configuration to achieve good performance.
                                                          
                                                          
As shown in the example, even a very simple grep MapReduce
program is fairly long. On the other hand, the same query in
SQL is much shorter and cleaner.
<!--l. 578--><p class="indent" >   MapReduce is independent of the underlying storage
system. It&#8217;s application developers&#8217; duty to organize data such
as building and using any index, partitioning and collocating
related data sets, etc. Unfortunately, these are not easy tasks in
the context of HDFS and MapReduce.
<!--l. 580--><p class="noindent" >
   <h5 class="subsubsectionHead"><span class="titlemark">3.2.5.3   </span> <a 
 id="x1-390003.2.5.3"></a>Limited Parallel Computing Model</h5>
<!--l. 581--><p class="noindent" >The simple computing model of MapReduce brings us no
explicit handling of data transfer and synchronization in
programs, and no deadlock. But it is a limited parallel
computing model, essentially a scatter-gather processing
model. For non-trivial algorithms, programmers try hard to
&#8220;MapReducize&#8221; them, often in a non-intuitive way.
<!--l. 583--><p class="indent" >   After years of practice, the community has realized these
problems and tries to address them in different ways. For
example, Apache Spark aims on the speed by keeping data in
memory. Apache Pig provides a DSL and Hive provides a SQL
dialect on the top of MapReduce to ease the programming.
Google Dremel and Cloudera Impala target on interactive
analysis with SQL queries. Microsoft Dryad/Apache Tez
provides a more general parallel computing framework that
models computations in DAGs. Google Pregel and Apache
Giraph concerns computing problems on large graphs. Apache
Storm focuses on real time event processing. We will look into
all of them in the rest of book. First, we will check out Tez and
Spark in this chapter.
                                                          
                                                          
<!--l. 586--><p class="noindent" >
   <h3 class="sectionHead"><span class="titlemark">3.3   </span> <a 
 id="x1-400003.3"></a>Tez</h3>
<a 
 id="dx1-40001"></a>
<!--l. 588--><p class="noindent" >MapReduce provides a scatter-gather parallel computing model,
which is very limited. Dryad, a research project at Microsoft
Research, attempted to support a more general purpose
runtime for parallel data processing<a 
 id="dx1-40002"></a> <span class="cite">[<a 
href="#XIsard:2007:DDD">50</a>]</span>. A Dryad job is a
directed acyclic graph (DAG) where each vertex is a program
and edges represent data channels (files, TCP pipes, or
shared-memory FIFOs). The DAG defines the data flow of
the application, and the vertices of the graph defines the
operations that are to be performed on the data. It is a
logical computation graph that is automatically mapped
onto physical resources by the runtime. Dryad includes a
domain-specific language, in C++ as a library using a mixture
of method calls and operator overloading, that is used
to create and model a Dryad execution graph. Dryad is
notable for allowing graph vertices to use an arbitrary
number of inputs and outputs, while MapReduce restricts all
computations to take a single input set and generate a single
output set. Although Dryad provides a nice alternative to
MapReduce, Microsoft discontinued active development on
Dryad, shifting focus to the Apache Hadoop framework in
October 2011.
<!--l. 590--><p class="indent" >   Interestingly, the Apache Hadoop community recently
picked up the idea of Dryad and developed Apache Tez <span class="cite">[<a 
href="#XTez">19</a>,&#x00A0;<a 
href="#XTezTutorial">70</a>]</span>,
a new runtime framework on YARN, during the Stinger
initiative<a 
 id="dx1-40003"></a> of Hive<a 
 id="dx1-40004"></a> <span class="cite">[<a 
href="#XStinger">36</a>]</span>. Similar to Dryad, Tez is an application
                                                          
                                                          
framework which allows for a complex directed-acyclic-graph of
tasks for processing data. Edges of data flow graph determine
how the data is transferred and the dependency between the
producer and consumer vertices. Edge properties enable Tez to
instantiate user tasks, configure their inputs and outputs,
schedule them appropriately and define how to route data
between the tasks. The edge properties include:
      <dl class="description"><dt class="description">
<span 
class="cmbx-12">Data movement</span> </dt><dd 
class="description">determines routing of data between
      tasks.
           <ul class="itemize1">
           <li class="itemize">One-To-One:  Data  from  the  <span 
class="cmmi-12">i</span><sup><span 
class="cmmi-8">th</span></sup>  producer  task
           routes to the <span 
class="cmmi-12">i</span><sup><span 
class="cmmi-8">th</span></sup> consumer task.
           </li>
           <li class="itemize">Broadcast: Data from a producer task routes to
           all consumer tasks.
           </li>
           <li class="itemize">Scatter-Gather: Producer tasks scatter data into
           shards  and  consumer  tasks  gather  the  shards.
           The <span 
class="cmmi-12">i</span><sup><span 
class="cmmi-8">th</span></sup> shard from all producer tasks routes to
           the <span 
class="cmmi-12">i</span><sup><span 
class="cmmi-8">th</span></sup> consumer task.</li></ul>
      </dd><dt class="description">
<span 
class="cmbx-12">Scheduling</span> </dt><dd 
class="description">determines when a consumer task is scheduled.
           <ul class="itemize1">
           <li class="itemize">Sequential:  Consumer  task  may  be  scheduled
           after a producer task completes.
           </li>
           <li class="itemize">Concurrent:     Consumer     task     must     be
           co-scheduled with a producer task.</li></ul>
                                                          
                                                          
      </dd><dt class="description">
<span 
class="cmbx-12">Data source</span> </dt><dd 
class="description">determines the lifetime/reliability of a task
      output.
           <ul class="itemize1">
           <li class="itemize">Persisted: Output will be available after the task
           exits. Output may be lost later on.
           </li>
           <li class="itemize">Persisted-Reliable: Output is reliably stored and
           will always be available.
           </li>
           <li class="itemize">Ephemeral: Output is available only while the
           producer task is running.</li></ul>
      </dd></dl>
<!--l. 610--><p class="noindent" >For example, MapReduce would be expressed with the scatter-gather,
sequential and persisted edge properties.
<!--l. 612--><p class="indent" >   The vertex in the data flow graph defines the user logic
that transforms the data. Tez models each vertex as a
composition of Input, Processor and Output modules. Input
and Output determine the data format and how and where it is
read/written. An input represents a pipe through which a
processor can accept input data from a data source such as
HDFS or the output generated by another vertex, while an
output represents a pipe through which a processor can
generate output data for another vertex to consume or
to a data sink such as HDFS. Processor holds the data
transformation logic, which consumes one or more Inputs and
produces one or more Outputs.
<!--l. 614--><p class="indent" >   The Tez runtime expands the logical graph into a physical
graph by adding parallelism at the vertices, i.e. multiple tasks
                                                          
                                                          
are created per logical vertex to perform the computation in
parallel. A logical edge in a DAG is also materialized as a
number of physical connections between the tasks of two
connected vertices. Tez also supports pluggable vertex
management modules to collect information from tasks and
change the data flow graph at runtime to optimize performance
and resource usage.
<!--l. 616--><p class="indent" >   With Tez, Apache Hive is now able to process data in a
single Tez job, which may take multiple MapReduce jobs. If the
data processing is too complicated to finish in a single Tez job,
Tez session can encompass multiple jobs by leveraging common
services. This provides additional performance optimizations.
<hr class="figure"><div class="figure" 
>
                                                          
                                                          
<a 
 id="x1-400054"></a>
                                                          
                                                          

<!--l. 618--><p class="noindent" ><img 
src="images/PigHive_MR.png" alt="PIC"  
> <img 
src="images/PigHive_Tez.png" alt="PIC"  
>
<br /> <div class="caption" 
><span class="id">Figure&#x00A0;3.4: </span><span  
class="content">Pig/Hive on MapReduce vs Tez</span></div><!--tex4ht:label?: x1-400054 -->
                                                          
                                                          
<!--l. 624--><p class="indent" >   </div><hr class="endfigure">
<!--l. 626--><p class="indent" >   Like MapReduce, Tez is still a lower-level programming model.
To obtain good performance, the developer must understand
the structure of the computation and the organization and
properties of the system resources.
   <h3 class="sectionHead"><span class="titlemark">3.4   </span> <a 
 id="x1-410003.4"></a>YARN</h3>
<a 
 id="dx1-41001"></a>
<!--l. 630--><p class="noindent" >Originally, Hadoop was restricted mainly to the paradigm
MapReduce, where the resource management is done by
JobTracker and TaskTacker. The JobTracker farms out
MapReduce tasks to specific nodes in the cluster, ideally the
nodes that have the data, or at least are in the same rack. A
TaskTracker is a node in the cluster that accepts tasks - Map,
Reduce and Shuffle operations - from a JobTracker. Because
Hadoop has stretched beyond MapReudce (e.g. HBase, Storm,
etc.), Hadoop now architecturally decouples the resource
management features from the programming model of
MapReduce, which makes Hadoop clusters more generic.
The new resource manager is referred to as MapReduce
2.0 (MRv2) or YARN <span class="cite">[<a 
href="#XYARN2011:279">43</a>]</span>. Now MapReduce is one kind
of applications running in a YARN container and other
types of applications can be written generically to run on
YARN.
<!--l. 633--><p class="indent" >   YARN employs a master-slave model and includes several
components:
      <ul class="itemize1">
      <li class="itemize">The      global      Resource      Manager      is      the
      ultimate authority that arbitrates resources among all
                                                          
                                                          
      applications in the system.
      </li>
      <li class="itemize">The  per-application  Application  Master  negotiates
      resources  from  the  Resource  Manager  and  works
      with the Node Managers to execute and monitor the
      component tasks.
      </li>
      <li class="itemize">The  per-node  slave  Node  Manager<a 
 id="dx1-41002"></a>  is  responsible
      for launching the applications&#8217; containers, monitoring
      their resource usage and reporting to the Resource
      Manager.</li></ul>
<!--l. 640--><p class="indent" >   <hr class="figure"><div class="figure" 
>
                                                          
                                                          
<a 
 id="x1-410035"></a>
                                                          
                                                          

<!--l. 641--><p class="noindent" ><img 
src="images/yarn-architecture.png" alt="PIC"  
>
<br /> <div class="caption" 
><span class="id">Figure&#x00A0;3.5: </span><span  
class="content">YARN Architecture</span></div><!--tex4ht:label?: x1-410035 -->
                                                          
                                                          
<!--l. 645--><p class="indent" >   </div><hr class="endfigure">
<!--l. 647--><p class="indent" >   The Resource Manager<a 
 id="dx1-41004"></a>, consisting of Scheduler<a 
 id="dx1-41005"></a> and
Application Manager<a 
 id="dx1-41006"></a>, is the central authority that arbitrates
resources among various competing applications in the cluster.
The Scheduler is responsible for allocating resources to the
various running applications subject to the constraints of
capacities, queues etc. The Application Manager is responsible
for accepting job-submissions, negotiating the first container for
executing the application specific Application Master and
provides the service for restarting the Application Master
container on failure.
<!--l. 651--><p class="indent" >   The Scheduler uses the abstract notion of a Resource
Container<a 
 id="dx1-41007"></a> which incorporates elements such as memory, CPU,
disk, network etc. Initially, YARN uses the memory-based
scheduling. Each node is configured with a set amount
of memory and applications request containers for their
tasks with configurable amounts of memory. Recently,
YARN added CPU as a resource in the same manner. Nodes
are configured with a number of &#8220;virtual cores&#8221; (vcores)
and applications give a vcore number in the container
request.
<!--l. 653--><p class="indent" >   The Scheduler has a pluggable policy plug-in, which is
responsible for partitioning the cluster resources among the
various queues, applications etc. For example, the Capacity
Scheduler<a 
 id="dx1-41008"></a> is designed to maximize the throughput and the
utilization of shared, multi-tenant clusters. Queues<a 
 id="dx1-41009"></a> are the
primary abstraction in the Capacity Scheduler. The capacity of
each queue specifies the percentage of cluster resources that are
available for applications submitted to the queue. Furthermore,
queues can be set up in a hierarchy. YARN also sports a Fair
Scheduler<a 
 id="dx1-41010"></a> that tries to assign resources to applications
                                                          
                                                          
such that all applications get an equal share of resources
over time on average using dominant resource fairness
<span class="cite">[<a 
href="#XGhodsi:2011:DRF">39</a>]</span>.
<!--l. 656--><p class="indent" >   The protocol between YARN and applications is as follows.
First an Application Submission Client communicates with
the Resource Manager to acquire a new Application Id.
Then it submit the Application to be run by providing
sufficient information (e.g. the local files/jars, command line,
environment settings, etc.) to the Resource Manager to launch
the Application Master. The Application Master is then
expected to register itself with the Resource Manager and
request for and receive containers. After a container is
allocated to it, the Application Master communicates with the
Node Manager to launch the container for its task by
specifying the launch information such as command line
specification, environment, etc. The Application Master
also handles failures of job containers. Once the task is
completed, the Application Master signals the Resource
Manager.
<!--l. 659--><p class="indent" >   As the central authority of the YARN cluster, the Resource
Manager is also the single point of failure (SPOF<a 
 id="dx1-41011"></a>). To make it
fault tolerant, an Active/Standby architecture can be employed
since Hadoop 2.4. Multiple Resource Manager instances (listed
in the configuration file yarn-site.xml) can be brought up but
only one instance is Active at any point of time while
others are in Standby mode. When the Active goes down
or becomes unresponsive, another Resource Manager is
automatically elected by a ZooKeeper-based method to be the
Active. ZooKeeper<a 
 id="dx1-41012"></a> is a replicated CP key-value store,
which we will discuss in details later. Clients, Application
Masters and Node Managers try connecting to the Resource
                                                          
                                                          
Managers in a round-robin fashion until they hit the new
Active.
                                                          
                                                          
<!--l. 662--><p class="indent" >
                                                          
                                                          
   <h2 class="chapterHead"><span class="titlemark">Chapter&#x00A0;4</span><br /><a 
 id="x1-420004"></a>Spark</h2> <a 
 id="dx1-42001"></a>Although MapReduce is great for large
scale data processing, it is not friendly for iterative algorithms
or interactive analytics because the data have to be repeatedly
loaded for each iteration or be materialized and replicated
on the distributed file system between successive jobs.
Apache Spark <span class="cite">[<a 
href="#XZaharia:2010:SCC">82</a>,&#x00A0;<a 
href="#XZaharia:2012:RDD">81</a>,&#x00A0;<a 
href="#XSpark">16</a>]</span> is designed to solve this problem
by reusing the working dataset. Initially Spark was built
on top of Mesos but can now also run on top of YARN
or standalone today. The overall framework and parallel
computing model of Spark is similar to MapReduce but
with an important innovation, reliant distributed dataset<a 
 id="dx1-42002"></a>
(RDD)<a 
 id="dx1-42003"></a>.
   <h3 class="sectionHead"><span class="titlemark">4.1   </span> <a 
 id="x1-430004.1"></a>RDD</h3>
<!--l. 667--><p class="noindent" >An RDD<a 
 id="dx1-43001"></a> is a read-only collection of objects partitioned across a
cluster of computers that can be operated on in parallel. A
Spark application consists of a driver program that creates
RDDs from HDFS files or an existing Scala collection. The
driver program may transform an RDD in parallel by invoking
supported operations with user-defined functions, which returns
another RDD. The driver can also persist an RDD in memory,
allowing it to be reused efficiently across parallel operations.
In fact, the semantics of RDDs are way more than just
parallelization:
      <dl class="description"><dt class="description">
<span 
class="cmbx-12">Abstract</span> </dt><dd 
class="description">The elements of an RDD does not have to exist
      in  physical  memory.  In  this  sense,  an  element  of
      RDD is an expression rather than a value. The value
      can be computed by evaluating the expression when
      necessary.
                                                          
                                                          
      </dd><dt class="description">
<span 
class="cmbx-12">Lazy and Ephemeral</span> </dt><dd 
class="description">One can construct an RDD from a
      file
      or by transforming an existing RDD such as <span 
class="pcrr7t-x-x-120">map()</span>,
      <span 
class="pcrr7t-x-x-120">filter()</span>,   <span 
class="pcrr7t-x-x-120">groupByKey()</span>,   <span 
class="pcrr7t-x-x-120">reduceByKey()</span>,
      <span 
class="pcrr7t-x-x-120">join()</span>, <span 
class="pcrr7t-x-x-120">cogroup()</span>, <span 
class="pcrr7t-x-x-120">cartesian()</span>, etc. However,
      no real data loading or computation happens at the
      time of construction. Instead, they are materialized on
      demand when they are used in some operation, and
      are discarded from memory after use.
      </dd><dt class="description">
<span 
class="cmbx-12">Caching and Persistence</span> </dt><dd 
class="description">We can cache a dataset in memory
      across operations, which allows future actions to be
      much faster. Caching is a key tool for iterative algorithms
      and fast interactive use cases. Caching is actually one
      special case of persistence that allows different storage
      levels, e.g. persisting the dataset on disk, persisting
      it in memory but as serialized Java objects (to save
      space), replicating it across nodes, or storing it off-heap
      in Tachyon<span class="footnote-mark"><a 
href="bigdata9.html#fn1x5"><sup class="textsuperscript">1</sup></a></span><a 
 id="x1-43002f1"></a> 
      <span class="cite">[<a 
href="#XTachyon">75</a>]</span>. These levels are set by passing a <span 
class="pcrr7t-x-x-120">StorageLevel</span>
      object to <span 
class="pcrr7t-x-x-120">persist()</span>. The cache() method is a shorthand
      for using the default storage level <span 
class="pcrr7t-x-x-120">StorageLevel.MEMORY</span><span 
class="pcrr7t-x-x-120">_ONLY</span>
      (store deserialized objects in memory).
      </dd><dt class="description">
<span 
class="cmbx-12">Fault Tolerant</span> </dt><dd 
class="description">If  any  partition  of  an  RDD  is  lost,
      it   will   automatically   be   recomputed   using   the
      transformations that originally created it.</dd></dl>
                                                          
                                                          
<!--l. 679--><p class="noindent" >The operations on RDDs take user-defined functions, which are
closures in functional programming as Spark is implemented in
Scala. A closure can refer to variables in the scope when
created, which will be copied to the workers when Spark runs a
closure. Spark optimizes this process by shared variables for a
couple of cases:
      <dl class="description"><dt class="description">
<span 
class="cmbx-12">Broadcast variables</span> </dt><dd 
class="description"><a 
 id="dx1-43003"></a> If  a  large  read-only  data  is  used
      in  multiple  operations,  it  is  better  to  copy  it
      to  the  workers  only  once.  Similar  to  the  idea  of
      DistributedCache, this can be achieved by broadcast
      variables that are created from a variable <span 
class="pcrr7t-x-x-120">v </span>by calling
      <span 
class="pcrr7t-x-x-120">SparkContext.broadcast(v)</span>.
      </dd><dt class="description">
<span 
class="cmbx-12">Accumulators</span> </dt><dd 
class="description"><a 
 id="dx1-43004"></a> Accumulators are variables that are only
      &#8220;added&#8221; to through an associative operation and can
      therefore  be  efficiently  supported  in  parallel.  They
      can  be  used  to  implement  counters  or  sums.  Only
      the driver program can read the accumulator&#8217;s value.
      Spark  natively  supports  accumulators  of  numeric
      types.</dd></dl>
<!--l. 686--><p class="noindent" >By reusing cached data in RDDs, Spark offers great performance
improvement over MapReduce (10x <span 
class="cmsy-10x-x-120">~ </span>100x faster). Thus, it is
very suitable for iterative machine learning algorithms. Similar
to MapReduce, Spark is independent of the underlying storage
system. It is application developers&#8217; duty to organize data such
as building and using any index, partitioning and collocating
related data sets, etc. These are critical for interactive analytics.
Merely caching is insufficient and not effective for extremely
large data.
                                                          
                                                          
<!--l. 689--><p class="noindent" >
   <h3 class="sectionHead"><span class="titlemark">4.2   </span> <a 
 id="x1-440004.2"></a>Implementation</h3>
<!--l. 690--><p class="noindent" >The RDD object implements a simple interface, which consists
of three operations:
      <dl class="description"><dt class="description">
<span 
class="pcrb7t-x-x-120">getPartitions</span> </dt><dd 
class="description">returns a list of partition IDs.
      </dd><dt class="description">
<span 
class="pcrb7t-x-x-120">getIterator(partition)</span> </dt><dd 
class="description">iterates over a partition.
      </dd><dt class="description">
<span 
class="pcrb7t-x-x-120">getPreferredLocations(partition)</span> </dt><dd 
class="description">is   used   to
      achieve data locality.</dd></dl>
<!--l. 696--><p class="noindent" >When a parallel operation is invoked on a dataset, Spark
creates a task to process each partition of the dataset and
sends these tasks to worker nodes. Spark tries to send each
task to one of its preferred locations. Once launched on a
worker, each task calls <span 
class="pcrr7t-x-x-120">getIterator </span>to start reading its
partition.
<!--l. 698--><p class="noindent" >
   <h3 class="sectionHead"><span class="titlemark">4.3   </span> <a 
 id="x1-450004.3"></a>API</h3>
<!--l. 699--><p class="noindent" >Spark is implemented in Scala and provides high-level APIs in
Scala, Java, and Python. The following examples are in Scala.
A Spark program needs to create a <span 
class="pcrr7t-x-x-120">SparkContext </span>object:
<!--l. 700-->
                                                          
                                                          
<div class="lstlisting" id="listing-11"><span class="label"><a 
 id="x1-45001r1"></a></span><span 
class="pcrr7t-x-x-109">val</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><span 
class="pcrr7t-x-x-109">conf</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><span 
class="pcrr7t-x-x-109">=</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><span 
class="pcrr7t-x-x-109">new</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><span 
class="pcrr7t-x-x-109">SparkConf</span><span 
class="pcrr7t-x-x-109">()</span><span 
class="pcrr7t-x-x-109">.</span><span 
class="pcrr7t-x-x-109">setAppName</span><span 
class="pcrr7t-x-x-109">(</span><span 
class="pcrr7t-x-x-109">appName</span><span 
class="pcrr7t-x-x-109">)</span><span 
class="pcrr7t-x-x-109">.</span><span 
class="pcrr7t-x-x-109">setMaster</span><span 
class="pcrr7t-x-x-109">(</span><span 
class="pcrr7t-x-x-109">master</span><span 
class="pcrr7t-x-x-109">)</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><br /><span class="label"><a 
 id="x1-45002r2"></a></span><span 
class="pcrr7t-x-x-109">val</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><span 
class="pcrr7t-x-x-109">sc</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><span 
class="pcrr7t-x-x-109">=</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><span 
class="pcrr7t-x-x-109">new</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><span 
class="pcrr7t-x-x-109">SparkContext</span><span 
class="pcrr7t-x-x-109">(</span><span 
class="pcrr7t-x-x-109">conf</span><span 
class="pcrr7t-x-x-109">)</span>
   </div>
<!--l. 704--><p class="indent" >   The <span 
class="pcrr7t-x-x-120">appName </span>parameter is a name for your application to
show on the cluster UI and the <span 
class="pcrr7t-x-x-120">master </span>is a cluster URL or a
special &#8220;local&#8221; string to run in local mode.
<!--l. 706--><p class="indent" >   Then we can create RDDs from any storage source
supported by Hadoop. Spark supports text files, SequenceFiles,
etc. Text file RDDs can be created using <span 
class="pcrr7t-x-x-120">SparkContext</span>&#8217;s
<span 
class="pcrr7t-x-x-120">textFile </span>method. This method takes an URI for the file
(directories, compressed files, and wildcards as well) and reads
it as a collection of lines. <!--l. 708-->
<div class="lstlisting" id="listing-12"><span class="label"><a 
 id="x1-45003r1"></a></span><span 
class="pcrr7t-x-x-109">val</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><span 
class="pcrr7t-x-x-109">lines</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><span 
class="pcrr7t-x-x-109">=</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><span 
class="pcrr7t-x-x-109">sc</span><span 
class="pcrr7t-x-x-109">.</span><span 
class="pcrr7t-x-x-109">textFile</span><span 
class="pcrr7t-x-x-109">(</span><span 
class="pcrr7t-x-x-109">"</span><span 
class="pcrr7t-x-x-109">data</span><span 
class="pcrr7t-x-x-109">.</span><span 
class="pcrr7t-x-x-109">txt</span><span 
class="pcrr7t-x-x-109">"</span><span 
class="pcrr7t-x-x-109">)</span>
   </div>
<!--l. 712--><p class="indent" >   We can create a new RDD by transforming from an existing
one, such as <span 
class="pcrr7t-x-x-120">map</span>, <span 
class="pcrr7t-x-x-120">flatMap</span>, <span 
class="pcrr7t-x-x-120">filter</span>, etc. We can also
aggregate all the elements of an RDD using some function, e.g.
<span 
class="pcrr7t-x-x-120">reduce</span>, <span 
class="pcrr7t-x-x-120">reduceByKey</span>, etc. <!--l. 713-->
<div class="lstlisting" id="listing-13"><span class="label"><a 
 id="x1-45004r1"></a></span><span 
class="pcrr7t-x-x-109">val</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><span 
class="pcrr7t-x-x-109">lengths</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><span 
class="pcrr7t-x-x-109">=</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><span 
class="pcrr7t-x-x-109">lines</span><span 
class="pcrr7t-x-x-109">.</span><span 
class="pcrr7t-x-x-109">map</span><span 
class="pcrr7t-x-x-109">(</span><span 
class="pcrr7t-x-x-109">s</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><span 
class="pcrr7t-x-x-109">=&#x003E;</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><span 
class="pcrr7t-x-x-109">s</span><span 
class="pcrr7t-x-x-109">.</span><span 
class="pcrr7t-x-x-109">length</span><span 
class="pcrr7t-x-x-109">)</span>
   </div>
<!--l. 716--><p class="indent" >   Beyond the basic operations such as <span 
class="pcrr7t-x-x-120">map </span>and <span 
class="pcrr7t-x-x-120">reduce</span>,
Spark also provides advanced operations such as <span 
class="pcrr7t-x-x-120">union</span>,
<span 
class="pcrr7t-x-x-120">intersection</span>, <span 
class="pcrr7t-x-x-120">join</span>, <span 
class="pcrr7t-x-x-120">cogroup</span>, which creates a new
dataset from two existing RDDs. All these operations take a
functions from the driver program to run on the cluster.
Thanks to the functional features of Scala, the code is a lot
                                                          
                                                          
simpler and cleaner than MapReduce as shown in the
example.
<!--l. 719--><p class="indent" >   As we discussed, RDDs are lazy and ephemeral. If we need
to access an RDD multiple times, it is better to persist
it in memory using the <span 
class="pcrr7t-x-x-120">persist </span>(or <span 
class="pcrr7t-x-x-120">cache</span>) method.
<!--l. 720-->
<div class="lstlisting" id="listing-14"><span class="label"><a 
 id="x1-45005r1"></a></span><span 
class="pcrr7t-x-x-109">lengths</span><span 
class="pcrr7t-x-x-109">.</span><span 
class="pcrr7t-x-x-109">persist</span>
   </div>
<!--l. 724--><p class="indent" >   Spark also supports a rich set of higher-level tools including
Spark SQL for SQL and structured data processing, MLlib
for machine learning, GraphX for graph processing, and
Spark Streaming for event processing. We will discuss these
technologies later in related chapters.
                                                          
                                                          
   <h2 class="chapterHead"><span class="titlemark">Chapter&#x00A0;5</span><br /><a 
 id="x1-460005"></a>Analytics and Data Warehouse</h2> With big data at
hand, we want to crunch numbers from them. MapReduce and
TeZ are good tools for ad-hoc analytics. However, their
programming models are very low level. Custom code has to be
written for even simple operations like projection and
filtering. It is even more tedious and verbose to implement
common relational operators such as join. Several efforts,
including Pig and Hive, have been devoted to simplify the
development of MapReduce/Tez programs by providing
high level DSL or SQL that can be translated to native
MapReduce/Tez code. Similarly, Shark and Spark SQL bring
SQL on top of Spark. Moreover, Cloudera Impala and Apache
Drill introduces native massively parallel processing query
engine to Hadoop for interactive analysis of web-scale
datasets.
   <h3 class="sectionHead"><span class="titlemark">5.1   </span> <a 
 id="x1-470005.1"></a>Pig</h3>
<!--l. 730--><p class="noindent" >Different from many other projects that bring SQL to
Hadoop, Pig is special in that it provides a procedural
(data flow) programming language Pig Latin as it was
designed for experienced programmers. However, SQL
programmers won&#8217;t have difficulties to understand Pig Latin
programs because most statements just look like SQL
clauses.
<!--l. 732--><p class="indent" >   A Pig Latin program is a sequence of steps, each of which
carries out a single data processing at fairly high level,
e.g. loading, filtering, grouping, etc. The input data can
be loaded from the file system or HBase by the operator
LOAD:
   <!--l. 734-->
                                                          
                                                          
<div class="lstlisting" id="listing-15"><span class="label"><a 
 id="x1-47001r1"></a></span><span 
class="pcrr7t-x-x-109">grunt</span><span 
class="pcrr7t-x-x-109">&#x003E;</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><span 
class="pcrr7t-x-x-109">persons</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><span 
class="pcrr7t-x-x-109">=</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><span 
class="pcrr7t-x-x-109">LOAD</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><span 
class="pcrr7t-x-x-109">'</span><span 
class="pcrr7t-x-x-109">person</span><span 
class="pcrr7t-x-x-109">.</span><span 
class="pcrr7t-x-x-109">csv</span><span 
class="pcrr7t-x-x-109">'</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><span 
class="pcrr7t-x-x-109">USING</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><span 
class="pcrr7t-x-x-109">PigStorage</span><span 
class="pcrr7t-x-x-109">(</span><span 
class="pcrr7t-x-x-109">'</span><span 
class="pcrr7t-x-x-109">,</span><span 
class="pcrr7t-x-x-109">'</span><span 
class="pcrr7t-x-x-109">)</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><span 
class="pcrr7t-x-x-109">AS</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><span 
class="pcrr7t-x-x-109">(</span><span 
class="pcrr7t-x-x-109">name</span><span 
class="pcrr7t-x-x-109">:</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><span 
class="pcrr7t-x-x-109">chararray</span><span 
class="pcrr7t-x-x-109">,</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><span 
class="pcrr7t-x-x-109">age</span><span 
class="pcrr7t-x-x-109">:</span><span 
class="pcrr7t-x-x-109">int</span><span 
class="pcrr7t-x-x-109">,</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><span 
class="pcrr7t-x-x-109">address</span><span 
class="pcrr7t-x-x-109">:</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><span 
class="pcrr7t-x-x-109">(</span><span 
class="pcrr7t-x-x-109">street</span><span 
class="pcrr7t-x-x-109">:</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><span 
class="pcrr7t-x-x-109">chararray</span><span 
class="pcrr7t-x-x-109">,</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><span 
class="pcrr7t-x-x-109">city</span><span 
class="pcrr7t-x-x-109">:</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><span 
class="pcrr7t-x-x-109">chararray</span><span 
class="pcrr7t-x-x-109">,</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><span 
class="pcrr7t-x-x-109">state</span><span 
class="pcrr7t-x-x-109">:</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><span 
class="pcrr7t-x-x-109">chararray</span><span 
class="pcrr7t-x-x-109">,</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><span 
class="pcrr7t-x-x-109">zip</span><span 
class="pcrr7t-x-x-109">:</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><span 
class="pcrr7t-x-x-109">int</span><span 
class="pcrr7t-x-x-109">)</span><span 
class="pcrr7t-x-x-109">)</span><span 
class="pcrr7t-x-x-109">;</span>
   </div>
<!--l. 737--><p class="indent" >   where <span 
class="cmmi-12">grunt &#x003E; </span>is the prompt of Pig console and PigStorage
is a built-in deserializer for structured text files. Various
deserializers are available. User defined functions (UDFs) can
also be used to parse data in unsupported format. The AS
clause defines a schema that assigns names to fields and
declares types for fields. Although schemas are optional,
programmer are encouraged to use them whenever possible.
Note that such a &#8220;schema on read&#8221; is very different from the
relational approach that requires rigid predefined schemas.
Therefore, there is no need copying or reorganizing the
data.
<!--l. 739--><p class="indent" >   Pig has a rich data model. Primitive data types include int,
long, float, double, chararray, bytearray, boolean, datetime,
biginteger and bigdecimal. And complex data types include
tuple, bag (a collection of tuples), and map (a set of key value
pairs). Different from relational model, the fields of tuples can
be any data types. Similarly, the map values can be any types
(the map key is always type chararray). That is, nested data
structures are supported.
<!--l. 741--><p class="indent" >   Once the input data have been specified, there is a
rich set of relational operators to transform them. The
FOREACH...GENERATE operator, corresponding to the map
tasks of MapReduce, produces a new bag by projection,
applying functions, etc.
   <!--l. 743-->
<div class="lstlisting" id="listing-16"><span class="label"><a 
 id="x1-47002r1"></a></span><span 
class="pcrr7t-x-x-109">grunt</span><span 
class="pcrr7t-x-x-109">&#x003E;</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><span 
class="pcrr7t-x-x-109">flatten_persons</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><span 
class="pcrr7t-x-x-109">=</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><span 
class="pcrr7t-x-x-109">FOREACH</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><span 
class="pcrr7t-x-x-109">persons</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><span 
class="pcrr7t-x-x-109">GENERATE</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><span 
class="pcrr7t-x-x-109">name</span><span 
class="pcrr7t-x-x-109">,</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><span 
class="pcrr7t-x-x-109">age</span><span 
class="pcrr7t-x-x-109">,</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><span 
class="pcrr7t-x-x-109">FLATTEN</span><span 
class="pcrr7t-x-x-109">(</span><span 
class="pcrr7t-x-x-109">address</span><span 
class="pcrr7t-x-x-109">)</span><span 
class="pcrr7t-x-x-109">;</span>
                                                          
                                                          
   </div>
<!--l. 746--><p class="indent" >   where FLATTEN is a function to remove one level of
nesting. With the operator DESCRIBE, we can see the schema
difference between persons and flatten_persons:
   <!--l. 748-->
<div class="lstlisting" id="listing-17"><span class="label"><a 
 id="x1-47003r1"></a></span><span 
class="pcrr7t-x-x-109">grunt</span><span 
class="pcrr7t-x-x-109">&#x003E;</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><span 
class="pcrr7t-x-x-109">DESCRIBE</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><span 
class="pcrr7t-x-x-109">persons</span><span 
class="pcrr7t-x-x-109">;</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><br /><span class="label"><a 
 id="x1-47004r2"></a></span><span 
class="pcrr7t-x-x-109">persons</span><span 
class="pcrr7t-x-x-109">:</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><span 
class="pcrr7t-x-x-109">{</span><span 
class="pcrr7t-x-x-109">name</span><span 
class="pcrr7t-x-x-109">:</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><span 
class="pcrr7t-x-x-109">chararray</span><span 
class="pcrr7t-x-x-109">,</span><span 
class="pcrr7t-x-x-109">age</span><span 
class="pcrr7t-x-x-109">:</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><span 
class="pcrr7t-x-x-109">int</span><span 
class="pcrr7t-x-x-109">,</span><span 
class="pcrr7t-x-x-109">address</span><span 
class="pcrr7t-x-x-109">:</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><span 
class="pcrr7t-x-x-109">(</span><span 
class="pcrr7t-x-x-109">street</span><span 
class="pcrr7t-x-x-109">:</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><span 
class="pcrr7t-x-x-109">chararray</span><span 
class="pcrr7t-x-x-109">,</span><span 
class="pcrr7t-x-x-109">city</span><span 
class="pcrr7t-x-x-109">:</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><span 
class="pcrr7t-x-x-109">chararray</span><span 
class="pcrr7t-x-x-109">,</span><span 
class="pcrr7t-x-x-109">state</span><span 
class="pcrr7t-x-x-109">:</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><span 
class="pcrr7t-x-x-109">chararray</span><span 
class="pcrr7t-x-x-109">,</span><span 
class="pcrr7t-x-x-109">zip</span><span 
class="pcrr7t-x-x-109">:</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><span 
class="pcrr7t-x-x-109">int</span><span 
class="pcrr7t-x-x-109">)</span><span 
class="pcrr7t-x-x-109">}</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><br /><span class="label"><a 
 id="x1-47005r3"></a></span><span 
class="pcrr7t-x-x-109">grunt</span><span 
class="pcrr7t-x-x-109">&#x003E;</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><span 
class="pcrr7t-x-x-109">DESCRIBE</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><span 
class="pcrr7t-x-x-109">flatten_persons</span><span 
class="pcrr7t-x-x-109">;</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><br /><span class="label"><a 
 id="x1-47006r4"></a></span><span 
class="pcrr7t-x-x-109">flatten_persons</span><span 
class="pcrr7t-x-x-109">:</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><span 
class="pcrr7t-x-x-109">{</span><span 
class="pcrr7t-x-x-109">name</span><span 
class="pcrr7t-x-x-109">:</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><span 
class="pcrr7t-x-x-109">chararray</span><span 
class="pcrr7t-x-x-109">,</span><span 
class="pcrr7t-x-x-109">age</span><span 
class="pcrr7t-x-x-109">:</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><span 
class="pcrr7t-x-x-109">int</span><span 
class="pcrr7t-x-x-109">,</span><span 
class="pcrr7t-x-x-109">address</span><span 
class="pcrr7t-x-x-109">::</span><span 
class="pcrr7t-x-x-109">street</span><span 
class="pcrr7t-x-x-109">:</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><span 
class="pcrr7t-x-x-109">chararray</span><span 
class="pcrr7t-x-x-109">,</span><span 
class="pcrr7t-x-x-109">address</span><span 
class="pcrr7t-x-x-109">::</span><span 
class="pcrr7t-x-x-109">city</span><span 
class="pcrr7t-x-x-109">:</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><span 
class="pcrr7t-x-x-109">chararray</span><span 
class="pcrr7t-x-x-109">,</span><span 
class="pcrr7t-x-x-109">address</span><span 
class="pcrr7t-x-x-109">::</span><span 
class="pcrr7t-x-x-109">state</span><span 
class="pcrr7t-x-x-109">:</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><span 
class="pcrr7t-x-x-109">chararray</span><span 
class="pcrr7t-x-x-109">,</span><span 
class="pcrr7t-x-x-109">address</span><span 
class="pcrr7t-x-x-109">::</span><span 
class="pcrr7t-x-x-109">zip</span><span 
class="pcrr7t-x-x-109">:</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><span 
class="pcrr7t-x-x-109">int</span><span 
class="pcrr7t-x-x-109">}</span>
   </div>
<!--l. 754--><p class="indent" >   Frequently, we want to filter the data based on some
condition.
   <!--l. 756-->
<div class="lstlisting" id="listing-18"><span class="label"><a 
 id="x1-47007r1"></a></span><span 
class="pcrr7t-x-x-109">grunt</span><span 
class="pcrr7t-x-x-109">&#x003E;</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><span 
class="pcrr7t-x-x-109">adults</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><span 
class="pcrr7t-x-x-109">=</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><span 
class="pcrr7t-x-x-109">FILTER</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><span 
class="pcrr7t-x-x-109">flatten_persons</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><span 
class="pcrr7t-x-x-109">BY</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><span 
class="pcrr7t-x-x-109">age</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><span 
class="pcrr7t-x-x-109">&#x003E;</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><span 
class="pcrr7t-x-x-109">18;</span>
   </div>
<!--l. 759--><p class="indent" >   Aggregations can be done by GROUP operator, which
corresponds to the reduce tasks in MapReduce.
   <!--l. 761-->
<div class="lstlisting" id="listing-19"><span class="label"><a 
 id="x1-47008r1"></a></span><span 
class="pcrr7t-x-x-109">grunt</span><span 
class="pcrr7t-x-x-109">&#x003E;</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><span 
class="pcrr7t-x-x-109">grouped_by_state</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><span 
class="pcrr7t-x-x-109">=</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><span 
class="pcrr7t-x-x-109">GROUP</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><span 
class="pcrr7t-x-x-109">flatten_persons</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><span 
class="pcrr7t-x-x-109">BY</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><span 
class="pcrr7t-x-x-109">state</span><span 
class="pcrr7t-x-x-109">;</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><br /><span class="label"><a 
 id="x1-47009r2"></a></span><span 
class="pcrr7t-x-x-109">grunt</span><span 
class="pcrr7t-x-x-109">&#x003E;</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><span 
class="pcrr7t-x-x-109">DESCRIBE</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><span 
class="pcrr7t-x-x-109">grouped_by_state</span><span 
class="pcrr7t-x-x-109">;</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><br /><span class="label"><a 
 id="x1-47010r3"></a></span><span 
class="pcrr7t-x-x-109">grouped_by_state</span><span 
class="pcrr7t-x-x-109">:</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><span 
class="pcrr7t-x-x-109">{</span><span 
class="pcrr7t-x-x-109">group</span><span 
class="pcrr7t-x-x-109">:</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><span 
class="pcrr7t-x-x-109">chararray</span><span 
class="pcrr7t-x-x-109">,</span><span 
class="pcrr7t-x-x-109">flatten_persons</span><span 
class="pcrr7t-x-x-109">:</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><span 
class="pcrr7t-x-x-109">{(</span><span 
class="pcrr7t-x-x-109">name</span><span 
class="pcrr7t-x-x-109">:</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><span 
class="pcrr7t-x-x-109">chararray</span><span 
class="pcrr7t-x-x-109">,</span><span 
class="pcrr7t-x-x-109">age</span><span 
class="pcrr7t-x-x-109">:</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><span 
class="pcrr7t-x-x-109">int</span><span 
class="pcrr7t-x-x-109">,</span><span 
class="pcrr7t-x-x-109">address</span><span 
class="pcrr7t-x-x-109">::</span><span 
class="pcrr7t-x-x-109">street</span><span 
class="pcrr7t-x-x-109">:</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><span 
class="pcrr7t-x-x-109">chararray</span><span 
class="pcrr7t-x-x-109">,</span><span 
class="pcrr7t-x-x-109">address</span><span 
class="pcrr7t-x-x-109">::</span><span 
class="pcrr7t-x-x-109">city</span><span 
class="pcrr7t-x-x-109">:</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><span 
class="pcrr7t-x-x-109">chararray</span><span 
class="pcrr7t-x-x-109">,</span><span 
class="pcrr7t-x-x-109">address</span><span 
class="pcrr7t-x-x-109">::</span><span 
class="pcrr7t-x-x-109">state</span><span 
class="pcrr7t-x-x-109">:</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><span 
class="pcrr7t-x-x-109">chararray</span><span 
class="pcrr7t-x-x-109">,</span><span 
class="pcrr7t-x-x-109">address</span><span 
class="pcrr7t-x-x-109">::</span><span 
class="pcrr7t-x-x-109">zip</span><span 
class="pcrr7t-x-x-109">:</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><span 
class="pcrr7t-x-x-109">int</span><span 
class="pcrr7t-x-x-109">)</span><span 
class="pcrr7t-x-x-109">}}</span>
   </div>
<!--l. 766--><p class="indent" >   The result of a GROUP operation is a relation that includes
one tuple per group of two fields:
<!--l. 768--><p class="indent" >   The first field is named &#8220;group&#8221; and is the same type as the
group key. The second field takes the name of the original
relation and is type bag. We can also cogroup two or more
relations.
   <!--l. 772-->
                                                          
                                                          
<div class="lstlisting" id="listing-20"><span class="label"><a 
 id="x1-47011r1"></a></span><span 
class="pcrr7t-x-x-109">grunt</span><span 
class="pcrr7t-x-x-109">&#x003E;</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><span 
class="pcrr7t-x-x-109">cogrouped_by_name</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><span 
class="pcrr7t-x-x-109">=</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><span 
class="pcrr7t-x-x-109">COGROUP</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><span 
class="pcrr7t-x-x-109">persons</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><span 
class="pcrr7t-x-x-109">BY</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><span 
class="pcrr7t-x-x-109">name</span><span 
class="pcrr7t-x-x-109">,</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><span 
class="pcrr7t-x-x-109">flatten_persons</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><span 
class="pcrr7t-x-x-109">BY</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><span 
class="pcrr7t-x-x-109">name</span><span 
class="pcrr7t-x-x-109">;</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><br /><span class="label"><a 
 id="x1-47012r2"></a></span><span 
class="pcrr7t-x-x-109">grunt</span><span 
class="pcrr7t-x-x-109">&#x003E;</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><span 
class="pcrr7t-x-x-109">DESCRIBE</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><span 
class="pcrr7t-x-x-109">cogrouped_by_name</span><span 
class="pcrr7t-x-x-109">;</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><br /><span class="label"><a 
 id="x1-47013r3"></a></span><span 
class="pcrr7t-x-x-109">cogrouped_by_name</span><span 
class="pcrr7t-x-x-109">:</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><span 
class="pcrr7t-x-x-109">{</span><span 
class="pcrr7t-x-x-109">group</span><span 
class="pcrr7t-x-x-109">:</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><span 
class="pcrr7t-x-x-109">chararray</span><span 
class="pcrr7t-x-x-109">,</span><span 
class="pcrr7t-x-x-109">persons</span><span 
class="pcrr7t-x-x-109">:</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><span 
class="pcrr7t-x-x-109">{(</span><span 
class="pcrr7t-x-x-109">name</span><span 
class="pcrr7t-x-x-109">:</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><span 
class="pcrr7t-x-x-109">chararray</span><span 
class="pcrr7t-x-x-109">,</span><span 
class="pcrr7t-x-x-109">age</span><span 
class="pcrr7t-x-x-109">:</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><span 
class="pcrr7t-x-x-109">int</span><span 
class="pcrr7t-x-x-109">,</span><span 
class="pcrr7t-x-x-109">address</span><span 
class="pcrr7t-x-x-109">:</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><span 
class="pcrr7t-x-x-109">(</span><span 
class="pcrr7t-x-x-109">street</span><span 
class="pcrr7t-x-x-109">:</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><span 
class="pcrr7t-x-x-109">chararray</span><span 
class="pcrr7t-x-x-109">,</span><span 
class="pcrr7t-x-x-109">city</span><span 
class="pcrr7t-x-x-109">:</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><span 
class="pcrr7t-x-x-109">chararray</span><span 
class="pcrr7t-x-x-109">,</span><span 
class="pcrr7t-x-x-109">state</span><span 
class="pcrr7t-x-x-109">:</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><span 
class="pcrr7t-x-x-109">chararray</span><span 
class="pcrr7t-x-x-109">,</span><span 
class="pcrr7t-x-x-109">zip</span><span 
class="pcrr7t-x-x-109">:</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><span 
class="pcrr7t-x-x-109">int</span><span 
class="pcrr7t-x-x-109">)</span><span 
class="pcrr7t-x-x-109">)</span><span 
class="pcrr7t-x-x-109">},</span><span 
class="pcrr7t-x-x-109">flatten_persons</span><span 
class="pcrr7t-x-x-109">:</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><span 
class="pcrr7t-x-x-109">{(</span><span 
class="pcrr7t-x-x-109">name</span><span 
class="pcrr7t-x-x-109">:</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><span 
class="pcrr7t-x-x-109">chararray</span><span 
class="pcrr7t-x-x-109">,</span><span 
class="pcrr7t-x-x-109">age</span><span 
class="pcrr7t-x-x-109">:</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><span 
class="pcrr7t-x-x-109">int</span><span 
class="pcrr7t-x-x-109">,</span><span 
class="pcrr7t-x-x-109">address</span><span 
class="pcrr7t-x-x-109">::</span><span 
class="pcrr7t-x-x-109">street</span><span 
class="pcrr7t-x-x-109">:</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><span 
class="pcrr7t-x-x-109">chararray</span><span 
class="pcrr7t-x-x-109">,</span><span 
class="pcrr7t-x-x-109">address</span><span 
class="pcrr7t-x-x-109">::</span><span 
class="pcrr7t-x-x-109">city</span><span 
class="pcrr7t-x-x-109">:</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><span 
class="pcrr7t-x-x-109">chararray</span><span 
class="pcrr7t-x-x-109">,</span><span 
class="pcrr7t-x-x-109">address</span><span 
class="pcrr7t-x-x-109">::</span><span 
class="pcrr7t-x-x-109">state</span><span 
class="pcrr7t-x-x-109">:</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><span 
class="pcrr7t-x-x-109">chararray</span><span 
class="pcrr7t-x-x-109">,</span><span 
class="pcrr7t-x-x-109">address</span><span 
class="pcrr7t-x-x-109">::</span><span 
class="pcrr7t-x-x-109">zip</span><span 
class="pcrr7t-x-x-109">:</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><span 
class="pcrr7t-x-x-109">int</span><span 
class="pcrr7t-x-x-109">)</span><span 
class="pcrr7t-x-x-109">}}</span>
   </div>
<!--l. 777--><p class="indent" >   In fact, the GROUP and COGROUP operators are
identical. Both operators work with one or more relations. For
readability, GROUP is used in statements involving one relation
while COGROUP is used when involving two or more
relations.
<!--l. 779--><p class="indent" >   A closely related but different operator is JOIN, which is a
syntactic sugar of COGROUP followed by FLATTEN.
   <!--l. 781-->
<div class="lstlisting" id="listing-21"><span class="label"><a 
 id="x1-47014r1"></a></span><span 
class="pcrr7t-x-x-109">grunt</span><span 
class="pcrr7t-x-x-109">&#x003E;</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><span 
class="pcrr7t-x-x-109">joined_by_name</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><span 
class="pcrr7t-x-x-109">=</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><span 
class="pcrr7t-x-x-109">JOIN</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><span 
class="pcrr7t-x-x-109">persons</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><span 
class="pcrr7t-x-x-109">BY</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><span 
class="pcrr7t-x-x-109">name</span><span 
class="pcrr7t-x-x-109">,</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><span 
class="pcrr7t-x-x-109">flatten_persons</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><span 
class="pcrr7t-x-x-109">BY</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><span 
class="pcrr7t-x-x-109">name</span><span 
class="pcrr7t-x-x-109">;</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><br /><span class="label"><a 
 id="x1-47015r2"></a></span><span 
class="pcrr7t-x-x-109">grunt</span><span 
class="pcrr7t-x-x-109">&#x003E;</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><span 
class="pcrr7t-x-x-109">DESCRIBE</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><span 
class="pcrr7t-x-x-109">joined_by_name</span><span 
class="pcrr7t-x-x-109">;</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><br /><span class="label"><a 
 id="x1-47016r3"></a></span><span 
class="pcrr7t-x-x-109">joined_by_name</span><span 
class="pcrr7t-x-x-109">:</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><span 
class="pcrr7t-x-x-109">{</span><span 
class="pcrr7t-x-x-109">persons</span><span 
class="pcrr7t-x-x-109">::</span><span 
class="pcrr7t-x-x-109">name</span><span 
class="pcrr7t-x-x-109">:</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><span 
class="pcrr7t-x-x-109">chararray</span><span 
class="pcrr7t-x-x-109">,</span><span 
class="pcrr7t-x-x-109">persons</span><span 
class="pcrr7t-x-x-109">::</span><span 
class="pcrr7t-x-x-109">age</span><span 
class="pcrr7t-x-x-109">:</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><span 
class="pcrr7t-x-x-109">int</span><span 
class="pcrr7t-x-x-109">,</span><span 
class="pcrr7t-x-x-109">persons</span><span 
class="pcrr7t-x-x-109">::</span><span 
class="pcrr7t-x-x-109">address</span><span 
class="pcrr7t-x-x-109">:</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><span 
class="pcrr7t-x-x-109">(</span><span 
class="pcrr7t-x-x-109">street</span><span 
class="pcrr7t-x-x-109">:</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><span 
class="pcrr7t-x-x-109">chararray</span><span 
class="pcrr7t-x-x-109">,</span><span 
class="pcrr7t-x-x-109">city</span><span 
class="pcrr7t-x-x-109">:</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><span 
class="pcrr7t-x-x-109">chararray</span><span 
class="pcrr7t-x-x-109">,</span><span 
class="pcrr7t-x-x-109">state</span><span 
class="pcrr7t-x-x-109">:</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><span 
class="pcrr7t-x-x-109">chararray</span><span 
class="pcrr7t-x-x-109">,</span><span 
class="pcrr7t-x-x-109">zip</span><span 
class="pcrr7t-x-x-109">:</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><span 
class="pcrr7t-x-x-109">int</span><span 
class="pcrr7t-x-x-109">)</span><span 
class="pcrr7t-x-x-109">,</span><span 
class="pcrr7t-x-x-109">flatten_persons</span><span 
class="pcrr7t-x-x-109">::</span><span 
class="pcrr7t-x-x-109">name</span><span 
class="pcrr7t-x-x-109">:</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><span 
class="pcrr7t-x-x-109">chararray</span><span 
class="pcrr7t-x-x-109">,</span><span 
class="pcrr7t-x-x-109">flatten_persons</span><span 
class="pcrr7t-x-x-109">::</span><span 
class="pcrr7t-x-x-109">age</span><span 
class="pcrr7t-x-x-109">:</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><span 
class="pcrr7t-x-x-109">int</span><span 
class="pcrr7t-x-x-109">,</span><span 
class="pcrr7t-x-x-109">flatten_persons</span><span 
class="pcrr7t-x-x-109">::</span><span 
class="pcrr7t-x-x-109">address</span><span 
class="pcrr7t-x-x-109">::</span><span 
class="pcrr7t-x-x-109">street</span><span 
class="pcrr7t-x-x-109">:</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><span 
class="pcrr7t-x-x-109">chararray</span><span 
class="pcrr7t-x-x-109">,</span><span 
class="pcrr7t-x-x-109">flatten_persons</span><span 
class="pcrr7t-x-x-109">::</span><span 
class="pcrr7t-x-x-109">address</span><span 
class="pcrr7t-x-x-109">::</span><span 
class="pcrr7t-x-x-109">city</span><span 
class="pcrr7t-x-x-109">:</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><span 
class="pcrr7t-x-x-109">chararray</span><span 
class="pcrr7t-x-x-109">,</span><span 
class="pcrr7t-x-x-109">flatten_persons</span><span 
class="pcrr7t-x-x-109">::</span><span 
class="pcrr7t-x-x-109">address</span><span 
class="pcrr7t-x-x-109">::</span><span 
class="pcrr7t-x-x-109">state</span><span 
class="pcrr7t-x-x-109">:</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><span 
class="pcrr7t-x-x-109">chararray</span><span 
class="pcrr7t-x-x-109">,</span><span 
class="pcrr7t-x-x-109">flatten_persons</span><span 
class="pcrr7t-x-x-109">::</span><span 
class="pcrr7t-x-x-109">address</span><span 
class="pcrr7t-x-x-109">::</span><span 
class="pcrr7t-x-x-109">zip</span><span 
class="pcrr7t-x-x-109">:</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><span 
class="pcrr7t-x-x-109">int</span><span 
class="pcrr7t-x-x-109">}</span>
   </div>
<!--l. 786--><p class="indent" >   Overall, a Pig Latin program is like a handcrafted query
execution plan. In contrast, a SQL based solution, e.g. Hive,
relies on an execution planner to automatically translate SQL
statements to an execution plan. Like SQL, Pig Latin has no
control structures. But it is possible to embed Pig Latin
statements and Pig commands in the Python, JavaScript and
Groovy scripts.
<!--l. 788--><p class="indent" >   When you run the above statements in the console of Pig,
you will notice that they finish instantaneously. It is because
Pig is lazy and there is no really computation happened. For
example, LOAD does not really read the data but just returns a
handle to a bag/relation. Only when a STORE command is
issued, Pig materialize the result of a Pig Latin expression
sequence to the file system. Before a STORE command, Pig
just builds a logical plan for every user defined bag. At the
                                                          
                                                          
point of a STORE command, the logical plan is compiled into a
physical plan (a directed acyclic graph of MapReduce jobs) and
is executed.
<!--l. 790--><p class="indent" >   It is possible to replace MapReduce with other execution
engines in Pig. For example, there are efforts to run Pig on top
of Spark. However, is it necessary? Spark already provides many
relational operators and the host language Scala is very nice to
write concise and expressive programs.
<!--l. 792--><p class="indent" >   In summary, Pig Latin is a simple and easy to use
DSL that makes MapReduce programming a lot easier.
Meanwhile, Pig keeps the flexibility of MapReduce to process
schemaless data in plain files. There is no need to do slow
and complex ETL tasks before analysis, which makes Pig
a great tool for quick ad-hoc analytics such as web log
analysis.
<!--l. 794--><p class="noindent" >
   <h3 class="sectionHead"><span class="titlemark">5.2   </span> <a 
 id="x1-480005.2"></a>Hive</h3>
<!--l. 795--><p class="noindent" >Although many statements in Pig Latin look just like SQL
clauses, it is a procedural programming language. In this
section we will discuss Apache Hive that first brought SQL to
Hadoop. Similar to Pig, Hive translates its own dialect of SQL
(HiveQL) queries to a directed acyclic graph of MapReduce (or
Tez since 0.13) jobs. However, the difference between Pig and
Hive is not only procedural vs declarative. Pig is a relatively
thin layer on top of MapReduce for offline analytics. But Hive is
towards a data warehouse. With the recent stinger initiative,
Hive is closer to interactive analytics by 100x performance
improvement.
                                                          
                                                          
<!--l. 797--><p class="indent" >   Pig uses a &#8220;schema on read&#8221; approach that users define the
(optional) schema on loading data. In contrast, Hive requires
users to provides schema, (optional) storage format and
serializer/deserializer (called SerDe) when creating a table.
These information is saved in the metadata repository (by
default an embedded Derby database) and will be used
whenever the table is referenced, e.g. to typecheck the
expressions in the query and to prune partitions based on query
predicates. The metadata store also provides data discovery
(e.g. SHOW TABLES and DESCRIBE) that enables users
to discover and explore relevant and specific data in the
warehouse. The following example shows how to create a
database and a table.
   <!--l. 799-->
<div class="lstlisting" id="listing-22"><span class="label"><a 
 id="x1-48001r1"></a></span><span 
class="pcrr7t-x-x-109">CREATE</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><span 
class="pcrr7t-x-x-109">DATABASE</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><span 
class="pcrr7t-x-x-109">portal</span><span 
class="pcrr7t-x-x-109">;</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><br /><span class="label"><a 
 id="x1-48002r2"></a></span><span 
class="pcrr7t-x-x-109">USE</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><span 
class="pcrr7t-x-x-109">portal</span><span 
class="pcrr7t-x-x-109">;</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><br /><span class="label"><a 
 id="x1-48003r3"></a></span><span 
class="pcrr7t-x-x-109">CREATE</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><span 
class="pcrr7t-x-x-109">TABLE</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><span 
class="pcrr7t-x-x-109">weblog</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><span 
class="pcrr7t-x-x-109">(</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><br /><span class="label"><a 
 id="x1-48004r4"></a></span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><span 
class="pcrr7t-x-x-109">host</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><span 
class="pcrr7t-x-x-109">STRING</span><span 
class="pcrr7t-x-x-109">,</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><br /><span class="label"><a 
 id="x1-48005r5"></a></span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><span 
class="pcrr7t-x-x-109">identity</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><span 
class="pcrr7t-x-x-109">STRING</span><span 
class="pcrr7t-x-x-109">,</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><br /><span class="label"><a 
 id="x1-48006r6"></a></span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><span 
class="pcrr7t-x-x-109">user</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><span 
class="pcrr7t-x-x-109">STRING</span><span 
class="pcrr7t-x-x-109">,</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><br /><span class="label"><a 
 id="x1-48007r7"></a></span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><span 
class="pcrr7t-x-x-109">time</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><span 
class="pcrr7t-x-x-109">STRING</span><span 
class="pcrr7t-x-x-109">,</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><br /><span class="label"><a 
 id="x1-48008r8"></a></span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><span 
class="pcrr7t-x-x-109">request</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><span 
class="pcrr7t-x-x-109">STRING</span><span 
class="pcrr7t-x-x-109">,</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><br /><span class="label"><a 
 id="x1-48009r9"></a></span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><span 
class="pcrr7t-x-x-109">status</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><span 
class="pcrr7t-x-x-109">STRING</span><span 
class="pcrr7t-x-x-109">,</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><br /><span class="label"><a 
 id="x1-48010r10"></a></span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><span 
class="pcrr7t-x-x-109">size</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><span 
class="pcrr7t-x-x-109">STRING</span><span 
class="pcrr7t-x-x-109">,</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><br /><span class="label"><a 
 id="x1-48011r11"></a></span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><span 
class="pcrr7t-x-x-109">referer</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><span 
class="pcrr7t-x-x-109">STRING</span><span 
class="pcrr7t-x-x-109">,</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><br /><span class="label"><a 
 id="x1-48012r12"></a></span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><span 
class="pcrr7t-x-x-109">agent</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><span 
class="pcrr7t-x-x-109">STRING</span><span 
class="pcrr7t-x-x-109">)</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><br /><span class="label"><a 
 id="x1-48013r13"></a></span><span 
class="pcrr7t-x-x-109">ROW</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><span 
class="pcrr7t-x-x-109">FORMAT</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><span 
class="pcrr7t-x-x-109">SERDE</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><span 
class="pcrr7t-x-x-109">'</span><span 
class="pcrr7t-x-x-109">org</span><span 
class="pcrr7t-x-x-109">.</span><span 
class="pcrr7t-x-x-109">apache</span><span 
class="pcrr7t-x-x-109">.</span><span 
class="pcrr7t-x-x-109">hadoop</span><span 
class="pcrr7t-x-x-109">.</span><span 
class="pcrr7t-x-x-109">hive</span><span 
class="pcrr7t-x-x-109">.</span><span 
class="pcrr7t-x-x-109">serde2</span><span 
class="pcrr7t-x-x-109">.</span><span 
class="pcrr7t-x-x-109">RegexSerDe</span><span 
class="pcrr7t-x-x-109">'</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><br /><span class="label"><a 
 id="x1-48014r14"></a></span><span 
class="pcrr7t-x-x-109">WITH</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><span 
class="pcrr7t-x-x-109">SERDEPROPERTIES</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><span 
class="pcrr7t-x-x-109">(</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><br /><span class="label"><a 
 id="x1-48015r15"></a></span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><span 
class="pcrr7t-x-x-109">"</span><span 
class="pcrr7t-x-x-109">input</span><span 
class="pcrr7t-x-x-109">.</span><span 
class="pcrr7t-x-x-109">regex</span><span 
class="pcrr7t-x-x-109">"</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><span 
class="pcrr7t-x-x-109">=</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><span 
class="pcrr7t-x-x-109">"</span><span 
class="pcrr7t-x-x-109">([^</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><span 
class="pcrr7t-x-x-109">]&#x22C6;)</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><span 
class="pcrr7t-x-x-109">([^</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><span 
class="pcrr7t-x-x-109">]&#x22C6;)</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><span 
class="pcrr7t-x-x-109">([^</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><span 
class="pcrr7t-x-x-109">]&#x22C6;)</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><span 
class="pcrr7t-x-x-109">(-|\\[[^\\]]&#x22C6;\\])</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><span 
class="pcrr7t-x-x-109">([^</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><span 
class="pcrr7t-x-x-109">\</span><span 
class="pcrr7t-x-x-109">"</span><span 
class="pcrr7t-x-x-109">]&#x22C6;|\</span><span 
class="pcrr7t-x-x-109">"</span><span 
class="pcrr7t-x-x-109">[^\</span><span 
class="pcrr7t-x-x-109">"</span><span 
class="pcrr7t-x-x-109">]&#x22C6;\</span><span 
class="pcrr7t-x-x-109">"</span><span 
class="pcrr7t-x-x-109">)</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><span 
class="pcrr7t-x-x-109">(-|[0-9]&#x22C6;)</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><span 
class="pcrr7t-x-x-109">(-|[0-9]&#x22C6;)</span><span 
class="pcrr7t-x-x-109">(?:</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><span 
class="pcrr7t-x-x-109">([^</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><span 
class="pcrr7t-x-x-109">\</span><span 
class="pcrr7t-x-x-109">"</span><span 
class="pcrr7t-x-x-109">]&#x22C6;|\</span><span 
class="pcrr7t-x-x-109">"</span><span 
class="pcrr7t-x-x-109">[^\</span><span 
class="pcrr7t-x-x-109">"</span><span 
class="pcrr7t-x-x-109">]&#x22C6;\</span><span 
class="pcrr7t-x-x-109">"</span><span 
class="pcrr7t-x-x-109">)</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><span 
class="pcrr7t-x-x-109">([^</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><span 
class="pcrr7t-x-x-109">\</span><span 
class="pcrr7t-x-x-109">"</span><span 
class="pcrr7t-x-x-109">]&#x22C6;|\</span><span 
class="pcrr7t-x-x-109">"</span><span 
class="pcrr7t-x-x-109">[^\</span><span 
class="pcrr7t-x-x-109">"</span><span 
class="pcrr7t-x-x-109">]&#x22C6;\</span><span 
class="pcrr7t-x-x-109">"</span><span 
class="pcrr7t-x-x-109">)</span><span 
class="pcrr7t-x-x-109">)</span><span 
class="pcrr7t-x-x-109">?</span><span 
class="pcrr7t-x-x-109">"</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><br /><span class="label"><a 
 id="x1-48016r16"></a></span><span 
class="pcrr7t-x-x-109">)</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><br /><span class="label"><a 
 id="x1-48017r17"></a></span><span 
class="pcrr7t-x-x-109">STORED</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><span 
class="pcrr7t-x-x-109">AS</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><span 
class="pcrr7t-x-x-109">TEXTFILE</span><span 
class="pcrr7t-x-x-109">;</span>
   </div>
<!--l. 818--><p class="indent" >   The interesting part of example is the bottom five lines that
specify custom regular expression SerDe and plain text file
format. If ROW FORMAT is not specified or ROW FORMAT
DELIMITED is specified, a native SerDe is used. Besides plain
text files, many other file formats are supported. Later we will
discuss more details on ORC files, which improve query
performance significantly.
<!--l. 820--><p class="indent" >   Different from relational data warehouses, Hive supports
nested data models with complex types array, map, and struct.
For example, the following statement creates a table with a
complex schema.
   <!--l. 822-->
                                                          
                                                          
<div class="lstlisting" id="listing-23"><span class="label"><a 
 id="x1-48018r1"></a></span><span 
class="pcrr7t-x-x-109">CREATE</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><span 
class="pcrr7t-x-x-109">TABLE</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><span 
class="pcrr7t-x-x-109">complex_table</span><span 
class="pcrr7t-x-x-109">(</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><br /><span class="label"><a 
 id="x1-48019r2"></a></span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><span 
class="pcrr7t-x-x-109">id</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><span 
class="pcrr7t-x-x-109">STRING</span><span 
class="pcrr7t-x-x-109">,</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><br /><span class="label"><a 
 id="x1-48020r3"></a></span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><span 
class="pcrr7t-x-x-109">value</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><span 
class="pcrr7t-x-x-109">FLOAT</span><span 
class="pcrr7t-x-x-109">,</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><br /><span class="label"><a 
 id="x1-48021r4"></a></span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><span 
class="pcrr7t-x-x-109">list_of_maps</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><span 
class="pcrr7t-x-x-109">ARRAY</span><span 
class="pcrr7t-x-x-109">&#x003C;</span><span 
class="pcrr7t-x-x-109">MAP</span><span 
class="pcrr7t-x-x-109">&#x003C;</span><span 
class="pcrr7t-x-x-109">STRING</span><span 
class="pcrr7t-x-x-109">,</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><span 
class="pcrr7t-x-x-109">STRUCT</span><span 
class="pcrr7t-x-x-109">&#x003C;</span><span 
class="pcrr7t-x-x-109">x</span><span 
class="pcrr7t-x-x-109">:</span><span 
class="pcrr7t-x-x-109">INT</span><span 
class="pcrr7t-x-x-109">,</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><span 
class="pcrr7t-x-x-109">y</span><span 
class="pcrr7t-x-x-109">:</span><span 
class="pcrr7t-x-x-109">INT</span><span 
class="pcrr7t-x-x-109">&#x003E;&#x003E;&#x003E;</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><br /><span class="label"><a 
 id="x1-48022r5"></a></span><span 
class="pcrr7t-x-x-109">)</span><span 
class="pcrr7t-x-x-109">;</span>
   </div>
<!--l. 829--><p class="indent" >   By default, all the data files for a table are located in a
single directory. Tables can be physically partitioned based on
values of one or more columns with the PARTITIONED BY
clause. A separate directory is created for each distinct value
combination in the partition columns. Partitioning can greatly
speed up queries that test those columns. Note that the
partitioning columns are not part of the table data and
the partition column values are encoded in the directory
path of that partition (and also stored in the metadata
store). Moreover, tables or partitions can be bucketed using
CLUSTERED BY columns, and data can be sorted within that
bucket via SORT BY columns.
<!--l. 831--><p class="indent" >   Now we can load some data into our table:
   <!--l. 833-->
<div class="lstlisting" id="listing-24"><span class="label"><a 
 id="x1-48023r1"></a></span><span 
class="pcrr7t-x-x-109">LOAD</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><span 
class="pcrr7t-x-x-109">DATA</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><span 
class="pcrr7t-x-x-109">LOCAL</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><span 
class="pcrr7t-x-x-109">INPATH</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><span 
class="pcrr7t-x-x-109">'</span><span 
class="pcrr7t-x-x-109">portal</span><span 
class="pcrr7t-x-x-109">/</span><span 
class="pcrr7t-x-x-109">logs</span><span 
class="pcrr7t-x-x-109">'</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><span 
class="pcrr7t-x-x-109">OVERWRITE</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><span 
class="pcrr7t-x-x-109">INTO</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><span 
class="pcrr7t-x-x-109">TABLE</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><span 
class="pcrr7t-x-x-109">weblog</span><span 
class="pcrr7t-x-x-109">;</span>
   </div>
<!--l. 836--><p class="indent" >   Note that Hive does not do any verification of data against
the schema or transformation while loading data into tables.
The input files are simply copied or moved into the Hive&#8217;s file
system namespace. If the keyword LOCAL is specified, the
input files are assumed in the local file system, otherwise in
HDFS. While not necessary in this example, the keyword
OVERWRITE signifies that existing data in the table is
overwritten. If the OVERWRITE keyword is omitted, data files
are appended to existing data sets.
                                                          
                                                          
<!--l. 838--><p class="indent" >   Tables can also be created and populated by the results of a
query in a create-table-as-select (CTAS) statement that
includes two parts. The SELECT part can be any SELECT
statement supported by HiveQL. The CREATE part of the
CTAS takes the resulting schema from the SELECT part and
creates the target table with other table properties such as the
SerDe and storage format.
   <!--l. 840-->
<div class="lstlisting" id="listing-25"><span class="label"><a 
 id="x1-48024r1"></a></span><span 
class="pcrr7t-x-x-109">CREATE</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><span 
class="pcrr7t-x-x-109">TABLE</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><span 
class="pcrr7t-x-x-109">orc_weblog</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><br /><span class="label"><a 
 id="x1-48025r2"></a></span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><span 
class="pcrr7t-x-x-109">STORED</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><span 
class="pcrr7t-x-x-109">AS</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><span 
class="pcrr7t-x-x-109">ORC</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><br /><span class="label"><a 
 id="x1-48026r3"></a></span><span 
class="pcrr7t-x-x-109">AS</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><br /><span class="label"><a 
 id="x1-48027r4"></a></span><span 
class="pcrr7t-x-x-109">SELECT</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><span 
class="pcrr7t-x-x-109">&#x22C6;</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><span 
class="pcrr7t-x-x-109">FROM</span><span 
class="pcrr7t-x-x-109">&#x00A0;</span><span 
class="pcrr7t-x-x-109">weblog</span><span 
class="pcrr7t-x-x-109">;</span>
   </div>
<!--l. 846--><p class="indent" >   Similarly, query results can be inserted into tables by the
INSERT clause. INSERT OVERWRITE will overwrite any
existing data in the table or partition while INSERT INTO will
append to the table or partition. Multiple insert clauses can be
specified in the same query, which minimize the number of data
scans required.
<!--l. 848--><p class="indent" >   Hive does not support the OLTP-style INSERT INTO that
inserts a new record. HiveQL does not have UPDATE and
DELETE clauses either. This is actually a good design choice as
these clauses are not necessary for data warehouses. Without
them, Hive can use very simple mechanisms to deal with reader
and writer concurrency.
<!--l. 850--><p class="indent" >   For queries, HiveQL is pretty much like what you see in
SQL. Besides common SQL features (e.g. JOIN, WHERE,
HAVING, GROUP BY, SORT BY, ...), HiveQL also have
extensions such as TABLESAMPLE, LATERAL VIEW,
OVER, etc. We will not dive into the syntax of query
statements. Instead, we will discuss the stinger initiative, which
improves the query performance significantly.
                                                          
                                                          
<!--l. 852--><p class="indent" >   A big contribution of stinger initiative is the Optimized
Record Columnar (ORC) file. In previous example, we use
TEXTFILE in which each line/row contains a record. In fact,
most relational and document databases employ such a
row-oriented storage format. However, column-oriented file
format has advantages for data warehouses where aggregates
are computed over large numbers of data items. For example,
only required column values on each query are scanned and
transferred on query execution. Besides, column data is of
uniform type and thus may achieve better compression,
especially if the cardinality of the column is low. Before ORC
files, Hive already had a columnar file format RCFile. However,
RCFile is data-type-agnostic and its corresponding SerDe
serializes a single row at a time. In ORC Files, the SerDe is
de-emphasized and the ORC file writer is data type aware. So
the ORC file can decompose a complex column to multiple child
columns and various type-specific data encoding schemes can be
applied to primitive data streams to store data efficiently.
Besides, the ORC file also supports indexes. Well, these
indexes are not B-trees but basically data statistics and
position pointers. The data statistics are used in query
optimization and to answer simple aggregation queries.
They are also helpful to avoid unnecessary data read. The
position pointers are used to locate the index groups and
stripes.
<!--l. 854--><p class="indent" >   The stinger initiative also put a lot of efforts to improve the
query planning and execution. For example, unnecessary
Map-only jobs are eliminated. In Hive, a Map-only job is
generated when the query planner converts a Reduce Join to a
Map Join. Now, Hive tries to merge the generated Map-only job
to its child job if the total size of small tables used to build
                                                          
                                                          
hash tables in the merged job is under a configurable threshold.
Besides, a correlation optimizer was developed to avoid
unnecessary data loading and repartitioning so that Hive loads
the common table only once instead of multiple times
and the optimized plan will have less number of shuffling
phases.
<!--l. 856--><p class="indent" >   Besides MapReduce, Hive now embeds Apache Tez as
an execution engine. Compared to MapReduce&#8217;s simple
scatter/gather model, Tez offers a customizable execution
architecture that models complex computations as dataflow
graphs with dynamic performance optimizations. With Tez,
Hive can translate complex SQL statements into efficient
physical plans. For example, several reduce sinks can be linked
directly in Tez and data can be pipelined without the need of
temporary HDFS files. This pattern is referred to as MRR
(Map - reduce - reduce*). Join is also much easier in Tez
because a Tez task may take multiple bipartite edges as
input thus exposing the input relations directly to the join
implementation. The shuffle join task taking multiple feeds is
called multi-parent shuffle join (MPJ). Both MRR and
MPJ are employed in Hive to speed up a wide variety of
queries.
<!--l. 858--><p class="indent" >   Another potential benefit of Tez is to avoid unnecessary
disk writes. In MapReduce, map outputs are partitioned,
sorted and written to disk, then pulled, merge-sorted and
fed into the reducers. Tez allows for small datasets to be
handled entirely in memory. This is attractive as many
analytic queries generate small intermediate datasets after the
heavy lifting. Moreover, Tez allows complete control over
the processing, e.g. stopping processing when limits are
met. Unfortunately, these feature are not used in Hive
                                                          
                                                          
currently.
<!--l. 860--><p class="indent" >   There is also work to employ Spark as the third execution
engine in Hive, called Hive on Spark. Hive on Spark is still in
early stage and it is not designed to replace Tez or MapReduce
as each has different strengths depending on the use case. Shark
and Spark SQL are similar attempts. We will discuss them in
details later.
<!--l. 862--><p class="indent" >   Finally, let&#8217;s briefly talk about the vectorized query
execution. But first to note that &#8220;vectorized&#8221; does not mean
using vector computing facility such as SSE/AVX or CUDA.
Instead, it aims to improve the runtime execution efficiency by
taking advantage of the characteristics of modern CPUs. The
one-row-at-a-time model of MapReduce is not friendly to
modern CPUs that heavily relay on pipelines, superscalar
(multiple issue), and cache. In the vectorized execution model,
data are processed in batches of rows through the operator tree,
whose expressions work on column vectors and produce
output in column vectors. The vectorized expressions are
carefully designed to minimize branching and function calls. A
good compiler may also unroll the tight loops of vectorized
expressions to effectively make use of the superscalar pipelines
without delays. Furthermore, the size of batch is configurable
and should be chosen to fit the entire batch in the processor
cache. Experiments show that the vectorized execution
engine is 3 - 5 times faster than the original Hive execution
engine.
<!--l. 864--><p class="indent" >   Invented by Facebook, Hive has been battle proven. Today,
Facebook&#8217;s Hive data warehouse holds 300 PB data with an
incoming daily rate of about 600 TB! The stinger initiative
makes Hive more suitable for interactive analytics although the
speed is not the fastest in the market.
                                                          
                                                          
<!--l. 866--><p class="noindent" >
   <h3 class="sectionHead"><span class="titlemark">5.3   </span> <a 
 id="x1-490005.3"></a>Impala</h3>
<!--l. 867--><p class="noindent" >After Apache Hive first brought SQL to Hadoop, several new
SQL on Hadoop solutions were introduced. In particular,
there are Cloudera Impala and Apache Drill, which run on
Hadoop and both are inspired by Google Dremel that
was designed for interactive analysis of web-scale datasets
<span class="footnote-mark"><a 
href="bigdata10.html#fn1x6"><sup class="textsuperscript">1</sup></a></span><a 
 id="x1-49001f1"></a>. In
a nutshell, they are native massively parallel processing query
engine on read-only data. Impala is implemented in C++ while
Drill is implemented in Java. Both Impala and Drill can
query Hive tables directly. Impala actually uses Hive&#8217;s
metastore.
<!--l. 869--><p class="indent" >   Hive is basically a front end to parse SQL statements,
generate and optimize logical plans, translate them into
physical plans that are finally executed by a backend such as
MapReduce or Tez. Dremel and its derivatives are different as
they execute queries natively without translating them into
MapReduce jobs. For example, the core Impala component is a
daemon process that runs on each node of the cluster as the
query planner, coordinator, and execution engine. Each node
can accept queries. The planner turns a request into collections
of parallel plan fragments. The coordinator initiates execution
on remote nodes in the cluster. The execution engine reads and
                                                          
                                                          
writes to data files, and transmits intermediate query results
back to the coordinator node.
<!--l. 871--><p class="indent" >   The two core technologies of Dremel are columnar
storage for nested data and the tree architecture for query
execution:
      <dl class="description"><dt class="description">
<span 
class="cmbx-12">Columnar Storage</span> </dt><dd 
class="description">Data is stored in a columnar storage
      fashion  to  achieve  very  high  compression  ratio  and
      scan throughput.
      </dd><dt class="description">
<span 
class="cmbx-12">Tree Architecture</span> </dt><dd 
class="description">The  architecture  forms  a  massively
      parallel distributed multi-level serving tree for pushing
      down a query to the tree and then aggregating the
      results from the leaves.</dd></dl>
<!--l. 878--><p class="noindent" >These are good ideas and have been adopted by other systems. For
example, Hive 0.13 has the ORC file for columnar storage and
can use Tez as the execution engine that structures the
computation as a directed acyclic graph. Both (and other
innovations) help a lot to improve the performance of Hive.
However, the recent benchmark from Cloudera (the vendor of
Impala) <span class="cite">[<a 
href="#XClouderaImpala2014">28</a>]</span> and the benchmark by AMPLab <span class="cite">[<a 
href="#XAMPLabBenchmark2014">4</a>]</span> show
that Impala still has the performance lead over Hive. It is
well known that benchmarks are often biased due to the
hardware setting, software tweaks, queries in testing, etc.
But it is still meaningful to find out what possible design
choice and implementation details cause this performance
difference. And it may help both communities improve the
offerings in the future. What follows is a list of possible
reasons:
                                                          
                                                          
      <ul class="itemize1">
      <li class="itemize">As a native query engine, Impala avoids the startup
      overhead  of  MapReduce/Tez  jobs.  It  is  well  known
      that MapReduce programs take some time before all
      nodes are running at full capacity. In Hive, every query
      suffers this &#8220;cold start&#8221; problem. In contrast, Impala
      daemon processes are started at boot time, and thus
      are always ready to execute a query.
      </li>
      <li class="itemize">Hadoop reuses JVM instances to reduce the startup
      overhead partially. However, it also introduces another
      problem.  For  big  data  processing,  we  prefer  large
      memory.  For  example,  the  recommended  physical
      memory for an Impala node is 128 GB or higher. The
      nodes in the aforementioned benchmark have 384 GB
      memory. Such a big heap is actually a big challenge
      to the garbage collection system of the reused JVM
      instances. The stop-of-the-world GC pauses may add
      high  latency  to  queries.  Impala&#8217;s  execution  engine,
      written  in  C++  native  code,  avoids  this  problem.
      Impala may also do a better job on cache management.
      </li>
      <li class="itemize">Impala  process  are  multithreaded.  Importantly,  the
      scanning portion of plan fragments are multithreaded
      as well as making use of SSE4.2 instructions. The I/O
      and network systems are also highly multithreaded.
      Therefore,   each   single   Impala   node   runs   more
      efficiently by a high level local parallelism.
      </li>
                                                          
                                                          
      <li class="itemize">Impala&#8217;s  query  execution  is  pipelined  as  much  as
      possible. In case of aggregation, the coordinator starts
      the final aggregation as soon as the pre-aggregation
      fragments has started to return results. In contrast,
      sort and reduce can only start once all the mappers are
      done in MapReduce. Tez currently does not support
      pipelined execution yet.
      </li>
      <li class="itemize">MapReduce   materializes   all   intermediate   results
      while  Impala  streams  intermediate  results  between
      executors. Tez allows different types of Input/Output
      including file, TCP, etc. But it seems that Hive does
      not  use  this  feature  yet  to  avoid  unnecessary  disk
      writes.
      </li>
      <li class="itemize">The  reducer  of  MapReduce  employs  a  pull  model
      to  get  Map  output  partitions.  For  sorted  output,
      Tez  makes  use  of  the  MapReduce  ShuffleHandler,
      which  requires  downstream  Inputs  to  pull  data
      over HTTP. With multiple reducers (or downstream
      Inputs) running simultaneously, it is highly likely that
      some of them will attempt to read from the same map
      node at the same time, inducing a large number of
      disk seeks and slowing the effective disk transfer rate.
      Hive&#8217;s query expressions are generated at compile time
      while Impala does runtime code generation for &#8220;big
      loops&#8221; using LLVM that can achieve more optimized
      code.
      </li>
                                                          
                                                          
      <li class="itemize">Tez  allows  complete  control  over  the  processing,
      e.g.  stopping  processing  when  limits  are  met.  It
      is  very  useful  for  top-k  calculation  and  straggler
      handling. Unfortunately, this feature is not used by
      Hive currently. (BTW, Dremel calculates approximate
      results  for  top-k  and  count-distinct  using  one-pass
      algorithms. It is not clear if Impala does the same.)
      </li>
      <li class="itemize">During          query          execution,          Dremel
      computes a histogram of tablet processing time. If a
      tablet takes a disproportionately long time to process,
      it is rescheduled to another server. If trading speed
      against accuracy is acceptable, Dremel can return the
      results before scanning all the data, which may reduce
      the  response  time  significantly  as  a  small  fraction
      of the tables often take a lot longer. It is not clear
      if Impala implements a similar mechanism although
      straggler handling was stated on the roadmap.</li></ul>
<!--l. 890--><p class="noindent" >In summary, Dremel and its derivatives provide us an inexpensive
way to do interactive big data analytics. The Hadoop ecosystem
is now a real threat to the traditional relational MPP data
warehouse systems. The benchmark by AMPLab shows
that Amazon Redshift (based on ParAccel by Actian)
still has the performance lead over Impala but the gap is
small. With continuous improvements (e.g. both Hive and
Impala are working on cost based plan optimizer), we can
expect SQL on Hadoop/HDFS at higher level in near
feature.
                                                          
                                                          
   <h3 class="sectionHead"><span class="titlemark">5.4   </span> <a 
 id="x1-500005.4"></a>Shark and Spark SQL</h3>
<!--l. 893--><p class="noindent" >We have reviewed Apache Hive and Cloudera Impala, which are
great for ad hoc analysis of big data. Today, Facebook&#8217;s Hive
data warehouse holds 300 PB data with an incoming daily rate
of about 600 TB! It is amazing but it does&#8217;t mean that
most analytics is on that scale (even for Facebook). In
fact, queries usually focus on a particular subset or time
window and touch only a small number of columns of
tables.
<!--l. 895--><p class="indent" >   In <span class="cite">[<a 
href="#XRowstron:2012:NEG">69</a>]</span>, Microsoft Research analyzed 174,000 jobs submitted
to a production analytics cluster in Microsoft in a single
month in 2011 and found that the median job input data
set size was less than 14 GB. They also estimated that
the median input data size of the Hadoop jobs on the
production clusters at Yahoo is less than 12.5 GB. A 2012
paper from Facebook revealed that Facebook jobs follow a
power-law distribution with small jobs dominating. From the
graphs in the paper, it appears that at least 90% of the
jobs have input sizes under 100 GB. For sure, the input
sizes of today&#8217;s jobs are bigger. But many of them should
be comfortably fit into the main memory of a cluster.
Therefore, in-memory computation does make a lot of
sense for interactive analytics at this scale. As Spark gains
popularity, there are several efforts to build SQL on top of
it.
<!--l. 897--><p class="indent" >   The first attempt was Shark, which is built on the Hive
codebase. Shark uses the Hive query compiler to parse
a HiveQL query and generate an abstract syntax tree,
which is then turned into a logical plan with some basic
optimizations. Then Shark applies additional optimizations and
creates a physical plan of RDD operations, then executes
                                                          
                                                          
them over Spark. It sounds straightforward but a naive
implementation may be inefficient. The Shark team does an
excellent job to ensure the high performance. First of all,
Shark implements a columnar memory store on top of
Spark&#8217;s native memory store to reduce the big memory
overhead of JVM. Shark stores all columns of primitive types
as JVM primitive arrays. Complex data types such as
map and array are realized and concatenated into a byte
array. Since each column creates only one JVM object,
it helps a lot to reduce the effect of garbage collection.
Shark also try to maximize the throughput of distributed
data loading. Each data loading task decides whether each
column in a partition should be compressed, and chooses
the best compression scheme for each partition rather
than conforming to a global scheme. Moreover, Shark
implemented a cost-based query optimizer that selects more
efficient join order based on table and column statistics. The
statistics may be manually calculated by Hive (ANALYZE
TABLE statement) and stored in metastore. Otherwise,
Shark collects the statistics when creating and caching a
RDD.
<!--l. 899--><p class="indent" >   Recently, Shark team announced that they are ending
the development of Shark and will focus their resources
towards Spark SQL. Before diving into Spark SQL, we should
notice that the Hive community proposed the Hive on
Spark initiative that will add Spark as the third execution
engine to Hive. Because the implementation may take
significant time and resources, the project will take a phased
approach.
<!--l. 901--><p class="indent" >   Spark SQL takes a different design from Shark. In Shark,
Spark is used as the backend engine, which the users does
                                                          
                                                          
not need to know. But Spark SQL is developed as part
of Spark. Like using JDBC in Java, Spark SQL allows
users to mix SQL and imperative/functional programming.
The core of Spark SQL is SchemaRDD, a new type of
RDD that has an associated schema. Similar to a table in
a traditional relational database, SchemaRDDs can be
used in relational queries in addition to standard RDD
functions. A SchemaRDD can be created from an existing
RDD using the SQLContext.createSchemaRDD() function
(or implicitly converting an RDD of Scala case classes
by importing a SQLContext). A SchemaRDD can also
be created by loading data in from external sources, e.g.
Parquet file, a JSON dataset, or Hive queries through
HiveContext.
<!--l. 903--><p class="indent" >   Similar to Shark, Spark SQL employs an in-memory
columnar store. Different form Shark, Spark SQL does
not use any query optimizations of Hive. Hive&#8217;s query
optimizer has a lot of complexity to address the limitations of
MapReduce. But many of those do not apply in Spark. So
Spark SQL designs a new query optimizer framework called
Catalyst.
                                                          
                                                          
<!--l. 905--><p class="indent" >
                                                          
                                                          
   <h2 class="chapterHead"><span class="titlemark">Chapter&#x00A0;6</span><br /><a 
 id="x1-510006"></a>NoSQL</h2> So far, we have been focusing on read
only data stores and analytics. Business intelligence and
analytics are extremely important for us to understand users
and thus improve the business and operational efficiency.
Without transaction-oriented applications that provide users
the services, however, what data can we analyze? From
this chapter, we switch to online transaction processing
(OLTP) data stores. OLTP systems facilitate and manage
transaction-oriented applications, typically for data entry and
retrieval transaction processing. OLTP applications are high
throughput and insert or update-intensive in database
management. These applications are used concurrently by a
huge number of users. For example, social networking are
serving hundred millions users at the same time. The key goals
of OLTP applications are availability, speed, concurrency and
recoverability.
<!--l. 909--><p class="noindent" >In particular, this chapter is about NoSQL (Not Only SQL)
database. Departing from relational model, NoSQL is a hot
term nowadays although the name is kind of misleading. The
data model (e.g., key-value pair, document, or graph) is surely
very different from the tabular relations in the RDBMS.
However, these non-relational data models are actually not new.
For example, BerkeleyDB, a key-value store, was initially
released in 1994 (20 years ago as of writing this book).
In the web and social network era, the motivations of
(distributed) NoSQL movement are mainly towards to
horizontal scaling and high availability. By playing with the
CAP theorem, many NoSQL stores compromise consistency in
favor of availability and partition tolerance, which also
brings the simplicity of design. Note that a distributed
database system does not have to drop consistency. For
                                                          
                                                          
instance, TeraData, Google&#8217;s F1, and Apache Trafodion are
ACID-compliant. Correspondingly, these systems are much
more complicated.
<!--l. 911--><p class="indent" >   This chapter starts with the CAP theorem, which plays a
central role in many distribution systems&#8217; design. Based on the
CAP theorem, distributed database can be classified into CP or
AP schools. Unfortunately, there are a lot of misunderstandings
of the theorem. We will dive into what exactly the theorem
means. Then we will look into Zookeeper that essentially an
in-memory database. In practice, it is mostly used as a
coordination service shared by many distributed applications.
The following section studies Apache HBase, a popular NoSQL
database on Hadoop. Modeled after Google&#8217;s BigTable, HBase
brings real-time random access to Hadoop. While HBase
provides row-rise strong consistency, Riak, an open source
implementation of Amazon&#8217;s Dynamo, is an example of
high available NoSQL database that compromises the
consistency. With the knowledge of HBase/BigTable and
Riak/Dynamo, it is easy for us to understand the design of
Apache Cassandra that is a hybrid of BigTable?s data model
and Dynamo?s system design. Finally, we discuss MongoDB. As
the most popular NoSQL database, MongoDB employs a
document data model and provides friendly data-centric
API.
   <h3 class="sectionHead"><span class="titlemark">6.1   </span> <a 
 id="x1-520006.1"></a>The CAP Theorem</h3>
<!--l. 915--><p class="noindent" >In PODC 2000, Eric Brewer conjectured that a distributed
shared-data system cannot simultaneously provide all three of
the following desirable properties <span class="cite">[<a 
href="#XBrewer:2000:TRD">25</a>]</span>:
      <dl class="description"><dt class="description">
                                                          
                                                          
<span 
class="cmbx-12">Consistency</span> </dt><dd 
class="description">All nodes see the same data at the same time.
      It is equivalent to having a single up-to-date copy of
      the data.
      </dd><dt class="description">
<span 
class="cmbx-12">Availability</span> </dt><dd 
class="description">Every request received by a non-failing node
      in the system must result in a response. Even when
      severe  network  failures  occur,  every  request  must
      terminate.
      </dd><dt class="description">
<span 
class="cmbx-12">Partition tolerance</span> </dt><dd 
class="description">The  system  continues  to  operate
      despite arbitrary message loss or failure of part of the
      system.</dd></dl>
<!--l. 921--><p class="noindent" >In 2002, Gilbert and Lynch proved this in the
asynchronous<span class="footnote-mark"><a 
href="bigdata11.html#fn1x7"><sup class="textsuperscript">1</sup></a></span><a 
 id="x1-52001f1"></a>  and
partially synchronous<span class="footnote-mark"><a 
href="bigdata12.html#fn2x7"><sup class="textsuperscript">2</sup></a></span><a 
 id="x1-52002f2"></a> 
network models <span class="cite">[<a 
href="#XGilbert:2002:BCF">40</a>]</span>. Thus it is called the CAP Theorem<a 
 id="dx1-52003"></a>
now.
<!--l. 923--><p class="indent" >   Firstly one should notice that the definition of consistency
in CAP is different from the one in ACID<a 
 id="dx1-52004"></a> (Atomicity,
                                                          
                                                          
Consistency, Isolation, Durability). The consistency in
ACID means that a transaction preserves all the database
rules. On the other hand, the consistency in CAP refers
only to single copy consistency, a strict subset of ACID
consistency.
<!--l. 925--><p class="indent" >   The CAP theorem attempted to justify the design
formulation of &#8220;2 of 3&#8221; CAP properties, leaving three
viable design options: CP, AP, and CA. However, CA
is not really a coherent option in distributed computing
because a system that is not Partition-tolerant will be
forced to give up Consistency or Availability during a
partition<span class="footnote-mark"><a 
href="bigdata13.html#fn3x7"><sup class="textsuperscript">3</sup></a></span><a 
 id="x1-52005f3"></a> .
Therefore, the theorem is generally interpreted as: during a
network partition, a distributed system must choose either
Consistency or Availability. Note that CAP should allow perfect
C and A most of the time since partitions are rare. In fact, a
simple centralized algorithm meets these requirements. In
practice, it is common assuming that a single datacenter has no
partitions within, and thus allows the designs for CA within a
single site.
<!--l. 927--><p class="indent" >   Furthermore, a distributed system may not be simply
classified as CP or AP because the choice between C and A can
occur many times within the same system at very fine
granularity <span class="cite">[<a 
href="#XBrewer:2012">26</a>]</span>. Not only can subsystems make different
choices, but the choice can change according to the operation or
even the specific data or user involved.
                                                          
                                                          
<!--l. 929--><p class="indent" >   In the CAP theorem, the three properties are treated as
binary. For example, Gilbert and Lynch require 100%
availability for simplicity. But the availability could be
continuous from 0 to 100 percent in real world. The consistency
can also have many levels, e.g. different C in CAP and ACID.
Due to the latency, the system may also have disagreement
about whether a partition exists. In practice, the essence of
CAP takes place during a timeout, a period when the program
must make the partition decision. Pragmatically a partition is a
time bound on communication. Therefore, there is no global
notion of a partition, since some nodes might detect a partition,
and others might not.
   <h3 class="sectionHead"><span class="titlemark">6.2   </span> <a 
 id="x1-530006.2"></a>ZooKeeper</h3>
<!--l. 934--><p class="noindent" >In YARN, the Resource Manager is a single point of failure
(SPOF). Multiple Resource Manager instances can be brought
up for fault tolerance but only one instance is Active. When
the Active goes down or becomes unresponsive, another
Resource Manager has to be elected to be the Active.
Such a leader election problem is common for distributed
systems with a active/standby design. YARN relays on
ZooKeeper for electing the new Active. In fact, distributed
systems also face other common problems such as naming
service, configuration management, synchronization, group
membership, etc. ZooKeeper<a 
 id="dx1-53001"></a> is a highly reliable distributed
coordination service for all these use cases <span class="cite">[<a 
href="#XZooKeeper">21</a>]</span>. Higher order
constructs, e.g. barriers, message queues, locks, two-phase
commit, and leader election, can also be implemented with
ZooKeeper. In the rest of book, we will find that many
                                                          
                                                          
distributed services depend on the ZooKeeper, which is actually
the goal of ZooKeeper: implementing the coordination
service once and well and shared by many distributed
applications.
<!--l. 937--><p class="indent" >   Essentially, ZooKeeper is a distributed in-memory CP data
store that has the following guarantees:
      <dl class="description"><dt class="description">
<span 
class="cmbx-12">Sequential Consistency</span> </dt><dd 
class="description">Updates  from  a  client  will  be
      applied in the order that they were sent.
      </dd><dt class="description">
<span 
class="cmbx-12">Atomicity</span> </dt><dd 
class="description">Updates  either  succeed  or  fail.  No  partial
      results.
      </dd><dt class="description">
<span 
class="cmbx-12">Single System Image</span> </dt><dd 
class="description">A client will see the same view of
      the service regardless of the server that it connects to.
      </dd><dt class="description">
<span 
class="cmbx-12">Reliability</span> </dt><dd 
class="description">Once an update has been applied, it will persist
      from that time forward until a client overwrites the
      update.
      </dd><dt class="description">
<span 
class="cmbx-12">Timeliness</span> </dt><dd 
class="description">The clients view of the system is guaranteed
      to be up-to-date within a certain time bound.</dd></dl>
<!--l. 946--><p class="noindent" >
   <h4 class="subsectionHead"><span class="titlemark">6.2.1   </span> <a 
 id="x1-540006.2.1"></a>Data Model</h4>
                                                          
                                                          
<!--l. 947--><p class="noindent" >ZooKeeper has a hierarchal namespace, much like a file system.
The major difference is that each node (called znode)
in the namespace, both internal node and leaf, can have
associated data. The data stored at each znode is accessed
atomically. Reads get all the data bytes associated with a
znode and a write replaces it. To achieve high throughput
and low latency, ZooKeeper keeps all the data in main
memory. For recoverability, updates are logged to disk
and the whole data tree is also snapshot in a fuzzy way
(of both the data content and snapshot frequency). So
ZooKeeper is like an in-memory key-value pair data store, of
which the key namespace is organized in a tree structure.
However, ZooKeeper is not intended to be used as a general
database or large object store. In fact, the ZooKeeper
client and the server implementations have sanity checks to
ensure that znodes have less than 1M of data. In practice,
the data should be at the scale of kilobytes on average
as ZooKeeper is designed to manage coordination data
such as configuration, status information, rendezvous,
etc.
<!--l. 949--><p class="indent" >   Each znode has an Access Control List (ACL<a 
 id="dx1-54001"></a>) and a stat
structure that includes timestamps and version numbers for
data changes and ACL changes. ZooKeeper stamps each update
in the form of zxid (ZooKeeper Transaction Id), which exposes
the total ordering of all changes to ZooKeeper. When a znode&#8217;s
data or ACL changes, the version numbers increase too. For
every read, the client also receives the version of the data. And
when it performs an update or a delete, it must supply
the version of the data. If the version it supplies doesn&#8217;t
match the current version of the data, the update will
fail.
                                                          
                                                          
<!--l. 951--><p class="indent" >   Clients can also set watches on znodes. A watches is an
one-time trigger. Changes to a znode trigger the watch
associated with it and then clear the watch. When a watch
triggers, the client will receive a notification from ZooKeeper.
Watches are sent asynchronously to watchers. But ZooKeeper
guarantees that a client will see a watch event for a znode it is
watching before seeing the new data that corresponds to that
znode. Besides, the order of watch events from ZooKeeper
corresponds to the order of the updates as seen by the
ZooKeeper service.
<!--l. 953--><p class="indent" >   Specially, ZooKeeper also has ephemeral nodes, which exist
as long as the session that created the znode is active. When
the session ends, the znode is deleted. With ephemeral nodes,
we can easily implement the group membership of distributed
systems. The group is represented by a znode. Each group
member can create an ephemeral node under the group node. If
a member leaves or fails abnormally, the corresponding znode
will be deleted automatically when ZooKeeper detects the
failure.
<!--l. 955--><p class="indent" >   Another special kind of znode is sequence node whose name
is automatically appended a monotonically increasing counter
by ZooKeeper. This counter is unique to the parent znode. A
simple way of implementing leader election with ZooKeeper
is to use sequence and ephemeral nodes under a group
node. The process that created the znode with the smallest
appended sequence number is the leader. If the group size is
not very big, all application processes can watch upon
the current smallest znode. If the leader goes offline, the
corresponding ephemeral node is removed and and all other
processes can observe who is the new leader. If the group is
very large, this design may cause a burst of operations
                                                          
                                                          
that ZooKeeper has to process, referred as to the &#8220;herd
effect&#8221;. An alternative approach is that each client watches
upon only the largest znode that is smaller than its own
znode. When a process receives a notification that the
smallest znode is removed, it then executes the leader
procedure. This avoids the herd effect as only one process is
notified.
<!--l. 957--><p class="indent" >   With watches and sequence nodes, one may also implement
message queues with ZooKeeper. Just like not using ZooKeeper
as a general database, however, it is not recommended to
replace the normal message queue with ZooKeeper. The design
of ZooKeeper does not fit the typical use cases of message
queues. The performance of ZooKeeper is bad if there are many
nodes with thousands of children. The 1MB size limit of
ZooKeeper also prevents large messages.
<!--l. 959--><p class="noindent" >
   <h4 class="subsectionHead"><span class="titlemark">6.2.2   </span> <a 
 id="x1-550006.2.2"></a>Atomic Broadcast</h4>
<!--l. 960--><p class="noindent" >To be fault tolerant, ZooKeeper should be replicated over a sets
of hosts called an ensemble. The servers that make up
the ZooKeeper service must all know about each other.
As long as a majority of the servers are available, the
ZooKeeper service will be available. More specifically, the
service requires at least 2<span 
class="cmmi-12">f </span>+ 1 servers to tolerate up to <span 
class="cmmi-12">f</span>
crash failures. In practice, a ZooKeeper service usually
consists of three to seven machines. Because Zookeeper
requires a majority, it is best to use an odd number of
machines.
<!--l. 962--><p class="indent" >   <hr class="figure"><div class="figure" 
>
                                                          
                                                          
<a 
 id="x1-550011"></a>
                                                          
                                                          

<!--l. 963--><p class="noindent" ><img 
src="images/zookeeper.jpg" alt="PIC"  
>
<br /> <div class="caption" 
><span class="id">Figure&#x00A0;6.1: </span><span  
class="content">ZooKeeper Components</span></div><!--tex4ht:label?: x1-550011 -->
                                                          
                                                          
<!--l. 967--><p class="indent" >   </div><hr class="endfigure">
<!--l. 969--><p class="indent" >   Every ZooKeeper server services clients and clients connect
to exactly one server. To create a client session, the application
code must provide a connection string containing a comma
separated list of host:port pairs, each corresponding to
a ZooKeeper server. The ZooKeeper client library will
pick an arbitrary server and try to connect to it. If the
client becomes disconnected from the server, the client will
automatically try the next server in the list until a connection is
re-established.
<!--l. 971--><p class="indent" >   To provide high read throughput, ZooKeeper services
the read requests from the local replica of state at each
server. In contrast, all write requests are forwarded to a
single server, referred as to the leader. The leader uses an
atomic broadcast protocol, called Zab<a 
 id="dx1-55002"></a>, to keep all the
servers in sync <span class="cite">[<a 
href="#XReed:2008:STO">67</a>]</span>. Such a leader is elected through a
leader election algorithm and synchronized with a quorum
of other servers, called followers. By sending all updates
through the leader, non-idempotent requests are transformed
into idempotent transactions. To guarantee the correct
transformation, ZooKeeper enforces that there is only one
leader in Zab. And the Zab protocol meets the following
requirements:
      <dl class="description"><dt class="description">
<span 
class="cmbx-12">Reliable delivery</span> </dt><dd 
class="description">If  a  message,  <span 
class="cmmi-12">m</span>,  is  delivered  by  one
      server,  then  it  will  be  eventually  delivered  by  all
      correct servers.
      </dd><dt class="description">
<span 
class="cmbx-12">Total order</span> </dt><dd 
class="description">If message <span 
class="cmmi-12">a </span>is delivered before message <span 
class="cmmi-12">b </span>by
      one  server,  then  every  server  that  delivers  <span 
class="cmmi-12">a </span>and  <span 
class="cmmi-12">b</span>
                                                          
                                                          
      delivers <span 
class="cmmi-12">a </span>before <span 
class="cmmi-12">b</span>.
      </dd><dt class="description">
<span 
class="cmbx-12">Causal order</span> </dt><dd 
class="description">If  message  <span 
class="cmmi-12">a </span>causally  precedes  message  <span 
class="cmmi-12">b</span>
      and  both  messages  are  delivered,  then  <span 
class="cmmi-12">a </span>must  be
      ordered before <span 
class="cmmi-12">b</span>.
      </dd><dt class="description">
<span 
class="cmbx-12">Prefix property</span> </dt><dd 
class="description">If <span 
class="cmmi-12">m </span>is the last message delivered for a
      leader <span 
class="cmmi-12">L</span>, any message proposed before <span 
class="cmmi-12">m </span>by <span 
class="cmmi-12">L </span>must
      also be delivered.</dd></dl>
<!--l. 979--><p class="indent" >   Zab at a high level is a leader based protocol similar to
Paxos<a 
 id="dx1-55003"></a> <span class="cite">[<a 
href="#XLamport:1998:PP">54</a>]</span>. Compared to Paxos, Zab is primarily designed for
primary-backup systems rather than for state machine replication.
The Zab protocol consists of two modes: recovery/leader
activation and broadcast/active messaging. When the service
starts or after a leader failure, Zab transitions to recovery
mode. Recovery mode ends when a leader emerges and a
quorum of servers have synchronized their state with the leader.
Synchronizing their state consists of guaranteeing that the
leader and new server have the same state. Once a leader has a
quorum of synchronized followers, it accepts messages to
propose and coordinates message delivery. The broadcast
looks just like two-phase commit <span class="cite">[<a 
href="#Xopac:2009">23</a>]</span> without the need to
handle aborts and all communication channels are assumed
FIFO:
      <ul class="itemize1">
      <li class="itemize">The leader sends proposals to all followers in the order
      that requests have been received. Before proposing a
      message the leader assigns a monotonically increasing
      unique zxid.
                                                          
                                                          
      </li>
      <li class="itemize">Followers  process  messages  in  the  order  they  are
      received.
      </li>
      <li class="itemize">The leader will issue a COMMIT to all followers as
      soon as a quorum of followers have ACKed a message.
      </li>
      <li class="itemize">Followers deliver the message when they receive the
      COMMIT from the leader.</li></ul>
   <h3 class="sectionHead"><span class="titlemark">6.3   </span> <a 
 id="x1-560006.3"></a>HBase</h3>
<!--l. 991--><p class="noindent" >Apache HBase <span class="cite">[<a 
href="#XHBase">11</a>]</span><a 
 id="dx1-56001"></a> is modeled after Google&#8217;s BigTable <span class="cite">[<a 
href="#XChang:2006:BDS">27</a>]</span><a 
 id="dx1-56002"></a>,
implemented in Java, and run on top of Apache Hadoop. A
competing open source project is Apache Accumulo <span class="cite">[<a 
href="#XAccumulo">5</a>]</span><a 
 id="dx1-56003"></a> that
shares very similar architecture and features (especially now
HBase 0.98 supports cell-level security that was a unique offer
from Accumulo).
<!--l. 993--><p class="noindent" >
   <h4 class="subsectionHead"><span class="titlemark">6.3.1   </span> <a 
 id="x1-570006.3.1"></a>Data Model</h4>
<!--l. 995--><p class="noindent" >In BigTable-like stores, data are stored in tables, which are
made of rows and columns. Columns are grouped into column
families. A column name is made of its column family prefix
and a qualifier. The column family prefix must be composed of
printable characters. The column qualifiers can be made
of any arbitrary bytes. In HBase, column families must
be declared up front at schema definition time whereas
                                                          
                                                          
new columns can be added to any column family without
pre-announcing them. The only way to get a complete set of
columns that exist for a column family is to scan all the
rows.
<!--l. 997--><p class="indent" >   Table row keys are uninterrpreted byte arrays. Rows are
lexicographically sorted by row keys. In HBase, the empty
byte array is used to denote both the start and end of a
table&#8217;s namespace. A cell&#8217;s content is an uninterpreted array
of bytes. And table cells are versioned. A (row, column,
version) tuple exactly specifies a cell. The version is specified
using a long integer. Typically this long contains time
instances.
<!--l. 1000--><p class="indent" >   The four primary data model operations are Scan,
Get, Put, and Delete. Scan allows iteration over multiple
rows while Get returns columns for a specified row. It
can be specified to retrieve everything, or all columns
from specific families, or specific columns. By default,
when doing a Get, the latest/highest version of the cell is
returned. It is possible to return more than one version with
Get.setMaxVersions() or to return versions other than the latest
by Get.setTimeRange(). Without specifying the version, Put
always creates a new version of a cell with the server&#8217;s
currentTimeMillis. But the user may specify the version on a
per-column level. The user-provided version may be a time in
the past or the future, or a non-time purpose long value. To
overwrite an existing value, an exact version should be
provided. Delete can happen on a specific version of a cell or
all versions. To save space, HBase also cleans up old or
expired versions. To declare how much data to retain,
one may define the number of versions or the time to live
(TTL).
                                                          
                                                          
<!--l. 1003--><p class="indent" >   Deletes work by creating tombstone markers. Once a
tombstone<a 
 id="dx1-57001"></a> marker is set, the &#8220;deleted&#8221; cells become effectively
invisible for Get and Scan operations but are not immediately
removed from store files. There is a snag with the tombstone
approach, namely &#8220;Deletes mask Puts&#8221;<a 
 id="dx1-57002"></a>. Once a tombstone
marker is set, even Puts after the Delete will be masked by the
delete tombstone. Performing the Put will not fail. However
when you do a Get, the Put has no effect but will start working
after the major compaction, which will really remove deletes
and tombstone markers.
<!--l. 1005--><p class="noindent" >
   <h4 class="subsectionHead"><span class="titlemark">6.3.2   </span> <a 
 id="x1-580006.3.2"></a>Storage</h4>
<!--l. 1007--><p class="noindent" >Physically, HBase uses HDFS to store data. Empty cells
are not stored as tables usually have a large number of
columns and are very sparse. In HBase, tables are stored on a
per-column family basis. All column family members are
stored together on HDFS. Recall that HDFS is a write-once
(appending-only since 0.20) file system. It is very efficient for
reading a large portion of big files but not designed for
random access. So how does HBase provide random, realtime
read/write access on top HDFS (which is actually the exact
reason to build HBase)? Here we come to the concept of
Store. In HBase, a Store corresponds to a column family
in a Region (see next section for details). A Store hosts
a MemStore<a 
 id="dx1-58001"></a> and a set of zero or more StoreFiles<a 
 id="dx1-58002"></a>. The
MemStore holds in-memory modifications to the Store
                                                          
                                                          
<span class="footnote-mark"><a 
href="bigdata14.html#fn4x7"><sup class="textsuperscript">4</sup></a></span><a 
 id="x1-58003f4"></a>.
When the MemStore reaches a certain size or the total
size of all MemStores reaches the upper limit (both
are configureable), the sorted key-value pairs (the key
is a (row, column, version) tuple) in MemStore will be
flushed into a HDFS file called StoreFile in HFile<a 
 id="dx1-58004"></a> format
<span class="footnote-mark"><a 
href="bigdata15.html#fn5x7"><sup class="textsuperscript">5</sup></a></span><a 
 id="x1-58005f5"></a>.
<!--l. 1010--><p class="indent" >   HFile is based on SSTable<a 
 id="dx1-58007"></a> file in the BigTable. An SSTable
provides a persistent, ordered immutable map from keys to
values, where both keys and values are arbitrary byte strings.
Because it is ordered, SSTable supports both lookups by
key and iterations over all key-value pairs in a specified
key range. Each SSTable contains a sequence of blocks of
configurable size. The end of SSTable contains a location
index of blocks. With the index loaded into memory, a
lookup can be performed with a single disk seek. The
appropriate block is found by a binary search in the in-memory
index and is then read from disk. An SSTable may be
completely mapped into memory to avoid frequent I/O
operations.
<!--l. 1012--><p class="indent" >   For a valid write operation, it is firstly written to the
write-ahead-log (WAL)<a 
 id="dx1-58008"></a> and then its contents are inserted into
the MemStore. In contrast, a read operation is performed on
the merged view of the MemStore and all StoreFiles. If there
                                                          
                                                          
are a lot of StoreFiles that are not in memory, a read operation
will involve many disk accesses. To reduce the number of I/O,
Bloom filters <span class="cite">[<a 
href="#XBloom:1970:STH">24</a>]</span> can be created for StoreFiles of a particular
Store (in HFile v2, Bloom filters are created at block level). By
checking if a StoreFile might contain a specified key with
Bloom filters, HBase significantly reduces the number
of disk seeks required for read operations. Bloom filters
also avoid disk access for most lookups of non-existent
keys.
<!--l. 1014--><p class="indent" >   A Store may have many StoreFiles that are created for each
flush. Over time, many versions of a row may exist in different
StoreFiles. Each of these versions may have different sets of
columns. Thus, reading a whole (or a large portion of) row
could require many seeks in different files if too many files
accumulate. In order to reduce the number of StoreFiles per
Store, a background process called compaction is executed to
merge StoreFiles. There are two types of compactions:
minor and major. Minor compactions pick up a couple of
smaller adjacent StoreFiles and rewrite them as one. Minor
compactions do not drop deletes and expired cells. In contrast,
major compactions pick up all the StoreFiles in the Store and
generate a single StoreFile per Store that removes deletes and
expired cells.
<!--l. 1016--><p class="indent" >   The compaction improves the average latency of reads at
the expense of rewriting the same data multiple times (write
amplification). Especially, the strategy how to choose files to
merge in minor compaction has a big impact on the balance
between read performance and write amplification. HBase
supports pluggable compaction policies, including ratio based,
exploring, and stripe compaction.
<!--l. 1018--><p class="indent" >   This storage design is actually a kind of log-structured
                                                          
                                                          
merge-tree (LSM tree) <span class="cite">[<a 
href="#XO'Neil96thelog-structured">64</a>]</span><a 
 id="dx1-58009"></a>, which maintains data in two (or
more) separate structures, each of which is optimized for
its respective underlying storage medium. And data is
synchronized between the two structures efficiently. A
log-structured storage engine avoids overwrites and uses
sequential writes to update data, which is friendly for
both hard disks (HDD) and solid-state disks (SSD). The
sequential writes avoid random disk seek on HDD and thus
greatly improve latency. On SSD, sequential writes avoids
write amplification and disk failure and thus improve both
performance and SSD lifetime. Besides HBase, LevelDB,
RocksDB, WiredTiger, Apache Cassandra, and SQLite4 also
employ LSM trees.
   <h4 class="subsectionHead"><span class="titlemark">6.3.3   </span> <a 
 id="x1-590006.3.3"></a>Architecture</h4>
<!--l. 1022--><p class="noindent" >HBase is a distributed database designed to run on a cluster of
machines. HBase supports horizontal scalability by auto-sharding,
which means that tables are dynamically partitioned by rows
and distributed by the system.
<!--l. 1024--><p class="indent" >   <hr class="figure"><div class="figure" 
>
                                                          
                                                          
<a 
 id="x1-590012"></a>
                                                          
                                                          

<!--l. 1025--><p class="noindent" ><img 
src="images/hbase-architecture.png" alt="PIC"  
>
<br /> <div class="caption" 
><span class="id">Figure&#x00A0;6.2: </span><span  
class="content">HBase Architecture</span></div><!--tex4ht:label?: x1-590012 -->
                                                          
                                                          
<!--l. 1029--><p class="indent" >   </div><hr class="endfigure">
   <h5 class="subsubsectionHead"><span class="titlemark">6.3.3.1   </span> <a 
 id="x1-600006.3.3.1"></a>Sharding</h5>
<!--l. 1033--><p class="noindent" >The basic unit of sharding is called a Region<a 
 id="dx1-60001"></a> in HBase. A
region is a contiguous and sorted range of rows of a table
stored together on disk. Initially, there is only one region
for a table. However, when regions become too large, a
region is split into two at the middle key (recall that rows
are lexicographically sorted by row keys). Regions are
served by RegionServer<a 
 id="dx1-60002"></a>. Each RegionServer is responsible a
set of regions but one region can be served only by one
RegionServer.
<!--l. 1035--><p class="indent" >   Typically, HBase setups a RegionServer co-located with an
HDFS DataNode on the same physical node. When StoreFiles
are written into HDFS, one copy is written locally and two are
written to other nodes. As long as the regions are not
moved, there is good data locality. When the regions are
reassigned to a new RegionServer, the data locality is lost
and the RegionServer needs to read the data over the
network from remote DataNodes until the data is rewritten
locally.
<!--l. 1037--><p class="indent" >   In general, HBase is designed to run with a small (20-200)
number of relatively large (5-20Gb) regions per RegionServer. A
large number of regions per RegionServer will cause a
lot memory overhead and possibly too many flushes and
compactions. When there are too many regions, one can
consolidate them with the utility Merge.
<!--l. 1039--><p class="indent" >   The coordination work of assigning a region to a RegionServer
is done by the Master server, which is implemented by
HMaster<a 
 id="dx1-60003"></a>. The Master typically runs on the NameNode. Each
                                                          
                                                          
region is assigned to a RegionServer on startup. However, the
Master may decide to move a region from one RegionServer to
another for load balance. Besides, the Master also handles
RegionServer failures by assigning the regions to another
RegionServer. The Master also performs administrative
operations such as monitoring all RegionServer instances and
all metadata changes (e.g. create, modify, and delete tables or
column families).
<!--l. 1041--><p class="indent" >   The catalog table hbase:meta (previously called META),
maintained by the Master, keeps a list of all regions in the
system. The hbase:meta exists as an HBase table. However, it
cannot be split and therefore consists of a single region. The
location of hbase:meta is stored in Zookeeper. For each
region, the hbase:meta table contains the region id, table
name, start key, the RegionServer serving this region,
etc.
<!--l. 1043--><p class="indent" >   The HBase client consists of two main parts: HBaseAdmin<a 
 id="dx1-60004"></a>
and HTable<a 
 id="dx1-60005"></a>. HBaseAdmin communicates with the Master to
execute administrative operations. HTable communicates
directly with the RegionServers to manipulate data. To locate
the RegionServers that are serving the particular row range of
interest, HTable queries the hbase:meta table. It then contacts
the corresponding RegionServer directly for data access. The
region information is cached in the client so that subsequent
requests need not go through the lookup process. If a region be
reassigned, the client will refresh the location information by
querying hbase:meta.
<!--l. 1045--><p class="noindent" >
   <h5 class="subsubsectionHead"><span class="titlemark">6.3.3.2   </span> <a 
 id="x1-610006.3.3.2"></a>Fault Tolerance</h5>
                                                          
                                                          
<!--l. 1047--><p class="noindent" >The Master looks like a single point of failure. Actually, we can
set up multiple Masters although only one is active. If the
active Master shuts down or loses its lease in ZooKeeper, the
remaining Masters compete to take over the active role.
Because the clients talk directly to the RegionServers, the
HBase cluster can still function in a steady state in short period
during the Master failover.
<!--l. 1049--><p class="indent" >   Recall that a region is managed by a single RegionServer at
a time. If a RegionServer fails, the corresponding regions are
not available until the detection and recovery steps have
happened. It is actually a single point of failure although
there are no global failures in HBase. Especially when the
RegionServer serving hbase:meta fails, the Master cannot
perform administrative tasks and new clients cannot proceed as
normal.
<!--l. 1051--><p class="indent" >   To be resilient to node failures, all StoreFiles are written
into HDFS. Besides, the write-ahead-log (WAL)<a 
 id="dx1-61001"></a> is also
written into HDFS. The Master detects the silent death
of RegionServers by watching the connections between
RegionServers and ZooKeeper. ZooKeeper itself employs
heartbeats. On a timeout, the Master declares the RegionServer
as dead and starts the recovery process. During the recovery,
the regions are reassigned to random RegionServers and each
RegionServer reads the WAL to recover the correct region
state. This is a complicated process and the mean time to
recovery (MTTR<a 
 id="dx1-61002"></a>) of HBase is often around 10 minutes if a
DataNode crash with default settings. But we may reduce
the MTTR to less than 2 minutes with careful settings
<span class="cite">[<a 
href="#XHBaseMTTR">56</a>]</span>.
                                                          
                                                          
<!--l. 1053--><p class="noindent" >
   <h5 class="subsubsectionHead"><span class="titlemark">6.3.3.3   </span> <a 
 id="x1-620006.3.3.3"></a>Cluster Replication</h5>
<!--l. 1055--><p class="noindent" >HBase provides a cluster replication mechanism which keeps
one cluster&#8217;s state synchronized with that of another cluster.
Cluster replication is useful to backup and disaster recovery,
geographic data distribution, or offline data analytics, etc.
Cluster replication employs a source-push methodology that
uses the WAL of the source cluster to propagate the changes.
The WAL records all the mutations (Put/Delete) and
the source cluster Region Servers ship the edits to the
destination cluster Region Servers, which replay all the
updates. The replication is done asynchronously and each
RegionServer replicates their own stream of WAL edits. In a
write heavy application, the destination cluster may notably lag
behind from the source. Zookeeper plays a key role in
cluster replication, where it coordinates almost all the major
replication activities.
<!--l. 1057--><p class="indent" >   In practice, cluster replication can be implemented in
multiple modes:
      <dl class="description"><dt class="description">
<span 
class="cmbx-12">Master-Slave Replication</span> </dt><dd 
class="description">The replication is done in a
      single direction, i.e., transactions from one cluster are
      pushed to other cluster.
      </dd><dt class="description">
<span 
class="cmbx-12">Master-Master Replication</span> </dt><dd 
class="description">The   replication   is   sent
      across  in  both  the  directions,  for  different  or  same
      tables, i.e., both the clusters are acting both as master
      and slave. In the case that they are replicating the
      same table, cluster id is used to prevent replication
      loops.
                                                          
                                                          
      </dd><dt class="description">
<span 
class="cmbx-12">Cyclic Replication</span> </dt><dd 
class="description">More  than  two  clusters  takes  part
      in  replication  setup.  There  are  various  possible
      combinations of master-slave and master-master set
      up between any two clusters.</dd></dl>
<!--l. 1067--><p class="noindent" >
   <h5 class="subsubsectionHead"><span class="titlemark">6.3.3.4   </span> <a 
 id="x1-630006.3.3.4"></a>Consistency</h5>
<!--l. 1068--><p class="noindent" >Because one region can be served only by one RegionServer,
HBase provides row level consistency. In fact, HBase supports
row-level ACID<a 
 id="dx1-63001"></a> in limited ways. Basically, multiple Puts and
Deletes to the same row provide all ACID guarantees. Besides,
the <span 
class="pcrr7t-x-x-120">mutateRowsWithLocks </span>method of <span 
class="pcrr7t-x-x-120">HRegion </span>can be
used by coprocessors to implement atomic operations at the
region level.
<!--l. 1070--><p class="indent" >   But HBase does not guarantee any consistency between
regions. Although all rows returned will consist of a complete
row that existed at some point in the table&#8217;s history, a Scan is
not a consistent view of a table. Moreover, HBase has no mixed
read/write transactions.
<!--l. 1072--><p class="noindent" >
   <h5 class="subsubsectionHead"><span class="titlemark">6.3.3.5   </span> <a 
 id="x1-640006.3.3.5"></a>Timeline-Consistent High Available Reads</h5>
<!--l. 1073--><p class="noindent" >With the design that each region is served by only one
RegionServer, HBase has the strong consistency guarantee.
However, when a RegionServer become unavailable, the hosted
regions become unavailable during detection and recovery. On
the other hand, some applications prefer high availability for
                                                          
                                                          
reads. For example, the data may be read-only or stale data is
acceptable. Through &#8220;region replication&#8221;, HBase provides
timeline-consistent high available reads. In this mode (region
replication is set to 2 or more), the master will assign the
replicas of the regions to multiple RegionServers. All of the
replicas for a single region will have a unique replica_id,
starting from 0. The region replica having replica_id 0 is called
the primary region, and the others secondary regions. Only the
primary can accept writes from the client and thus the writes
are not highly-available. The writes are asynchronously sent to
the secondary region replicas using Async WAL replication,
which works similarly to cluster replication but instead happens
inside the cluster.
<!--l. 1075--><p class="indent" >   For reads, one may provide an additional consistency
parameter (STRONG or TIMELINE). In case of the STRONG
consistency, the read is always performed by the primary
regions. In case of TIMELINE consistency, the read RPC will
be sent to the primary region server first. If the primary does
not respond back in a short interval (10ms by default), parallel
RPC to secondary region replicas will be sent. Then the result
is returned from whichever RPC is finished first. The API
Result.isStale() is added to inspect if the result is from a
secondary region.
<!--l. 1077--><p class="indent" >   The feature is called timeline-consistent because the
secondaries apply the edits in the order that the primary
committed them. Therefore, the secondaries will contain a
snapshot of the primaries data at any time point. However, the
client could observe edits out-of-order, and can go back in time,
if it observes reads from one secondary replica first, then
another secondary replica because there is no stickiness to
region replicas or a transaction-id based guarantee.
                                                          
                                                          
<!--l. 1079--><p class="indent" >   Note that the goal and design of this feature is not to
balance the load of reads because the read requests are always
sent to the primary. Instead, it tries to satisfy the low latency
guarantees for some applications.
<!--l. 1081--><p class="noindent" >
   <h4 class="subsectionHead"><span class="titlemark">6.3.4   </span> <a 
 id="x1-650006.3.4"></a>Security</h4>
<!--l. 1082--><p class="noindent" >Data security is important for any real world applications.
HBase offers a strong security model including authentication,
authorization, and encryption.
<!--l. 1084--><p class="indent" >   HBase can be configured to use Kerberos<a 
 id="dx1-65001"></a> authentication at
the RPC layer via Simple Authentication and Security Layer
(SASL)<a 
 id="dx1-65002"></a>. If enabled, it ensures that only authorized users can
communicate with HBase. Note that the underlying HDFS
should also be configured to use strong authentication.
Otherwise, there is no benefit at all. Even better, HBase
optionally supports transparent encryption for protecting HFile
and WAL data at rest. Besides, the RPC between HBase
and clients can also be configured to employ encrypted
communication.
<!--l. 1086--><p class="indent" >   Once the users are authenticated via Kerberos, HBase&#8217;s
authorization mechanism allows restricted access for specified
users. An access control list (ACL) specifies which users shall be
granted access to an object (table or column family), as
well as which operations (read, write, create, execute,
admin) are allowed. Note that the hbase:meta table is
readable by every user, regardless of the user&#8217;s other grants or
restrictions.
                                                          
                                                          
<!--l. 1089--><p class="noindent" >
   <h5 class="subsubsectionHead"><span class="titlemark">6.3.4.1   </span> <a 
 id="x1-660006.3.4.1"></a>Cell Level Security</h5>
<!--l. 1091--><p class="noindent" >Since version 0.98, ACLs in HBase can be specified at the cell
scope. Cell-level ACLs are implemented using tags, which
require HFile v3. A tag is a piece of metadata which is part of a
cell, separate from the key, value, and version. Besides
cell-level ACLs, HBase can store visibility expressions
into tags, providing cell-level security capabilities similar
to Apache Accumulo. The visibility labels allow us to
label cells and control access to labelled cells, to further
restrict who can read or write to certain subsets of the
data.
<!--l. 1093--><p class="indent" >   When mutations are applied, users can specify a security
label for each cell by passing a CellVisibility object. Security
labels consist of a set of user-defined tokens that are required to
read the associated cell. The security label expression syntax
supports boolean logic operations. When a client attempts to
read data, any security labels present are examined against the
set of authorizations passed with Scanner. If the authorizations
are determined to be insufficient to satisfy the security label,
the cell is suppressed from the results. Each user has a set of
associated security labels, which can be manipulated in the
shell.
<!--l. 1095--><p class="indent" >   Both ACL controller and visibility label controller are
implemented as coprocessors, which we will discuss in details in
what follows.
<!--l. 1097--><p class="noindent" >
   <h4 class="subsectionHead"><span class="titlemark">6.3.5   </span> <a 
 id="x1-670006.3.5"></a>Coprocessor</h4>
<!--l. 1099--><p class="noindent" >Coprocessors<a 
 id="dx1-67001"></a> provide a powerful way to add custom functionalities
                                                          
                                                          
at the server side against locally-stored data <span class="cite">[<a 
href="#XHBaseCoprocessor">52</a>]</span>. This allows
users to efficiently summarize, filter, and aggregate data
directly on RegionServers. Compared to MapReduce, it
gives a dramatic performance improvement by removing
communication overheads.
<!--l. 1101--><p class="indent" >   The HBase coprocessor framework provides a library and
runtime environment for executing user code within the
RegionServers and the Master process. Coprocessors that can
be loaded globally on all tables and regions hosted by a region
server are called system coprocessors. In contrast, coprocessors
that are loaded on all regions for a table on a per-table basis are
known as table coprocessors. Moreover, the framework supports
two different types of coprocessors, the observers and the
endpoints.
<!--l. 1103--><p class="indent" >   The observers, like triggers in RDBMS, are executed from
core HBase code when certain events occur. HBase includes
three observer interfaces: RegionObserver provides hooks for
data manipulation events; WALObserver provides hooks for
write-ahead log related operations; and MasterObserver
provides hooks for DDL-type operation. Multiple observers can
be loaded at one place and they are chained to execute
sequentially by order of assigned priorities.
<!--l. 1105--><p class="indent" >   The endpoints, resembling stored procedures, can be
invoked at any time from the client and be executed remotely
at the target region or regions. The endpoint is an interface for
dynamic RPC extension. The endpoint implementation is
installed on the server side and can then be invoked with HBase
RPC. The client library provides convenience methods for
invoking such dynamic interfaces.
                                                          
                                                          
<!--l. 1107--><p class="noindent" >
   <h4 class="subsectionHead"><span class="titlemark">6.3.6   </span> <a 
 id="x1-680006.3.6"></a>Summary</h4>
<!--l. 1108--><p class="noindent" >As a BigTable clone, HBase provides a wide-column data model
and random real-time CRUD operations on top of HDFS. They
can horizontally scale out to efficiently serve billions of
rows and millions of columns by auto-sharding. Because
each region is served by only one RegionServer at a time,
they also support strong consistency for reads and writes.
Automatic failover of RegionServer is supported although
efforts are needed to reduce MTTR. With replications across
multi data centers, HBase adds more supports of disaster
recovery.
<!--l. 1111--><p class="noindent" >
   <h3 class="sectionHead"><span class="titlemark">6.4   </span> <a 
 id="x1-690006.4"></a>Riak</h3>
<!--l. 1112--><p class="noindent" >Riak <span class="cite">[<a 
href="#XRiak">22</a>]</span><a 
 id="dx1-69001"></a> is a highly available key-value store from Basho
Technologies. Riak is written in Erlang and is modeled after
Amazon.com&#8217;s Dynamo <span class="cite">[<a 
href="#XDeCandia:2007:DAH">31</a>]</span><a 
 id="dx1-69002"></a>. It is designed to provide an
&#8220;always-on&#8221; experience while sacrificing consistency under
certain scenarios. Riak is architected for low-latency, high
availability, fault-tolerance, operational simplicity, and
scalability.
<!--l. 1114--><p class="noindent" >
   <h4 class="subsectionHead"><span class="titlemark">6.4.1   </span> <a 
 id="x1-700006.4.1"></a>Data Model</h4>
<!--l. 1116--><p class="noindent" >Riak has a schemaless design and organizes data into the
Buckets of Objects. A buckets is essentially a flat namespace
and an object is comprised of a key/value pair. The unique keys
                                                          
                                                          
in a bucket are simply opaque binary strings to identify objects.
Interaction with the database is by retrieving or modifying the
entire object/value. There is no partial fetch or update of the
data. Besides, no operations (Get, Put, Post, and Delete) span
multiple data items.
<!--l. 1118--><p class="indent" >   Although it is very simple, the key/value pair data
model is very flexible. For example, it is frequently used for
session storage, user accounts, settings, and preferences,
etc. In general, the applications should be able to get the
keys &#8220;for free&#8221;, without having to perform any queries to
discover them, e.g. user id, session id, etc. If Bitcask is
used as storage backend, Riak also supports automatic
expiry of keys, which is helpful to session management
<span class="footnote-mark"><a 
href="bigdata16.html#fn6x7"><sup class="textsuperscript">6</sup></a></span><a 
 id="x1-70001f6"></a>. Since it is
schemaless, binaries such as images and PDF documents can be stored
directly <span class="footnote-mark"><a 
href="bigdata17.html#fn7x7"><sup class="textsuperscript">7</sup></a></span><a 
 id="x1-70002f7"></a> .
On the other hand, structured data (e.g. relational tables)
should be denormalized and usually stored as JSON or
XML.
<!--l. 1120--><p class="indent" >   Initially, Riak is completely agnostic toward the data stored
within it. However, Riak introduces several eventually
convergent Data Types in version 2.0. Inspired by <span class="cite">[<a 
href="#XCRDT2011">72</a>]</span>, Riak
Data Types are convergent replicated data types (CRDTs<a 
 id="dx1-70003"></a>, also
called commutative replicated data types). By default, Riak is
                                                          
                                                          
an eventual consistency system, in which a replica may execute
an operation without synchronizing with other replicas. The
operation is sent asynchronously to other replicas; every replica
eventually applies all updates, possibly in different orders.
Conflict resolution can be very difficult in Riak, which we will
discuss in details later.
<!--l. 1122--><p class="indent" >   Riak Data Types are introduced to relieve developers on
conflict resolution in some situations with builtin convergence
logic. Riak currently provides five CRDTs: <span 
class="pcrr7t-x-x-120">flag</span>, <span 
class="pcrr7t-x-x-120">register</span>,
<span 
class="pcrr7t-x-x-120">counter</span>, <span 
class="pcrr7t-x-x-120">set</span>, and <span 
class="pcrr7t-x-x-120">map</span>. They are operations based from the
point view of Riak clients. Of them, flags and registers cannot
be stored in a bucket/key by themselves. Instead, they must be
embedded in <span 
class="pcrr7t-x-x-120">map</span>s.
      <dl class="description"><dt class="description">
<span 
class="cmbx-12">Flag</span> </dt><dd 
class="description">A flag may take the value <span 
class="pcrr7t-x-x-120">enable </span>or <span 
class="pcrr7t-x-x-120">disable</span>. Flags
      support only two operations: <span 
class="pcrr7t-x-x-120">enable </span>and <span 
class="pcrr7t-x-x-120">disable</span>.
      </dd><dt class="description">
<span 
class="cmbx-12">Register</span> </dt><dd 
class="description">Registers  are  essentially  named  binaries.  The
      only operation that registers support is to change the
      binaries stored in them.
      </dd><dt class="description">
<span 
class="cmbx-12">Counters</span> </dt><dd 
class="description">Counters   take   integer   values   and   support
      increment and decrement operations. Therefore, it is
      not necessary to fetch, mutate, or put a counter.
      </dd><dt class="description">
<span 
class="cmbx-12">Sets</span> </dt><dd 
class="description">Sets  are  collections  of  unique  binary  values.  The
      available operations on sets include add an element,
      remove an element, add multiple elements, or remove
      multiple elements.
                                                          
                                                          
      </dd><dt class="description">
<span 
class="cmbx-12">Maps</span> </dt><dd 
class="description">A  map  is  a  collection  of  fields  that  supports  the
      nesting  of  multiple  Data  Types,  including  maps
      themselves. One can add or remove fields to/from the
      map.</dd></dl>
<!--l. 1144--><p class="indent" >   In the section of Consistency, we will discuss the details how
Data Types resolve conflicts.
   <h4 class="subsectionHead"><span class="titlemark">6.4.2   </span> <a 
 id="x1-710006.4.2"></a>Storage</h4>
<!--l. 1148--><p class="noindent" >In contrast to HBase and Accumulo replying on complicated
Hadoop HDFS, Riak stores data in the native file system.
Moreover, Riak supports pluggable storage backends, including
Bitcask, LevelDB, and Memory. It is possible to run multiple
backends within a single Riak cluster, which employs different
storage backends for different buckets.
<!--l. 1150--><p class="noindent" >
   <h5 class="subsubsectionHead"><span class="titlemark">6.4.2.1   </span> <a 
 id="x1-720006.4.2.1"></a>Bitcask</h5>
<!--l. 1151--><p class="noindent" >Bitcask is the default storage backend and uses a log-structured
hash table design <span class="cite">[<a 
href="#XBitcask">73</a>]</span><a 
 id="dx1-72001"></a>. A Bitcask instance is a directory, and
only one process can open the Bitcask for writing at a given
time. At any time point, the file being written is called the
active file of Bitcask. When the active file meets a size
threshold it will be closed and a new active file will be
created. Once a file is closed, it is considered immutable
and will never be opened for writing again. The active
file is only written by appending a key/value entry (with
timestamp and crc). Deletion is simply a write of a special
                                                          
                                                          
tombstone value. Thus, a Bitcask data file is simply a linear
sequence of the key/value entries. Such a log structured design
allows for minimal disk head movement for a stream of
writes.
<!--l. 1153--><p class="indent" >   Bitcask keeps an in-memory hash-table of all keys, called
keydir. A keydir maps every key in a Bitcask to a data
structure including file id, value size, value position in the
file, and timestamp. After an append completes, keydir is
updated too. With the kydir, a read needs only single
disk seek. However, it also results in a memory-bounded
keyspace.
<!--l. 1156--><p class="indent" >   A Bitcask may use up a lot of space over time because old
or deleted values still exist in the files. A merge process is used
for the compaction of Bitcask. The merge process iterates over
all non-active files in a Bitcask and produces a set of data files
containing only the live or latest versions of each present
key. Besides, it also creates a hint file for each data file.
A hint file is like the data files but each entry contains
the position and size of the value instead of the value
itself.
<!--l. 1158--><p class="indent" >   During the startup, we scan all of the data files in a
Bitcask to build the keydir. If the hint file exists for a data
file, it will be scanned instead for a much quicker startup
time.
<!--l. 1160--><p class="noindent" >
   <h5 class="subsubsectionHead"><span class="titlemark">6.4.2.2   </span> <a 
 id="x1-730006.4.2.2"></a>LevelDB</h5>
<!--l. 1161--><p class="noindent" >To store a large number of keys, LevelDB is preferred. LevelDB<a 
 id="dx1-73001"></a>
is an open source on-disk key-value store <span class="cite">[<a 
href="#XLevelDB">29</a>]</span><a 
 id="dx1-73002"></a>. It shares the
same general design as the BigTable tablet stack and is written
                                                          
                                                          
by Google fellows Jeffrey Dean and Sanjay Ghemawat. LevelDB
supports batching writes, forward and backward iteration, and
compression of the data via Google&#8217;s Snappy compression
library.
<!--l. 1163--><p class="indent" >   Although LevelDB uses a similar MemTable/SSTable design
as BigTable, the organization of the database files is different.
Each database is represented by a set of files organized into a
sequence of levels. Each level stores approximately ten times as
much data as the previous level. Especially, an appending-only
log file stores a sequence of recent updates. When the log file
reaches a pre-determined size, it is converted to a sorted table
and a new log file is created for future updates. The sorted
table generated from a log file is placed in a special <span 
class="cmti-12">young </span>level
(also called level-0).
<!--l. 1165--><p class="indent" >   Within a level (<span 
class="cmmi-12">L &#x003E;</span>= 1), LevelDB guarantees there&#8217;s no
overlapping in SSTables, meaning a row can appear at
most in one file on a single level. When the size of level <span 
class="cmmi-12">L</span>
exceeds its limit (says 10<sup><span 
class="cmmi-8">L</span></sup> MB), LevelDB will compact
it in a background thread. The compaction picks a file
from level <span 
class="cmmi-12">L </span>and all overlapping files from the next level
<span 
class="cmmi-12">L </span>+ 1. A compaction merges the contents of the picked
files to produce a sequence of level-(<span 
class="cmmi-12">L </span>+ 1) files. During
compaction, LevelDB will create a new level-(<span 
class="cmmi-12">L </span>+ 1) file when
the current output file has reached the target file size, or the
key range of the current output file has grown enough
to overlap more then ten level-(<span 
class="cmmi-12">L </span>+ 2) files. The second
rule ensures that a later compaction of a level-(<span 
class="cmmi-12">L </span>+ 1)
file will not pick up too much data from level-(<span 
class="cmmi-12">L </span>+ 2).
Compactions for a particular level rotate through the key
space.
<!--l. 1167--><p class="indent" >   Compactions from level-0 to level-1 are treated specially
                                                          
                                                          
because files in level-0 may overlap each other. When the
number of young files exceeds a certain threshold, all of the
young files are merged together with all of the overlapping
level-1 files to produce a sequence of new level-1 files. The
compactions gradually migrate new updates from the young
level to the largest level. There is no major compaction that
merge all files into one.
<!--l. 1169--><p class="indent" >   LevelDB may have to do a few disk seeks to satisfy a read:
one disk seek per level. The worst case is bounded at the total
number of levels. But it may require much less disk seek in
practice. If 10% of the database fits in memory, only one seek
for the last level is needed because all of the earlier levels
should end up cached in the OS buffer cache for most file
systems. If 1% fits in memory, LevelDB will need two
seeks.
<!--l. 1171--><p class="indent" >   Riak uses a fork of LevelDB, which is optimized to meet
Basho&#8217;s requirements such as multiple databases running
simultaneously, fast disaster recovery, aggressive delete
logic, etc. For example, this fork moves away from the
original single thread design to a thread pool that can run
simultaneous companions on the same database as well as other
databases.
<!--l. 1173--><p class="noindent" >
   <h5 class="subsubsectionHead"><span class="titlemark">6.4.2.3   </span> <a 
 id="x1-740006.4.2.3"></a>Memory</h5>
<!--l. 1174--><p class="noindent" >The Memory<a 
 id="dx1-74001"></a> storage backend uses in-memory tables to store all
data (both keys and values) but the data is never saved to disk
or any other persistence storage. It may be useful to store small
amounts of transient state.
                                                          
                                                          
<!--l. 1176--><p class="noindent" >
   <h4 class="subsectionHead"><span class="titlemark">6.4.3   </span> <a 
 id="x1-750006.4.3"></a>Architecture</h4>
<!--l. 1178--><p class="noindent" ><hr class="figure"><div class="figure" 
>
                                                          
                                                          
<a 
 id="x1-750013"></a>
                                                          
                                                          

<!--l. 1179--><p class="noindent" ><img 
src="images/riak-ring.png" alt="PIC"  
>
<br /> <div class="caption" 
><span class="id">Figure&#x00A0;6.3: </span><span  
class="content">Riak Ring</span></div><!--tex4ht:label?: x1-750013 -->
                                                          
                                                          
<!--l. 1183--><p class="noindent" ></div><hr class="endfigure">
<!--l. 1185--><p class="indent" >   All nodes in a Riak cluster are equal. Each node is fully
capable of serving any client request. There is no &#8220;master&#8221;.
This uniformity provides the basis for Riak&#8217;s fault-tolerance and
scalability. The symmetric architecture is based on consistent
hashing to distribute data around the cluster. In consistent
hashing, the output range of a hash function is treated as a
ring. Riak uses the SHA1 hash function to map the keys of data
items to an 160-bit integer space which is divided into
equally-sized partitions. Each virtual node (vnode) will claim a
partition on the ring. The physical nodes each attempt
to run roughly an equal number of vnodes. Consistent
hashing ensures data is evenly distributed around the
cluster.
<!--l. 1187--><p class="indent" >   Nodes can be added and removed from the cluster
dynamically and Riak will redistribute the data accordingly.
The ring state is shared around the cluster by a gossip protocol.
Whenever a node changes its claim on the ring, it announces its
change via this protocol. It also periodically re-announces what
it knows about the ring, in case any nodes missed previous
updates.
<!--l. 1189--><p class="indent" >   Riak automatically and asynchronously replicates data
to <span 
class="cmmi-12">N </span>(default <span 
class="cmmi-12">N </span>= 3) separate partitions on the Riak
Ring. Note that <span 
class="cmmi-12">N </span>= 3 simply means that three different
partitions/vnodes will receive copies of the data. There are no
guarantees that the three replicas will go to three different
physical nodes although Riak attempts to distribute the data
evenly.
<!--l. 1191--><p class="indent" >   <hr class="figure"><div class="figure" 
>
                                                          
                                                          
<a 
 id="x1-750024"></a>
                                                          
                                                          

<!--l. 1192--><p class="noindent" ><img 
src="images/riak-data-distribution.png" alt="PIC"  
>
<br /> <div class="caption" 
><span class="id">Figure&#x00A0;6.4: </span><span  
class="content">Riak Data Distribution</span></div><!--tex4ht:label?: x1-750024 -->
                                                          
                                                          
<!--l. 1196--><p class="indent" >   </div><hr class="endfigure">
<!--l. 1198--><p class="indent" >   For a write/Put request, any node may participate as the
coordinator for the request. The coordinating node consults the
ring state to determine which vnode owns the partition which
the value&#8217;s key belongs to, then sends the Put request to that
vnode, as well as the vnodes responsible for the next <span 
class="cmmi-12">N </span><span 
class="cmsy-10x-x-120">- </span>1
partitions in the ring. The Put request may also specify that at
least <span 
class="cmmi-12">W </span>(<span 
class="cmmi-12">&#x003C;</span>= <span 
class="cmmi-12">N</span>) of those vnodes reply with success, and that
<span 
class="cmmi-12">DW </span>(<span 
class="cmmi-12">&#x003C;</span>= <span 
class="cmmi-12">W</span>) reply with success only after durably storing the
value.
<!--l. 1200--><p class="indent" >   The read/Get request operates similarly. The request is
sent to the vnode/partition in which the key resides, as
well as to the next <span 
class="cmmi-12">N </span><span 
class="cmsy-10x-x-120">- </span>1 partitions. The request also
specifies <span 
class="cmmi-12">R </span>(<span 
class="cmmi-12">&#x003C;</span>= <span 
class="cmmi-12">N</span>), the number of vnodes that must reply
before a response is returned. With <span 
class="cmmi-12">R </span>= 1, the client will
get faster response but take a chance of receiving an old
copy.
<!--l. 1202--><p class="indent" >   To ensure high variability during node failures and network
partitions, Riak uses the concept of &#8220;sloppy quorum&#8221;<a 
 id="dx1-75003"></a>. That is,
all read and write operations are performed on the first <span 
class="cmmi-12">N</span>
healthy nodes from the preference list, which may not
always be the first <span 
class="cmmi-12">N </span>nodes encountered while walking the
consistent hashing ring. When a node is unavailable, the
neighboring nodes will temporarily accept its write requests
<span class="footnote-mark"><a 
href="bigdata18.html#fn8x7"><sup class="textsuperscript">8</sup></a></span><a 
 id="x1-75004f8"></a>.
When the node returns, data is transferred to the primary node
via the &#8220;hinted handoff&#8221;<a 
 id="dx1-75005"></a> process.
                                                          
                                                          
   <h4 class="subsectionHead"><span class="titlemark">6.4.4   </span> <a 
 id="x1-760006.4.4"></a>Consistency</h4>
<!--l. 1206--><p class="noindent" >By default, Riak is an eventual consistency system, in which a
replica may execute an operation without synchronizing with
other replicas. Therefore, conflicts between replicas of an object
are inevitable.
<!--l. 1208--><p class="indent" >   In read and write requests, the default values of <span 
class="cmmi-12">R </span>and <span 
class="cmmi-12">W</span>
are quorum, where quorum indicates a majority of the <span 
class="cmmi-12">N </span>value
(<span 
class="cmmi-12">N&#x2215;</span>2 + 1, or 2 for the default <span 
class="cmmi-12">N </span>value of 3). Consider that a
failed node just recovered but does not have requested
key-value or has an old copy, or that the client reads the
value immediately after a successful write such that the
replication process is not finished yet. Because <span 
class="cmmi-12">W </span>= 2 and
<span 
class="cmmi-12">R </span>= 2, the coordinating node will receive at least one
response with latest value, which will be returned to the
client. In general, one should set <span 
class="cmmi-12">W </span>+ <span 
class="cmmi-12">R &#x003E; N </span>so that a
read hopefully reflects the most recent write. Meanwhile a
read repair process will occur to force the errant nodes
to update their object values based on the value of the
successful read. Read repair is a passive process that is only
triggered when data is read. Riak also has an automatic
background process called active anti-entropy (AAE) that
compares and repairs any divergent, missing, or corrupted
replicas.
<!--l. 1212--><p class="indent" >   In above, we simply say that the read returns &#8220;latest&#8221;
value with <span 
class="cmmi-12">W </span>+ <span 
class="cmmi-12">R &#x003E; N</span>. However, it sounds simple but
is actually very challenging to determine which copy is
newer.
<!--l. 1214--><p class="noindent" >
                                                          
                                                          
   <h5 class="subsubsectionHead"><span class="titlemark">6.4.4.1   </span> <a 
 id="x1-770006.4.4.1"></a>Last Write Wins</h5>
<!--l. 1215--><p class="noindent" >A simple strategy is last-write-wins (LWW)<a 
 id="dx1-77001"></a>, which is based on
timestamps (recall that Riak attaches a timestamp for each
write). On a single server, we can easily to tell which write is
newer based on timestamp regardless of the accuracy of system
clock (assuming that the clock is always increasing). However,
the definition of &#8220;last&#8221; is hard in a distributed system
as clocks can not be perfectly synchronized. Therefore,
last-write-wins will necessarily drop some acknowledged writes
in the case of concurrent updates. A more reliable way to
use logical time such as vector clock or dotted version
vector.
<!--l. 1218--><p class="noindent" >
   <h5 class="subsubsectionHead"><span class="titlemark">6.4.4.2   </span> <a 
 id="x1-780006.4.4.2"></a>Vector Clock and Dotted Version Vector</h5>
<!--l. 1219--><p class="noindent" >When a value is stored in Riak, it is tagged with a piece
of metadata called a &#8220;causal context&#8221;<a 
 id="dx1-78001"></a> which establishes
the object&#8217;s initial version. Causal context comes in one
of two possible forms: vector clock and dotted version
vector.
<!--l. 1221--><p class="indent" >   Vector clock <span class="cite">[<a 
href="#Xfidge1988timestamps">33</a>,&#x00A0;<a 
href="#XMattern89virtualtime">58</a>]</span><a 
 id="dx1-78002"></a> is an algorithm for generating a partial
ordering of events in a distributed system and detecting
causality violations. A vector clock of a system of <span 
class="cmmi-12">N </span>processes is
a vector of <span 
class="cmmi-12">N </span>logical clocks, one clock per process. When a
key-value pair is added into bucket, it is tagged with a vector
clock of all zeros as the initial version. Later the vector clock is
extended for each update so that two versioned replicas can be
compared to determine:
      <ul class="itemize1">
      <li class="itemize">Whether one object is a direct descendant of the other
                                                          
                                                          
      </li>
      <li class="itemize">Whether  the  objects  are  direct  descendants  of  a
      common parent
      </li>
      <li class="itemize">Whether the objects are unrelated in recent heritage</li></ul>
<!--l. 1228--><p class="indent" >   Vector clocks can detect concurrent updates to the same
object but they may have the problem &#8220;sibling explosion&#8221;. If an
object is updated by five different clients concurrently
and tagged with the same vector clock, then five values
should be created as siblings. Depending on the order of
delivery of those updates to the different replicas, however,
sibling values may be duplicated and then result in sibling
explosion.
<!--l. 1230--><p class="indent" >   Dotted version vector <span class="cite">[<a 
href="#XPreguica:2012:BAE">66</a>]</span><a 
 id="dx1-78003"></a>, on the other hand, identify
each value with the update that created it. If five clients
concurrently update the object, each of these updates will be
marked with a dot (a minimal vector clock) that indicates the
specific event that introduced it. Therefore, duplicate values can
be identified and removed, reducing the likelihood of sibling
explosion
<!--l. 1232--><p class="indent" >   With causal context, each node of replicas can auto-repair
out-of-sync data when feasible. For siblings that are created by
multiple clients that concurrently reads a key-value and writes
it back, Riak cannot reconcile automatically. Prior to version
2.0, Riak simply accepts both writes. When a read comes for
the same key, Riak sends all the versions for that key and lets
the client to do manual reconciliation.
<!--l. 1234--><p class="indent" >   To relieve developers from handling data convergence at the
application level, Riak introduces Data Types, which have
builtin convergence rules:
                                                          
                                                          
<div class="center" 
>
<!--l. 1236--><p class="noindent" >
<div class="tabular"> <table id="TBL-2" class="tabular" 
cellspacing="0" cellpadding="0"  
><colgroup id="TBL-2-1g"><col 
id="TBL-2-1"><col 
id="TBL-2-2"></colgroup><tr 
class="hline"><td><hr></td><td><hr></td></tr><tr  
 style="vertical-align:baseline;" id="TBL-2-1-"><td  style="white-space:nowrap; text-align:left;" id="TBL-2-1-1"  
class="td11"><span 
class="cmbx-12">Data Type</span></td><td  style="white-space:wrap; text-align:left;" id="TBL-2-1-2"  
class="td11"><!--l. 1239--><p class="noindent" ><span 
class="cmbx-12">Convergence rule</span>                          </td>
</tr><tr 
class="hline"><td><hr></td><td><hr></td></tr><tr  
 style="vertical-align:baseline;" id="TBL-2-2-"><td  style="white-space:nowrap; text-align:left;" id="TBL-2-2-1"  
class="td11">Flags          </td><td  style="white-space:wrap; text-align:left;" id="TBL-2-2-2"  
class="td11"><!--l. 1241--><p class="noindent" ><span 
class="pcrr7t-x-x-120">enable </span>wins over <span 
class="pcrr7t-x-x-120">disable</span>.                </td>
</tr><tr  
 style="vertical-align:baseline;" id="TBL-2-3-"><td  style="white-space:nowrap; text-align:left;" id="TBL-2-3-1"  
class="td11">Registers     </td><td  style="white-space:wrap; text-align:left;" id="TBL-2-3-2"  
class="td11"><!--l. 1242--><p class="noindent" >The  most  chronologically  recent  value
wins, based on timestamps.                   </td>
</tr><tr  
 style="vertical-align:baseline;" id="TBL-2-4-"><td  style="white-space:nowrap; text-align:left;" id="TBL-2-4-1"  
class="td11">Counters     </td><td  style="white-space:wrap; text-align:left;" id="TBL-2-4-2"  
class="td11"><!--l. 1243--><p class="noindent" >Each actor keeps an independent count for
increments and decrements; upon merge,
the pairwise maximum of the counts for
each actor will win.                              </td>
</tr><tr  
 style="vertical-align:baseline;" id="TBL-2-5-"><td  style="white-space:nowrap; text-align:left;" id="TBL-2-5-1"  
class="td11">Sets            </td><td  style="white-space:wrap; text-align:left;" id="TBL-2-5-2"  
class="td11"><!--l. 1244--><p class="noindent" >If  an  element  is  concurrently  added  and
removed, the add will win.                    </td>
</tr><tr  
 style="vertical-align:baseline;" id="TBL-2-6-"><td  style="white-space:nowrap; text-align:left;" id="TBL-2-6-1"  
class="td11">Maps          </td><td  style="white-space:wrap; text-align:left;" id="TBL-2-6-2"  
class="td11"><!--l. 1245--><p class="noindent" >If a field is concurrently added or updated
and removed, the add/update will win.    </td>
</tr><tr 
class="hline"><td><hr></td><td><hr></td></tr><tr  
 style="vertical-align:baseline;" id="TBL-2-7-"><td  style="white-space:nowrap; text-align:left;" id="TBL-2-7-1"  
class="td11">         </td></tr></table></div></div>
<!--l. 1250--><p class="indent" >   Note that Riak Data Types do not guarantee strong
consistency. Besides, the built in convergence rules may not be
suitable in your applications.
<!--l. 1252--><p class="noindent" >
   <h5 class="subsubsectionHead"><span class="titlemark">6.4.4.3   </span> <a 
 id="x1-790006.4.4.3"></a>Strong Consistency</h5>
<!--l. 1253--><p class="noindent" >In version 2.0, strong consistency was added to complement
Riak&#8217;s standard eventually consistent, high availability mode.
When data is stored in a bucket with strong consistency
guarantees, a value is guaranteed readable by any client
immediately after a successful write has occurred to a given key.
In this sense, single-key strongly consistent operations are
atomic, and operations on a given key are linearizable. In
this mode, a quorum of primary vnodes responsible for
                                                          
                                                          
the key must be online and reachable or the request will
fail.
<!--l. 1255--><p class="noindent" >
   <h4 class="subsectionHead"><span class="titlemark">6.4.5   </span> <a 
 id="x1-800006.4.5"></a>Summary</h4>
<!--l. 1256--><p class="noindent" >In the default mode, Riak is in the school of AP systems to
provide an &#8220;always-on&#8221; experience. With a symmetric design,
all nodes in a Riak cluster are equal and each node is fully
capable of serving any client request. With sloppy quorum
and hinted handoff, Riak can process client requests even
during node failures or network partitions. On the other
hand, the eventually consistent model also bring a lot of
challenges to developers. Riak provides multiple options such as
last-write-wins, causal context, and Data Types to help conflict
resolution. They are suitable for different use cases although
none provides strong consistency. Finally, Riak 2.0 has an
optional mode to guarantee strong consistency. It is the user&#8217;s
call to be AP or CP.
<!--l. 1259--><p class="noindent" >
   <h3 class="sectionHead"><span class="titlemark">6.5   </span> <a 
 id="x1-810006.5"></a>Cassandra</h3>
<!--l. 1260--><p class="noindent" >Apache Cassandra <span class="cite">[<a 
href="#XLakshman:2010:CDS">53</a>,&#x00A0;<a 
href="#XCassandra">6</a>]</span><a 
 id="dx1-81001"></a> is a hybrid of BigTable&#8217;s data model
and Dynamo&#8217;s system design. Therefore, Cassandra provides
the flexible wide columnar model, and has linear scalability and
proven fault-tolerance on commodity hardware. Besides,
Cassandra&#8217;s support for replicating across multiple data centers
is best-in-class. Since many features of Cassandra are already
covered in previous sections as they are shared with HBase and
                                                          
                                                          
Riak, we will focus on the additional unique features in what
follows.
<!--l. 1262--><p class="noindent" >
   <h4 class="subsectionHead"><span class="titlemark">6.5.1   </span> <a 
 id="x1-820006.5.1"></a>Data Model</h4>
<!--l. 1264--><p class="noindent" >Cassandra provides a two-dimensional row-column view to the
data contained in a keyspace (i.e. table in HBase). Keyspaces
are used to group column families together. If you need
higher dimension to organize application data, there is the
concept of super columns, which are columns that contain
columns. However, super column is deprecated because of
performance issues. Instead, developers are encouraged to
use composite columns that was introduced in version
0.8.1.
<!--l. 1266--><p class="indent" >   Before jumping into composite columns, we need to
understand column sorting. Just like other key-value pair
databases, the data type of keys and values in Cassandra
are byte arrays. More interestingly, we can specify how
column names will be compared for sort order when results
are returned to the client. But why would we want to
sort column names? This especially sounds strange to a
relational database developer. In a relational database, we
usually have tall tables, i.e. millions skinny rows with
a handful columns. We could still follow this design in
Cassandra although different rows do not have to share same
column set. On the other hand, one wide row could have
millions columns in BigTable-like database, actually up
to 2 billion columns in Cassandra. In this case, column
names are usually part of the data, rather than purely
schema. For example, we can build inverted index with
                                                          
                                                          
terms as the keys, document ids as the column names, and
frequency as the value. One may even go with valueless
columns, i.e. column names are data themselves and the
values are not really meaningful. With wide-row design, it
is necessary to compare column names sometimes, for
instance, each row is the time series of stock price in a
day and the column names are time points. You can use
<span 
class="pcrr7t-x-x-120">compare</span><span 
class="pcrr7t-x-x-120">_with </span>attribute on a column family to tell Cassandra
how to sort the columns. The default is <span 
class="pcrr7t-x-x-120">BytesType</span>, which
is a straightforward lexical comparison of the bytes in
each column. Other options are <span 
class="pcrr7t-x-x-120">AsciiType</span>, <span 
class="pcrr7t-x-x-120">UTF8Type</span>,
<span 
class="pcrr7t-x-x-120">LexicalUUIDType</span>, <span 
class="pcrr7t-x-x-120">TimeUUIDType</span>, and <span 
class="pcrr7t-x-x-120">LongType</span>. You can
also specify the fully-qualified class name to a class extending
<span 
class="pcrr7t-x-x-120">org.apache.cassandra.db.marshal.AbstractType</span>.
Again, we are sorting column names, not values. However,
sorting column name providing a way to build second
index, which is very useful in real world. Now come back to
composite columns, which are arbitrary dimensional column
names that can have types like CompositeType(UTF8Type,
ReversedType(TimeUUIDType), LongType)). It is also really
simple: it is implemented as a comparator so adds very little
complexity.
<!--l. 1268--><p class="noindent" >
   <h4 class="subsectionHead"><span class="titlemark">6.5.2   </span> <a 
 id="x1-830006.5.2"></a>Storage</h4>
<!--l. 1270--><p class="noindent" >Cassandra uses a storage structure similar to BigTable,
including SSTable and MemTable. The details can be found in
the section of HBase. Compared to HBase, Cassandra stores
data in the native file system as Cassandra emphasizes
operational simplicity.
                                                          
                                                          
<!--l. 1272--><p class="noindent" >
   <h4 class="subsectionHead"><span class="titlemark">6.5.3   </span> <a 
 id="x1-840006.5.3"></a>Architecture</h4>
<!--l. 1274--><p class="noindent" >Same as Riak, Cassandra employs a ring topology but with
more partition options. You can provide any IPartitioner<a 
 id="dx1-84001"></a>
implementation to distribute data on nodes. Out of the box,
Cassandra provides Murmur3Partitioner, RandomPartitioner,
ByteOrderedPartitioner, and OrderPreservingPartitioner. Both
Murmur3Partitioner and RandomPartitioner force equal spacing
of tokens around the hash space. ByteOrderedPartitioner orders
rows lexically by key bytes. OrderPreservingPartitioner is
similar to ByteOrderedPartitioner but treats keys as UTF8
strings. With OrderPreservingPartitioner the keys themselves
are used to place on the ring. It brings data locality but also
potential bottleneck on hot spots.
<!--l. 1276--><p class="indent" >   Beyond partitions, Cassandra also supports pluggable
replication<a 
 id="dx1-84002"></a> strategies through IReplicaPlacementStrategy to
ensure reliability and fault tolerance. Out of the box, Cassandra
provides SimpleStrategy (rack unaware), LocalStrategy (rack
aware) and NetworkTopologyStrategy (datacenter aware). In
addition to setting the number of replicas, the strategy
sets the distribution of the replicas across the nodes in
the cluster depending on the cluster&#8217;s topology. We are
particularly interested in NetworkTopologyStrategy. With it, we
can deploy the cluster across multiple data centers and
specify how many replicas we want in each data center. If
configured properly, Cassandra is able to read locally without
incurring cross-datacenter latency, and handles failures
nicely.
<!--l. 1278--><p class="indent" >   With NetworkTopologyStrategy, the first replica is placed
                                                          
                                                          
according to the partitioner. Additional replicas are placed by
walking the ring clockwise until a node in a different rack is
found. If no such node exists, additional replicas are placed in
different nodes in the same rack. To achieve this, we need a
snitch maps IPs to racks and data centers. It defines how the
nodes are grouped together within the overall network
topology.
<!--l. 1282--><p class="noindent" >
   <h4 class="subsectionHead"><span class="titlemark">6.5.4   </span> <a 
 id="x1-850006.5.4"></a>CQL</h4>
<!--l. 1283--><p class="noindent" >Initially, Cassandra provides a low level Thrift based API
(insert, get, delete), similar to those of HBase. Later Cassandra
introduces the SQL-like Cassandra Query Language (CQL<a 
 id="dx1-85001"></a>) as
the best-practice interface. With CQL, schema is reintroduced
and is conventional in most applications. Besides ease of use,
native CQL drivers offering improved performance by
providing connection pooling, failover, tracing support,
etc.
<!--l. 1285--><p class="indent" >   The old wide columnar data model is still the foundation of
Cassandra but CQL adds an abstraction layer that hides
implementation details. In CQL, the data model is a partitioned
row store, where rows are organized into tables and the first
component of a table&#8217;s primary key is the partition key.
Within a partition, rows are clustered by the remaining
columns of the compound primary key (called clustering
columns).
<!--l. 1287--><p class="indent" >   Note that CQL does not bring the relational data model
to Cassandra. For example, it does not support joins or
subqueries. Rather, Cassandra emphasizes denormalization
through collection types (set, list, and map)<a 
 id="dx1-85002"></a>.
                                                          
                                                          
<!--l. 1289--><p class="indent" >   CQL also provides secondary index to access data using
attributes other than the partition key. It indexes column values
in a separate, hidden table from the one that contains the
values being indexed. The index is basically an inverted file. It
is not suitable for high-cardinality columns, frequently updated
or deleted columns, or searching for a row in a large partition
unless narrowly queried.
<!--l. 1291--><p class="noindent" >
   <h4 class="subsectionHead"><span class="titlemark">6.5.5   </span> <a 
 id="x1-860006.5.5"></a>Consistency</h4>
<!--l. 1293--><p class="noindent" >Like Riak, Cassandra also has read repairs and active
anti-entropy to resolve some consistency issues. However,
Cassandra&#8217;s conflict resolution simply relays on last-write-wins.
Cassandra 2.0 introduces &#8220;compare and set&#8221; based on Paxos
consensus protocol, but misleadingly labels it as &#8220;lightweight
transaction&#8221;<a 
 id="dx1-86001"></a>. A new IF clause has been introduced for both the
INSERT and UPDATE commands that lets the user invoke
lightweight transactions.
<!--l. 1295--><p class="noindent" >
   <h4 class="subsectionHead"><span class="titlemark">6.5.6   </span> <a 
 id="x1-870006.5.6"></a>Summary</h4>
<!--l. 1296--><p class="noindent" >Overall, Cassandra is a very nice system with flexible data
model, linear scalability and high availability. Cassandra offers
robust support for clusters spanning multiple data centers. CQL
provides a SQL-like alternative to the traditional RPC
interface. Apache Cassandra emphasizes operational simplicity.
It does not require ZooKeeper or any other third-party
components. It is very easy to configure and run. Actually
                                                          
                                                          
Cassandra starts only one JVM per node, which brings a lot of
simplicity to operation and maintenance.
<!--l. 1301--><p class="noindent" >
   <h3 class="sectionHead"><span class="titlemark">6.6   </span> <a 
 id="x1-880006.6"></a>MongoDB</h3>
<!--l. 1302--><p class="noindent" >Although key-value pairs are very flexible, it is tedious to map
them to objects in applications. In this section, we will learn
about a popular document-oriented database MongoDB <span class="cite">[<a 
href="#XMongoDB">61</a>]</span><a 
 id="dx1-88001"></a>.
MongoDB uses JSON-like documents with dynamic schemas,
making the integration of data in certain types of applications
easier and faster. Beyond key search, MongoDB supports search
by value, range queries, and text searches. Any field in a
document can be indexed (by B-Trees, similar to those in
RDBMS).
<!--l. 1304--><p class="noindent" >
   <h4 class="subsectionHead"><span class="titlemark">6.6.1   </span> <a 
 id="x1-890006.6.1"></a>Data Model</h4>
<!--l. 1306--><p class="noindent" >Documents are addressed in the database via a unique key. The
documents contain one or more fields, and each field contains
a value of a specific data type (maybe object or array).
Documents that tend to share a similar structure are organized
as collections. Compared to relational databases, collections
could be considered analogous to tables and documents
analogous to records. However, every record in a table has the
same sequence of fields, while documents in a collection may
have fields that are completely different. MongoDB stores
documents in a binary representation called BSON (Binary
JSON) <span class="cite">[<a 
href="#XBSON">60</a>]</span><a 
 id="dx1-89001"></a>. The BSON encoding extends the JSON to include
                                                          
                                                          
additional types such as date, int, long, double, byte array,
etc.
<!--l. 1308--><p class="noindent" >
   <h4 class="subsectionHead"><span class="titlemark">6.6.2   </span> <a 
 id="x1-900006.6.2"></a>Storage</h4>
<!--l. 1309--><p class="noindent" >MongoDB historically uses the memory mapped files for on-disk
storage. From version 3.0, MongoDB supports pluggable storage
engines, which means a rigorously defined interface between the
database and the way it stores data. The pluggable storage
engine API allows third parties to develop storage engines.
In version 3.0, the additional storage backends include
WiredTiger.
<!--l. 1311--><p class="noindent" >
   <h5 class="subsubsectionHead"><span class="titlemark">6.6.2.1   </span> <a 
 id="x1-910006.6.2.1"></a>Memory-Mapped Storage Engine</h5>
<!--l. 1312--><p class="noindent" >With the memory-mapped storage engine (referred as to
MMAPv1<a 
 id="dx1-91001"></a>), MongoDB uses memory mapped files for managing
and interacting with all data. A memory-mapped file is a
segment of virtual memory which has been assigned a direct
byte-for-byte correlation with some portion of a file or file-like
resource. Once mapped, the relationship between file and
memory allows MongoDB to interact with the data in the file as
if it were memory. MongoDB reads/writes to RAM directly and
OS takes care of the rest. This greatly simplifies the cache and
file access logic in MongoDB and also leverages OS&#8217;s LRU
(least recently used) cache behavior. On the other hand, it
incurs the fragmentation management and read-ahead
overhead. Clearly, this also limits the data size MongoDB
can handle on 32-bit systems due to inherent memory
                                                          
                                                          
limitations.
<!--l. 1315--><p class="indent" >   In this design, each database has a namespace file holding
entries which each points to (the first extent of) a collection or
index in data files. Data files are broken into contiguous disk
space called extents (grow exponentially up to 2GB) to hold
documents or B-Tree index nodes. Data files (and journals) are
aggressively pre-allocated. MongoDB stores each document
(plus some extra padding bytes as growth buffer) in an extent
as a contiguous block. The documents are updated in-place and
the whole document will be moved to a bigger space if the
update increases the size of document beyond its current
allocated space. All data files are memory mapped into the
virtual memory of mongod, the primary daemon process for the
MongoDB system.
<!--l. 1317--><p class="indent" >   <hr class="figure"><div class="figure" 
>
                                                          
                                                          
<a 
 id="x1-910025"></a>
                                                          
                                                          

<!--l. 1318--><p class="noindent" ><img 
src="images/mongodb-storage-structure.png" alt="PIC"  
>
<br /> <div class="caption" 
><span class="id">Figure&#x00A0;6.5: </span><span  
class="content">MongoDB Storage Structure</span></div><!--tex4ht:label?: x1-910025 -->
                                                          
                                                          
<!--l. 1322--><p class="indent" >   </div><hr class="endfigure">
<!--l. 1324--><p class="indent" >   The data and space are mainly organized through
doubly-linked-list. Each collection of data is organized in a
linked list of extents. Each extent points to a head/tail of
another linked list of documents (up to 16MB). Data files may
get fragmented over time because of updates and deletes.
It especially gets worse if documents have various sizes.
Fragmentation wastes memory and disk space and also make
writes scattered and slower. MongoDB provides compact
command for defragmentation.
<!--l. 1326--><p class="indent" >   Changes in memory mapped files are flushed to disk every
60 seconds. MongoDB uses write ahead logging to an on-disk
journal (called oplog) to guarantee write operation durability
and to provide crash resiliency. The write-ahead log is
committed every 100 milliseconds by default (configurable with
-journalCommitInterval). So the maximum data loss is 100 ms
on a hard crash. To achieve durability (i.e. data written to the
on-disk journal when acked), you can use the &#8220;j&#8221; option in write
concern (discussed in details later).
<!--l. 1328--><p class="indent" >   MongoDB uses a readers-writer lock that allows concurrent
read access to a database but exclusive write access to a single
write operation. Before version 2.2, this lock was implemented
on a per-mongod basis. Since version 2.2, the lock has been
implemented at the database level. In a properly designed
schema a write will hold the lock for approximately 10
microseconds. If a slow-running operation is predicted (e.g., a
document or an index entry will need to be paged in from disk),
then that operation will yield the write lock. In version 3.0, the
MMAPv1 storage engine adds support for collection-level
locking by default.
                                                          
                                                          
   <h5 class="subsubsectionHead"><span class="titlemark">6.6.2.2   </span> <a 
 id="x1-920006.6.2.2"></a>WiredTiger</h5>
<!--l. 1331--><p class="noindent" >WiredTiger <span class="cite">[<a 
href="#XWiredTiger">62</a>]</span><a 
 id="dx1-92001"></a> is introduced in version 3.0 to make
MongoDB burn through write-heavy workloads and be
more resource efficient. WiredTiger is a high performance,
scalable, transactional key-value store. WiredTiger takes full
advantage of modern, multi-core servers with access to large
amounts of RAM. WiredTiger&#8217;s transactions use optimistic
concurrency control algorithms that avoid the bottleneck of a
centralized lock manager. WiredTiger offers both LSM tree and
B-tree engines. To minimize on-disk overhead and I/O,
WiredTiger uses compact file formats and compression.
The WiredTiger storage engine provides document-level
locking.
<!--l. 1335--><p class="noindent" >
   <h4 class="subsectionHead"><span class="titlemark">6.6.3   </span> <a 
 id="x1-930006.6.3"></a>Cluster Architecture</h4>
<!--l. 1337--><p class="noindent" >Although MongoDB can run as a single instance, it is often
configured in a cluster environment to provide scalability and
high availability.
<!--l. 1339--><p class="noindent" >
   <h4 class="subsectionHead"><span class="titlemark">6.6.4   </span> <a 
 id="x1-940006.6.4"></a>Replic Set</h4>
<!--l. 1340--><p class="noindent" >High availability is achieved in MongoDB via Replica Set,
which provides data redundancy across multiple physical
servers, including a single primary as well as multiple
secondaries that replicate the primary&#8217;s oplog and apply the
operations to their data sets. The primary accepts all
                                                          
                                                          
write operations from clients and therefore provides strict
consistency.
<!--l. 1342--><p class="indent" >   By default, clients also read from the primary. To improve
read throughput, clients can specify a read preference
to send read operations to secondaries. With &#8220;nearest&#8221;
read preference, the client driver periodically pings the
members and will favor issuing queries to the one with
lowest latency. Notice that read request is issued to only
one node, there is no quorum read or read from multiple
nodes. With read preferences, however, the read results
may not reflect latest writes because replications are done
asynchronously.
<!--l. 1344--><p class="indent" >   MongoDB allows users to specify write availability in the
system, which is called the write concern. Write concern can
include the <span 
class="cmti-12">w </span>option to specify the required number of
acknowledgments from replica set before returning, the <span 
class="cmti-12">j </span>option
to require writes to the journal before returning, and <span 
class="cmti-12">wtimeout</span>
option to specify a time limit to prevent write operations from
blocking indefinitely. Prior to November 2012, MongoDB&#8217;s
client drivers return when the writes had only entered the
client&#8217;s outgoing queue. Now the default write concern
acknowledges writes received by the primary (but before writing
to journal), allowing the client to catch network exceptions and
duplicate key errors.
<!--l. 1346--><p class="indent" >   <hr class="figure"><div class="figure" 
>
                                                          
                                                          
<a 
 id="x1-940016"></a>
                                                          
                                                          

<!--l. 1347--><p class="noindent" ><img 
src="images/mongodb-replica-set.png" alt="PIC"  
>
<br /> <div class="caption" 
><span class="id">Figure&#x00A0;6.6: </span><span  
class="content">MongoDB Replica Set Structure</span></div><!--tex4ht:label?: x1-940016 -->
                                                          
                                                          
<!--l. 1351--><p class="indent" >   </div><hr class="endfigure">
<!--l. 1353--><p class="indent" >   Within the replica set, members are interconnected with
each other to exchange heartbeat message. When a primary
does not communicate with the other members of the set for
more than 10 seconds, the replica set will attempt to select
another member to become the new primary. The first
secondary that receives a majority of the votes becomes
primary. Because of asynchronous replication, the newly
elected primary does not necessary having all the latest
updates.
<!--l. 1355--><p class="indent" >   Note that a new primary may be elected even if the old
one did not crash because of network partition or simply
over-loaded primary. In these situations primaries will have
accepted write operations that have not replicated to the
secondaries after a failover occurs. When the former primary
rejoins the replica set and attempts to continue replication as a
secondary, the former primary must revert these operations to
maintain database consistency across the replica set, which may
result in data loss.
   <h4 class="subsectionHead"><span class="titlemark">6.6.5   </span> <a 
 id="x1-950006.6.5"></a>Sharding</h4>
<!--l. 1359--><p class="noindent" >Although replica set provides data redundancy and potentially
load balance of reads with eventually consistency, it does not
provide linear scalability for writes since all updates still has to
go to the single primary. To load balance writes, MongoDB
provides auto-sharding including range-based, hash-based, and
tag-award sharding. With sharding, a collection is partitioned
into chunks and have chunks distributed across multiple
shards.
<!--l. 1361--><p class="indent" >   <hr class="figure"><div class="figure" 
>
                                                          
                                                          
<a 
 id="x1-950017"></a>
                                                          
                                                          

<!--l. 1362--><p class="noindent" ><img 
src="images/mongodb-replica-set.png" alt="PIC"  
>
<br /> <div class="caption" 
><span class="id">Figure&#x00A0;6.7: </span><span  
class="content">MongoDB Sharding Architecture</span></div><!--tex4ht:label?: x1-950017 -->
                                                          
                                                          
<!--l. 1366--><p class="indent" >   </div><hr class="endfigure">
<!--l. 1368--><p class="indent" >   A sharded MongoDB cluster consists of the shards, config
servers, and routing instances. A shard is a MongoDB
instance that holds a subset of a collection&#8217;s data. Each
shard is usually a replica set in production although it
can be a single mongod instance. Each config server is a
mongod instance that holds metadata about the cluster.
The metadata maps chunks to shards. Each router is a
mongos instance that routes the reads and writes from
applications to the shards. Applications do not access the
shards directly.
<!--l. 1370--><p class="indent" >   For writes, the route server will forward the request to the
corresponding primary server hosting the chunk whose
key range covers the partition key of the document. In
case of reads, the routing server will examine whether the
partition key is part of the selection criteria and if so will only
route the request to the corresponding shard. However,
if the partition key is not part of the selection criteria,
then the routing server will forward the request to every
shard which will perform its local search, and the results
will be gathered at the routing server and return to the
client.
<!--l. 1372--><p class="indent" >   As chunks grow beyond the specified chunk size a mongos
instance will attempt to split the chunk in half. Splits may lead
to an uneven distribution of the chunks for a collection across
the shards. In such cases, the mongos instances will initiate a
round of migrations to redistribute chunks evenly across
shards.
   <h4 class="subsectionHead"><span class="titlemark">6.6.6   </span> <a 
 id="x1-960006.6.6"></a>Summary</h4>
                                                          
                                                          
<!--l. 1375--><p class="noindent" >MongoDB is an agile database that allows schemas to change
quickly as applications evolve. The JSON-like documents make
the integration of data in some applications easier and faster.
With careful setup, MongoDB clusters can also provide
scalability and high availability.
                                                          
                                                          
<!--l. 1--><p class="indent" >
                                                          
                                                          
   <h2 class="likechapterHead"><a 
 id="x1-10800013"></a>Index</h2>
   <div class="theindex"><span class="index-item">3V, 1 <br /></span>
<p class="theindex">
<span class="index-item">Accumulo, 22, 83 <br /></span>
<span class="index-item">ACID, 76, 92 <br /></span>
<span class="index-item">ACL, 79 <br /></span>
</p><p class="theindex">
<span class="index-item">Big Data, 1 <br /></span>
<span class="index-item">BigTable, 83 <br /></span>
<span class="index-subitem">&#x00A0;&#x00A0;&#x00A0;&#x00A0;SSTable, 86 <br /></span>
</p><p class="theindex">
<span class="index-item">CAP theorem, 76 <br /></span>
<span class="index-item">Cassandra, 107 <br /></span>
<span class="index-subitem">&#x00A0;&#x00A0;&#x00A0;&#x00A0;Collection, 111 <br /></span>
<span class="index-subitem">&#x00A0;&#x00A0;&#x00A0;&#x00A0;CQL, 110 <br /></span>
<span class="index-subitem">&#x00A0;&#x00A0;&#x00A0;&#x00A0;Lightweight transaction, 111 <br /></span>
<span class="index-subitem">&#x00A0;&#x00A0;&#x00A0;&#x00A0;Partitioner, 109 <br /></span>
<span class="index-subitem">&#x00A0;&#x00A0;&#x00A0;&#x00A0;Replication, 110 <br /></span>
<span class="index-item">Chukwa, 28 <br /></span>
<span class="index-item">CRDT, 97 <br /></span>
<span class="index-item">CRM, 4 <br /></span>
                                                          
                                                          
</p><p class="theindex">
<span class="index-item">Data <br /></span>
<span class="index-subitem">&#x00A0;&#x00A0;&#x00A0;&#x00A0;Data governance, 12 <br /></span>
<span class="index-subitem">&#x00A0;&#x00A0;&#x00A0;&#x00A0;Data management, 11 <br /></span>
<span class="index-subitem">&#x00A0;&#x00A0;&#x00A0;&#x00A0;Data strategy, 12 <br /></span>
<span class="index-item">Deletes mask Puts, 85 <br /></span>
<span class="index-item">Dotted version vector, 106 <br /></span>
<span class="index-item">Dryad, 44 <br /></span>
<span class="index-item">Dynamo, 96 <br /></span>
</p><p class="theindex">
<span class="index-item">Flume, 27 <br /></span>
</p><p class="theindex">
<span class="index-item">GFS, 16 <br /></span>
</p><p class="theindex">
<span class="index-item">Hadoop, 15 <br /></span>
<span class="index-item">HBase, 22, 83 <br /></span>
<span class="index-subitem">&#x00A0;&#x00A0;&#x00A0;&#x00A0;Coprocessor, 95 <br /></span>
<span class="index-subitem">&#x00A0;&#x00A0;&#x00A0;&#x00A0;HBaseAdmin, 89 <br /></span>
<span class="index-subitem">&#x00A0;&#x00A0;&#x00A0;&#x00A0;HFile, 85 <br /></span>
<span class="index-subitem">&#x00A0;&#x00A0;&#x00A0;&#x00A0;HMaster, 89 <br /></span>
<span class="index-subitem">&#x00A0;&#x00A0;&#x00A0;&#x00A0;HTable, 89 <br /></span>
<span class="index-subitem">&#x00A0;&#x00A0;&#x00A0;&#x00A0;MemStore, 85 <br /></span>
<span class="index-subitem">&#x00A0;&#x00A0;&#x00A0;&#x00A0;Region, 88 <br /></span>
<span class="index-subitem">&#x00A0;&#x00A0;&#x00A0;&#x00A0;RegionServer, 88 <br /></span>
<span class="index-subitem">&#x00A0;&#x00A0;&#x00A0;&#x00A0;StoreFile, 85 <br /></span>
<span class="index-item">HCM, 5 <br /></span>
<span class="index-item">HDFS, 16 <br /></span>
                                                          
                                                          
<span class="index-subitem">&#x00A0;&#x00A0;&#x00A0;&#x00A0;DataNode, 18 <br /></span>
<span class="index-subitem">&#x00A0;&#x00A0;&#x00A0;&#x00A0;EditLog, 19 <br /></span>
<span class="index-subitem">&#x00A0;&#x00A0;&#x00A0;&#x00A0;File System Shell, 27 <br /></span>
<span class="index-subitem">&#x00A0;&#x00A0;&#x00A0;&#x00A0;FsImage, 19 <br /></span>
<span class="index-subitem">&#x00A0;&#x00A0;&#x00A0;&#x00A0;HAR file, 21 <br /></span>
<span class="index-subitem">&#x00A0;&#x00A0;&#x00A0;&#x00A0;HDFS federation, 22 <br /></span>
<span class="index-subitem">&#x00A0;&#x00A0;&#x00A0;&#x00A0;MapFile, 85 <br /></span>
<span class="index-subitem">&#x00A0;&#x00A0;&#x00A0;&#x00A0;NameNode, 18 <br /></span>
<span class="index-subitem">&#x00A0;&#x00A0;&#x00A0;&#x00A0;Safemode, 20 <br /></span>
<span class="index-subitem">&#x00A0;&#x00A0;&#x00A0;&#x00A0;SequenceFile, 21 <br /></span>
<span class="index-item">Hive, 45 <br /></span>
</p><p class="theindex">
<span class="index-item">Internet of things, 1 <br /></span>
<span class="index-subitem">&#x00A0;&#x00A0;&#x00A0;&#x00A0;IoT, 1 <br /></span>
<span class="index-item">IoT, 7 <br /></span>
</p><p class="theindex">
<span class="index-item">Kafka, 28 <br /></span>
<span class="index-item">Kerberos, 93 <br /></span>
</p><p class="theindex">
<span class="index-item">Last-write-wins, 104 <br /></span>
<span class="index-item">LevelDB, 99 <br /></span>
<span class="index-item">LSM tree, 87 <br /></span>
</p><p class="theindex">
<span class="index-item">MapReduce, 29 <br /></span>
<span class="index-subitem">&#x00A0;&#x00A0;&#x00A0;&#x00A0;Combiner, 32 <br /></span>
<span class="index-subitem">&#x00A0;&#x00A0;&#x00A0;&#x00A0;DistributedCache, 40 <br /></span>
<span class="index-subitem">&#x00A0;&#x00A0;&#x00A0;&#x00A0;Map, 30 <br /></span>
                                                          
                                                          
<span class="index-subitem">&#x00A0;&#x00A0;&#x00A0;&#x00A0;Partitioner, 31 <br /></span>
<span class="index-subitem">&#x00A0;&#x00A0;&#x00A0;&#x00A0;Reduce, 30 <br /></span>
<span class="index-subitem">&#x00A0;&#x00A0;&#x00A0;&#x00A0;Shuffle, 31 <br /></span>
<span class="index-item">Mesos, 123 <br /></span>
<span class="index-item">MongoDB, 112 <br /></span>
<span class="index-subitem">&#x00A0;&#x00A0;&#x00A0;&#x00A0;BSON, 113 <br /></span>
<span class="index-subitem">&#x00A0;&#x00A0;&#x00A0;&#x00A0;MMAPv1, 113 <br /></span>
<span class="index-subitem">&#x00A0;&#x00A0;&#x00A0;&#x00A0;WiredTiger, 115 <br /></span>
<span class="index-item">MPI, 29 <br /></span>
<span class="index-item">MTTR, 90 <br /></span>
</p><p class="theindex">
<span class="index-item">Omega, 123 <br /></span>
</p><p class="theindex">
<span class="index-item">Paxos, 82 <br /></span>
</p><p class="theindex">
<span class="index-item">RDD, 51 <br /></span>
<span class="index-item">Riak, 96 <br /></span>
<span class="index-subitem">&#x00A0;&#x00A0;&#x00A0;&#x00A0;Bitcask, 98 <br /></span>
<span class="index-subitem">&#x00A0;&#x00A0;&#x00A0;&#x00A0;Causal context, 105 <br /></span>
<span class="index-subitem">&#x00A0;&#x00A0;&#x00A0;&#x00A0;Hinted Handoff, 104 <br /></span>
<span class="index-subitem">&#x00A0;&#x00A0;&#x00A0;&#x00A0;Memory, 101 <br /></span>
<span class="index-subitem">&#x00A0;&#x00A0;&#x00A0;&#x00A0;Sloppy quorum, 103 <br /></span>
</p><p class="theindex">
<span class="index-item">SASL, 93 <br /></span>
<span class="index-item">Spark, 51 <br /></span>
<span class="index-subitem">&#x00A0;&#x00A0;&#x00A0;&#x00A0;Accumulator, 53 <br /></span>
<span class="index-subitem">&#x00A0;&#x00A0;&#x00A0;&#x00A0;Broadcast variable, 53 <br /></span>
                                                          
                                                          
<span class="index-subitem">&#x00A0;&#x00A0;&#x00A0;&#x00A0;RDD, 51 <br /></span>
<span class="index-subitem">&#x00A0;&#x00A0;&#x00A0;&#x00A0;Reliant distributed dataset, 51 <br /></span>
<span class="index-item">SPOF, 50 <br /></span>
<span class="index-item">Sqoop, 28 <br /></span>
<span class="index-item">Stinger, 45 <br /></span>
<span class="index-item">Storm, 28 <br /></span>
</p><p class="theindex">
<span class="index-item">Tez, 44 <br /></span>
<span class="index-item">Thrift, 23 <br /></span>
<span class="index-item">Tombstone, 85 <br /></span>
</p><p class="theindex">
<span class="index-item">Vector clock, 105 <br /></span>
</p><p class="theindex">
<span class="index-item">Write ahead log, 86, 90 <br /></span>
</p><p class="theindex">
<span class="index-item">YARN, 47 <br /></span>
<span class="index-subitem">&#x00A0;&#x00A0;&#x00A0;&#x00A0;Application manager, 48 <br /></span>
<span class="index-subitem">&#x00A0;&#x00A0;&#x00A0;&#x00A0;Node manager, 48 <br /></span>
<span class="index-subitem">&#x00A0;&#x00A0;&#x00A0;&#x00A0;Queue, 49 <br /></span>
<span class="index-subitem">&#x00A0;&#x00A0;&#x00A0;&#x00A0;Resource container, 48 <br /></span>
<span class="index-subitem">&#x00A0;&#x00A0;&#x00A0;&#x00A0;Resource manager, 48 <br /></span>
<span class="index-subitem">&#x00A0;&#x00A0;&#x00A0;&#x00A0;Scheduler, 48 <br /></span>
<span class="index-subsubitem">&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;Capacity scheduler, 49 <br /></span>
<span class="index-subsubitem">&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;Fair scheduler, 49 <br /></span>
</p><p class="theindex">
<span class="index-item">Zab, 82 <br /></span>
                                                          
                                                          
<span class="index-item">ZooKeeper, 50, 78 <br /></span>
</p></div>
                                                          
                                                          
   <h2 class="likechapterHead"><a 
 id="x1-10900013"></a>Bibliography</h2>
     <div class="thebibliography">
     <p class="bibitem" ><span class="biblabel">
  [1]<span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span><a 
 id="XAccenture13Seattle"></a>Accenture. Accenture analytics and smart building
     solutions  are  helping  seattle  boost  energy  efficiency
     downtown, 2013.
     </p>
     <p class="bibitem" ><span class="biblabel">
  [2]<span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span><a 
 id="XAccenture14SmartGrid"></a>Accenture.  Accenture to help thames water prove
     the benefits of smart monitoring capabilities, 2014.
     </p>
     <p class="bibitem" ><span class="biblabel">
  [3]<span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span><a 
 id="XAdpHcm"></a>ADP.     Adp  hcm  solutions  for  large  business.
     <a 
href="http://www.adp.com/solutions/large-business/products.aspx" class="url" ><span 
class="pcrr7t-x-x-120">http://www.adp.com/solutions/large-business/products.aspx</span></a>,
     2014.
     </p>
     <p class="bibitem" ><span class="biblabel">
  [4]<span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span><a 
 id="XAMPLabBenchmark2014"></a>AMPLab.                Big      data      benchmark.
     <a 
href="http://amplab.cs.berkeley.edu/benchmark/" class="url" ><span 
class="pcrr7t-x-x-120">http://amplab.cs.berkeley.edu/benchmark/</span></a>.
     </p>
                                                          
                                                          
     <p class="bibitem" ><span class="biblabel">
  [5]<span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span><a 
 id="XAccumulo"></a>Apache.                                          Accumulo.
     <a 
href="http://accumulo.apache.org" class="url" ><span 
class="pcrr7t-x-x-120">http://accumulo.apache.org</span></a>.
     </p>
     <p class="bibitem" ><span class="biblabel">
  [6]<span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span><a 
 id="XCassandra"></a>Apache.                                          Cassandra.
     <a 
href="http://cassandra.apache.org" class="url" ><span 
class="pcrr7t-x-x-120">http://cassandra.apache.org</span></a>.
     </p>
     <p class="bibitem" ><span class="biblabel">
  [7]<span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span><a 
 id="XChukwa"></a>Apache.                                             Chukwa.
     <a 
href="http://chukwa.apache.org" class="url" ><span 
class="pcrr7t-x-x-120">http://chukwa.apache.org</span></a>.
     </p>
     <p class="bibitem" ><span class="biblabel">
  [8]<span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span><a 
 id="XHdfsShell"></a>Apache.             File     system     shell     guide.
     <a 
href="http://hadoop.apache.org/docs/current/hadoop-project-dist/hadoop-common/FileSystemShell.html" class="url" ><span 
class="pcrr7t-x-x-120">http://hadoop.apache.org/docs/current/hadoop-project-dist/hadoop-common/FileSystemShell.html</span></a>.
     </p>
     <p class="bibitem" ><span class="biblabel">
  [9]<span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span><a 
 id="XFlume"></a>Apache. Flume. <a 
href="http://flume.apache.org" class="url" ><span 
class="pcrr7t-x-x-120">http://flume.apache.org</span></a>.
     </p>
     <p class="bibitem" ><span class="biblabel">
 [10]<span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span><a 
 id="XHadoop"></a>Apache.                                             Hadoop.
     <a 
href="http://hadoop.apache.org" class="url" ><span 
class="pcrr7t-x-x-120">http://hadoop.apache.org</span></a>.
     </p>
     <p class="bibitem" ><span class="biblabel">
 [11]<span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span><a 
 id="XHBase"></a>Apache. Hbase. <a 
href="http://hbase.apache.org" class="url" ><span 
class="pcrr7t-x-x-120">http://hbase.apache.org</span></a>.
                                                          
                                                          
     </p>
     <p class="bibitem" ><span class="biblabel">
 [12]<span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span><a 
 id="XHdfsThrift"></a>Apache. HDFS APIs in perl, python, ruby and php.
     <a 
href="http://wiki.apache.org/hadoop/HDFS-APIs" class="url" ><span 
class="pcrr7t-x-x-120">http://wiki.apache.org/hadoop/HDFS-APIs</span></a>.
     </p>
     <p class="bibitem" ><span class="biblabel">
 [13]<span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span><a 
 id="XKafka"></a>Apache. Kafka. <a 
href="http://kafka.apache.org" class="url" ><span 
class="pcrr7t-x-x-120">http://kafka.apache.org</span></a>.
     </p>
     <p class="bibitem" ><span class="biblabel">
 [14]<span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span><a 
 id="XMapReduceTutorial"></a>Apache.                       Mapreduce        tutorial.
     <a 
href="http://hadoop.apache.org/docs/current/hadoop-mapreduce-client/hadoop-mapreduce-client-core/MapReduceTutorial.html" class="url" ><span 
class="pcrr7t-x-x-120">http://hadoop.apache.org/docs/current/hadoop-mapreduce-client/hadoop-mapreduce-client-core/MapReduceTutorial.html</span></a>.
     </p>
     <p class="bibitem" ><span class="biblabel">
 [15]<span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span><a 
 id="XMaven"></a>Apache. Maven. <a 
href="http://maven.apache.org" class="url" ><span 
class="pcrr7t-x-x-120">http://maven.apache.org</span></a>.
     </p>
     <p class="bibitem" ><span class="biblabel">
 [16]<span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span><a 
 id="XSpark"></a>Apache. Spark. <a 
href="http://spark.apache.org" class="url" ><span 
class="pcrr7t-x-x-120">http://spark.apache.org</span></a>.
     </p>
     <p class="bibitem" ><span class="biblabel">
 [17]<span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span><a 
 id="XSqoop"></a>Apache. Sqoop. <a 
href="http://sqoop.apache.org" class="url" ><span 
class="pcrr7t-x-x-120">http://sqoop.apache.org</span></a>.
     </p>
     <p class="bibitem" ><span class="biblabel">
 [18]<span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span><a 
 id="XStorm"></a>Apache. Storm. <a 
href="http://storm.apache.org" class="url" ><span 
class="pcrr7t-x-x-120">http://storm.apache.org</span></a>.
     </p>
                                                          
                                                          
     <p class="bibitem" ><span class="biblabel">
 [19]<span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span><a 
 id="XTez"></a>Apache. Tez. <a 
href="http://tez.apache.org" class="url" ><span 
class="pcrr7t-x-x-120">http://tez.apache.org</span></a>.
     </p>
     <p class="bibitem" ><span class="biblabel">
 [20]<span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span><a 
 id="XThrift"></a>Apache. Thrift. <a 
href="http://thrift.apache.org" class="url" ><span 
class="pcrr7t-x-x-120">http://thrift.apache.org</span></a>.
     </p>
     <p class="bibitem" ><span class="biblabel">
 [21]<span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span><a 
 id="XZooKeeper"></a>Apache.                                          Zookeeper.
     <a 
href="http://zookeeper.apache.org" class="url" ><span 
class="pcrr7t-x-x-120">http://zookeeper.apache.org</span></a>.
     </p>
     <p class="bibitem" ><span class="biblabel">
 [22]<span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span><a 
 id="XRiak"></a>Basho. Riak. <a 
href="http://basho.com/riak/" class="url" ><span 
class="pcrr7t-x-x-120">http://basho.com/riak/</span></a>.
     </p>
     <p class="bibitem" ><span class="biblabel">
 [23]<span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span><a 
 id="Xopac:2009"></a>Philip&#x00A0;A. Bernstein and Eric Newcomer. <span 
class="cmti-12">Principles</span>
     <span 
class="cmti-12">of  transaction  processing</span>.    The  Morgan  Kaufmann
     series in data management systems. Morgan Kaufmann
     Publishers, Burlington, MA, second edition, 2009.
     </p>
     <p class="bibitem" ><span class="biblabel">
 [24]<span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span><a 
 id="XBloom:1970:STH"></a>Burton&#x00A0;H. Bloom.  Space/time trade-offs in hash
     coding   with   allowable   errors.      <span 
class="cmti-12">Commun.  ACM</span>,
     13(7):422&#8211;426, July 1970.
     </p>
                                                          
                                                          
     <p class="bibitem" ><span class="biblabel">
 [25]<span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span><a 
 id="XBrewer:2000:TRD"></a>Eric&#x00A0;A.  Brewer.      Towards  robust  distributed
     systems (abstract).  In <span 
class="cmti-12">Proceedings of the Nineteenth</span>
     <span 
class="cmti-12">Annual ACM Symposium on Principles of Distributed</span>
     <span 
class="cmti-12">Computing</span>, PODC &#8217;00, 2000.
     </p>
     <p class="bibitem" ><span class="biblabel">
 [26]<span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span><a 
 id="XBrewer:2012"></a>Eric&#x00A0;A.  Brewer.    Cap  twelve  years  later:  How
     the  &#8220;rules&#8221;  have  changed.    <span 
class="cmti-12">Computer</span>,  45(2):23&#8211;29,
     February 2012.
     </p>
     <p class="bibitem" ><span class="biblabel">
 [27]<span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span><a 
 id="XChang:2006:BDS"></a>Fay   Chang,   Jeffrey   Dean,   Sanjay   Ghemawat,
     Wilson&#x00A0;C. Hsieh, Deborah&#x00A0;A. Wallach, Mike Burrows,
     Tushar   Chandra,   Andrew   Fikes,   and   Robert&#x00A0;E.
     Gruber.     Bigtable:  A  distributed  storage  system
     for  structured  data.     In  <span 
class="cmti-12">Proceedings  of  the  7th</span>
     <span 
class="cmti-12">USENIX Symposium on Operating Systems Design and</span>
     <span 
class="cmti-12">Implementation - Volume 7</span>, OSDI &#8217;06, 2006.
     </p>
     <p class="bibitem" ><span class="biblabel">
 [28]<span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span><a 
 id="XClouderaImpala2014"></a>Cloudera.      New   sql   choices   in   the   apache
     hadoop  ecosystem:  Why  impala  continues  to  lead.
     <a 
href="http://blog.cloudera.com/blog/2014/05/new-sql-choices-in-the-apache-hadoop-ecosystem-why-impala-continues-to-lead/" class="url" ><span 
class="pcrr7t-x-x-120">http://blog.cloudera.com/blog/2014/05/new-sql-choices-in-the-apache-hadoop-ecosystem-why-impala-continues-to-lead/</span></a>.
     </p>
     <p class="bibitem" ><span class="biblabel">
 [29]<span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span><a 
 id="XLevelDB"></a>Jeffrey  Dean  and  Sanjay  Ghemawat.    Leveldb.
     <a 
href="http://leveldb.org" class="url" ><span 
class="pcrr7t-x-x-120">http://leveldb.org</span></a>.
                                                          
                                                          
     </p>
     <p class="bibitem" ><span class="biblabel">
 [30]<span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span><a 
 id="XDean:2008:MSD"></a>Jeffrey Dean and Sanjay Ghemawat.  Mapreduce:
     Simplified data processing on large clusters. <span 
class="cmti-12">Commun.</span>
     <span 
class="cmti-12">ACM</span>, 51(1):107&#8211;113, January 2008.
     </p>
     <p class="bibitem" ><span class="biblabel">
 [31]<span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span><a 
 id="XDeCandia:2007:DAH"></a>Giuseppe                 DeCandia,                 Deniz
     Hastorun, Madan Jampani, Gunavardhan Kakulapati,
     Avinash   Lakshman,   Alex   Pilchin,   Swaminathan
     Sivasubramanian, Peter Vosshall, and Werner Vogels.
     Dynamo: Amazon&#8217;s highly available key-value store. In
     <span 
class="cmti-12">Proceedings of Twenty-first ACM SIGOPS Symposium</span>
     <span 
class="cmti-12">on  Operating  Systems  Principles</span>,  SOSP  &#8217;07,  pages
     205&#8211;220, 2007.
     </p>
     <p class="bibitem" ><span class="biblabel">
 [32]<span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span><a 
 id="XDeWitt:1992:PDS"></a>David  DeWitt  and  Jim  Gray.   Parallel  database
     systems:  The  future  of  high  performance  database
     systems. <span 
class="cmti-12">Commun. ACM</span>, 35(6):85&#8211;98, June 1992.
     </p>
     <p class="bibitem" ><span class="biblabel">
 [33]<span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span><a 
 id="Xfidge1988timestamps"></a>C.&#x00A0;J.  Fidge.     Timestamps  in  message-passing
     systems that preserve the partial ordering. <span 
class="cmti-12">Proceedings</span>
     <span 
class="cmti-12">of the 11th Australian Computer Science Conference</span>,
     10(1):5666, 1988.
     </p>
                                                          
                                                          
     <p class="bibitem" ><span class="biblabel">
 [34]<span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span><a 
 id="XForum:1994:MMI"></a>Message&#x00A0;P   Forum.      Mpi:   A   message-passing
     interface  standard.    Technical  report,  University  of
     Tennessee, Knoxville, TN, USA, 1994.
     </p>
     <p class="bibitem" ><span class="biblabel">
 [35]<span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span><a 
 id="XGartner2014"></a>Gartner.  Gartner&#8217;s 2014 hype cycle for emerging
     technologies  maps  the  journey  to  digital  business.
     <a 
href="http://www.gartner.com/newsroom/id/2819918" class="url" ><span 
class="pcrr7t-x-x-120">http://www.gartner.com/newsroom/id/2819918</span></a>,
     2014.
     </p>
     <p class="bibitem" ><span class="biblabel">
 [36]<span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span><a 
 id="XStinger"></a>Alan     Gates.            The     stinger     initiative:
     Making      apache      hive      100      times      faster.
     <a 
href="http://hortonworks.com/blog/100x-faster-hive/" class="url" ><span 
class="pcrr7t-x-x-120">http://hortonworks.com/blog/100x-faster-hive/</span></a>.
     </p>
     <p class="bibitem" ><span class="biblabel">
 [37]<span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span><a 
 id="XIndustrialInternetReport2014"></a>GE  and  Accenture.   Industrial  internet  insights
     report, 2014.
     </p>
     <p class="bibitem" ><span class="biblabel">
 [38]<span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span><a 
 id="XGhemawat:2003:GFS"></a>Sanjay Ghemawat, Howard Gobioff, and Shun-Tak
     Leung.   The  google  file  system.   In  <span 
class="cmti-12">Proceedings  of</span>
     <span 
class="cmti-12">the Nineteenth ACM Symposium on Operating Systems</span>
     <span 
class="cmti-12">Principles</span>, SOSP &#8217;03, pages 29&#8211;43, 2003.
     </p>
                                                          
                                                          
     <p class="bibitem" ><span class="biblabel">
 [39]<span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span>
     <a 
 id="XGhodsi:2011:DRF"></a>Ali Ghodsi, Matei Zaharia, Benjamin Hindman, Andy
     Konwinski, Scott Shenker, and Ion Stoica.  Dominant
     resource fairness: Fair allocation of multiple resource
     types.  In <span 
class="cmti-12">Proceedings of the 8th USENIX Conference</span>
     <span 
class="cmti-12">on  Networked  Systems  Design  and  Implementation</span>,
     NSDI&#8217;11, pages 323&#8211;336, 2011.
     </p>
     <p class="bibitem" ><span class="biblabel">
 [40]<span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span><a 
 id="XGilbert:2002:BCF"></a>Seth   Gilbert   and   Nancy   Lynch.       Brewer&#8217;s
     conjecture and the feasibility of consistent, available,
     partition-tolerant   web   services.      <span 
class="cmti-12">SIGACT  News</span>,
     33(2):51&#8211;59, June 2002.
     </p>
     <p class="bibitem" ><span class="biblabel">
 [41]<span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span><a 
 id="XGropp:1999:UMA"></a>William Gropp, Ewing Lusk, and Rajeev Thakur.
     <span 
class="cmti-12">Using                    MPI-2:                    Advanced</span>
     <span 
class="cmti-12">Features of the Message-Passing Interface</span>. MIT Press,
     Cambridge, MA, USA, 1999.
     </p>
     <p class="bibitem" ><span class="biblabel">
 [42]<span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span><a 
 id="XHDFS2010:265"></a>Apache Hadoop. Asf jira hdfs-265, 2010.
     </p>
     <p class="bibitem" ><span class="biblabel">
 [43]<span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span><a 
 id="XYARN2011:279"></a>Apache Hadoop. Asf jira mapreduce-279, 2011.
     </p>
                                                          
                                                          
     <p class="bibitem" ><span class="biblabel">
 [44]<span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span><a 
 id="XHDFS"></a>Apache     Hadoop.             Hdfs     architecture.
     <a 
href="http://hadoop.apache.org/docs/current/hadoop-project-dist/hadoop-hdfs/HdfsDesign.html" class="url" ><span 
class="pcrr7t-x-x-120">http://hadoop.apache.org/docs/current/hadoop-project-dist/hadoop-hdfs/HdfsDesign.html</span></a>,
     2014.
     </p>
     <p class="bibitem" ><span class="biblabel">
 [45]<span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span><a 
 id="XHindman:2011:MPF"></a>Benjamin   Hindman,   Andy   Konwinski,   Matei
     Zaharia,  Ali  Ghodsi,  Anthony&#x00A0;D.  Joseph,  Randy
     Katz,  Scott  Shenker,  and  Ion  Stoica.     Mesos:  A
     platform for fine-grained resource sharing in the data
     center.  In <span 
class="cmti-12">Proceedings of the 8th USENIX Conference</span>
     <span 
class="cmti-12">on  Networked  Systems  Design  and  Implementation</span>,
     NSDI&#8217;11, pages 295&#8211;308, 2011.
     </p>
     <p class="bibitem" ><span class="biblabel">
 [46]<span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span><a 
 id="XWatson2013Cancer"></a>IBM.      Ibm   watson   helps   fight   cancer   with
     evidence-based  diagnosis  and  treatment  suggestions,
     2013.
     </p>
     <p class="bibitem" ><span class="biblabel">
 [47]<span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span><a 
 id="XWatson2013Healthcare"></a>IBM.      Putting   watson   to   work:   Watson   in
     healthcare, 2013.
     </p>
     <p class="bibitem" ><span class="biblabel">
 [48]<span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span><a 
 id="XIBM2013"></a>IBM.                   What       is       big       data?
     <a 
href="http://www-01.ibm.com/software/data/bigdata/what-is-big-data.html" class="url" ><span 
class="pcrr7t-x-x-120">http://www-01.ibm.com/software/data/bigdata/what-is-big-data.html</span></a>,
     2013.
                                                          
                                                          
     </p>
     <p class="bibitem" ><span class="biblabel">
 [49]<span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span><a 
 id="XWatson2014"></a>IBM. Watson. <a 
href="http://www.ibm.com/smarterplanet/us/en/ibmwatson/" class="url" ><span 
class="pcrr7t-x-x-120">http://www.ibm.com/smarterplanet/us/en/ibmwatson/</span></a>,
     2014.
     </p>
     <p class="bibitem" ><span class="biblabel">
 [50]<span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span><a 
 id="XIsard:2007:DDD"></a>Michael  Isard,  Mihai  Budiu,  Yuan  Yu,  Andrew
     Birrell,  and  Dennis  Fetterly.     Dryad:  Distributed
     data-parallel programs from sequential building blocks.
     In  <span 
class="cmti-12">Proceedings  of  the  2Nd  ACM  SIGOPS/EuroSys</span>
     <span 
class="cmti-12">European  Conference  on  Computer  Systems  2007</span>,
     EuroSys &#8217;07, pages 59&#8211;72, 2007.
     </p>
     <p class="bibitem" ><span class="biblabel">
 [51]<span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span><a 
 id="XJain:1988:ACD"></a>Anil&#x00A0;K. Jain and Richard&#x00A0;C. Dubes.  <span 
class="cmti-12">Algorithms</span>
     <span 
class="cmti-12">for Clustering Data</span>. Prentice-Hall, Inc., 1988.
     </p>
     <p class="bibitem" ><span class="biblabel">
 [52]<span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span><a 
 id="XHBaseCoprocessor"></a>Mingjie       Lai,       Eugene       Koontz,       and
     Andrew    Purtell.          Coprocessor    introduction.
     <a 
href="http://blogs.apache.org/hbase/entry/coprocessor_introduction" class="url" ><span 
class="pcrr7t-x-x-120">http://blogs.apache.org/hbase/entry/coprocessor_introduction</span></a>.
     </p>
     <p class="bibitem" ><span class="biblabel">
 [53]<span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span><a 
 id="XLakshman:2010:CDS"></a>Avinash    Lakshman    and    Prashant    Malik.
     Cassandra: A decentralized structured storage system.
     <span 
class="cmti-12">SIGOPS Oper. Syst. Rev.</span>, 44(2):35&#8211;40, April 2010.
                                                          
                                                          
     </p>
     <p class="bibitem" ><span class="biblabel">
 [54]<span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span><a 
 id="XLamport:1998:PP"></a>Leslie Lamport.  The part-time parliament.  <span 
class="cmti-12">ACM</span>
     <span 
class="cmti-12">Trans. Comput. Syst.</span>, 16(2):133&#8211;169, May 1998.
     </p>
     <p class="bibitem" ><span class="biblabel">
 [55]<span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span><a 
 id="XLaney2012"></a>Douglas Laney.  The importance of &#8216;big data&#8217;: A
     definition. <span 
class="cmti-12">Gartner</span>, June 2012.
     </p>
     <p class="bibitem" ><span class="biblabel">
 [56]<span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span><a 
 id="XHBaseMTTR"></a>Nicolas      Liochon.               Introduction      to
     hbase     mean     time     to     recovery     (MTTR).
     <a 
href="http://hortonworks.com/blog/introduction-to-hbase-mean-time-to-recover-mttr/" class="url" ><span 
class="pcrr7t-x-x-120">http://hortonworks.com/blog/introduction-to-hbase-mean-time-to-recover-mttr/</span></a>.
     </p>
     <p class="bibitem" ><span class="biblabel">
 [57]<span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span><a 
 id="XMacLeodClarke2012"></a>David  MacLeod  and  Nita  Clarke.     Engaging
     for success: enhancing performance through employee
     engagement, 2012.
     </p>
     <p class="bibitem" ><span class="biblabel">
 [58]<span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span><a 
 id="XMattern89virtualtime"></a>Friedemann Mattern. Virtual time and global states
     of  distributed  systems.   In  <span 
class="cmti-12">Parallel  and  Distributed</span>
     <span 
class="cmti-12">Algorithms</span>, pages 215&#8211;226, 1989.
     </p>
     <p class="bibitem" ><span class="biblabel">
 [59]<span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span><a 
 id="XMcKusick:2009:GEF"></a>Marshall&#x00A0;Kirk McKusick and Sean Quinlan.  Gfs:
                                                          
                                                          
     Evolution on fast-forward.  <span 
class="cmti-12">Queue</span>, 7(7):10&#8211;20, August
     2009.
     </p>
     <p class="bibitem" ><span class="biblabel">
 [60]<span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span><a 
 id="XBSON"></a>MongoDB. Bson. <a 
href="http://bsonspec.org" class="url" ><span 
class="pcrr7t-x-x-120">http://bsonspec.org</span></a>.
     </p>
     <p class="bibitem" ><span class="biblabel">
 [61]<span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span><a 
 id="XMongoDB"></a>MongoDB.                                       Mongodb.
     <a 
href="https://www.mongodb.com" class="url" ><span 
class="pcrr7t-x-x-120">https://www.mongodb.com</span></a>.
     </p>
     <p class="bibitem" ><span class="biblabel">
 [62]<span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span><a 
 id="XWiredTiger"></a>MongoDB.                                      Wiredtiger.
     <a 
href="http://www.wiredtiger.com" class="url" ><span 
class="pcrr7t-x-x-120">http://www.wiredtiger.com</span></a>.
     </p>
     <p class="bibitem" ><span class="biblabel">
 [63]<span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span><a 
 id="XNunesKambil2001"></a>Paul&#x00A0;F. Nunes and Ajit Kambil.  Personalization?
     No thanks. <span 
class="cmti-12">Harvard Business Review</span>, April 2001.
     </p>
     <p class="bibitem" ><span class="biblabel">
 [64]<span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span><a 
 id="XO'Neil96thelog-structured"></a>Patrick  O&#8217;Neil,  Edward  Cheng,  Dieter  Gawlick,
     and Elizabeth O&#8217;Neil.  The log-structured merge-tree
     (lsm-tree), 1996.
     </p>
     <p class="bibitem" ><span class="biblabel">
 [65]<span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span><a 
 id="XPavlo:2009:CAL"></a>Andrew  Pavlo,  Erik  Paulson,  Alexander  Rasin,
                                                          
                                                          
     Daniel&#x00A0;J. Abadi, David&#x00A0;J. DeWitt, Samuel Madden,
     and Michael Stonebraker. A comparison of approaches
     to  large-scale  data  analysis.    In  <span 
class="cmti-12">Proceedings  of  the</span>
     <span 
class="cmti-12">2009  ACM  SIGMOD  International  Conference  on</span>
     <span 
class="cmti-12">Management of Data</span>,  SIGMOD  &#8217;09,  pages  165&#8211;178,
     2009.
     </p>
     <p class="bibitem" ><span class="biblabel">
 [66]<span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span><a 
 id="XPreguica:2012:BAE"></a>Nuno  Preguica,  Carlos  Bauqero,  Paulo Sergio
     Almeida,   Victor   Fonte,   and   Ricardo   Goncalves.
     Brief  announcement:  Efficient  causality  tracking  in
     distributed   storage   systems   with   dotted   version
     vectors. In <span 
class="cmti-12">Proceedings of the 2012 ACM Symposium on</span>
     <span 
class="cmti-12">Principles of Distributed Computing</span>, PODC &#8217;12, pages
     335&#8211;336, 2012.
     </p>
     <p class="bibitem" ><span class="biblabel">
 [67]<span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span><a 
 id="XReed:2008:STO"></a>Benjamin Reed and Flavio&#x00A0;P. Junqueira. A simple
     totally ordered broadcast protocol.  In <span 
class="cmti-12">Proceedings of</span>
     <span 
class="cmti-12">the 2Nd Workshop on Large-Scale Distributed Systems</span>
     <span 
class="cmti-12">and Middleware</span>, LADIS &#8217;08, 2008.
     </p>
     <p class="bibitem" ><span class="biblabel">
 [68]<span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span><a 
 id="XReichheldSasser1990"></a>Frederick&#x00A0;F.  Reichheld  and  Jr.  W.&#x00A0;Earl&#x00A0;Sasser.
     Zero defections: Quality comes to services.   <span 
class="cmti-12">Harvard</span>
     <span 
class="cmti-12">Business</span>, September 1990.
     </p>
                                                          
                                                          
     <p class="bibitem" ><span class="biblabel">
 [69]<span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span><a 
 id="XRowstron:2012:NEG"></a>Antony Rowstron, Dushyanth Narayanan, Austin
     Donnelly, Greg O&#8217;Shea, and Andrew Douglas. Nobody
     ever  got  fired  for  using  hadoop  on  a  cluster.    In
     <span 
class="cmti-12">Proceedings of the 1st International Workshop on Hot</span>
     <span 
class="cmti-12">Topics in Cloud Data Processing</span>, HotCDP &#8217;12, pages
     1&#8211;5, 2012.
     </p>
     <p class="bibitem" ><span class="biblabel">
 [70]<span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span><a 
 id="XTezTutorial"></a>Bikas     Saha.            Apache     tez:     A     new
     chapter       in       hadoop       data       processing.
     <a 
href="http://hortonworks.com/blog/apache-tez-a-new-chapter-in-hadoop-data-processing/" class="url" ><span 
class="pcrr7t-x-x-120">http://hortonworks.com/blog/apache-tez-a-new-chapter-in-hadoop-data-processing/</span></a>.
     </p>
     <p class="bibitem" ><span class="biblabel">
 [71]<span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span><a 
 id="XSchwarzkopf:2013:OFS"></a>Malte   Schwarzkopf,   Andy   Konwinski,   Michael
     Abd-El-Malek,  and  John  Wilkes.   Omega:  Flexible,
     scalable  schedulers  for  large  compute  clusters.    In
     <span 
class="cmti-12">Proceedings of the 8th ACM European Conference on</span>
     <span 
class="cmti-12">Computer Systems</span>, EuroSys &#8217;13, pages 351&#8211;364, 2013.
     </p>
     <p class="bibitem" ><span class="biblabel">
 [72]<span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span><a 
 id="XCRDT2011"></a>Marc Shapiro, Nuno Preguia, Carlos Baquero, and
     Marek Zawirski. A comprehensive study of convergent
     and  commutative  replicated  data  types.    Technical
     Report RR-7506, INRIA, 2011.
     </p>
     <p class="bibitem" ><span class="biblabel">
 [73]<span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span><a 
 id="XBitcask"></a>Justin  Sheehy  and  David  Smith.     Bitcask:  A
                                                          
                                                          
     log-structured  hash  table  for  fast  key/value  data.
     <a 
href="http://github.com/basho/basho_docs/raw/master/source/data/bitcask-intro.pdf" class="url" ><span 
class="pcrr7t-x-x-120">http://github.com/basho/basho_docs/raw/master/source/data/bitcask-intro.pdf</span></a>.
     </p>
     <p class="bibitem" ><span class="biblabel">
 [74]<span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span><a 
 id="XTar2Seq"></a>Stuart    Sierra.          A    million    little    files.
     <a 
href="http://stuartsierra.com/2008/04/24/a-million-little-files" class="url" ><span 
class="pcrr7t-x-x-120">http://stuartsierra.com/2008/04/24/a-million-little-files</span></a>,
     2008.
     </p>
     <p class="bibitem" ><span class="biblabel">
 [75]<span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span><a 
 id="XTachyon"></a>Tachyon      Team.               Tachyon      project.
     <a 
href="http://tachyon-project.org" class="url" ><span 
class="pcrr7t-x-x-120">http://tachyon-project.org</span></a>.
     </p>
     <p class="bibitem" ><span class="biblabel">
 [76]<span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span><a 
 id="XTop500"></a>Top500.org.             Numerical     wind     tunnel:
     National      aerospace      laboratory      of      japan.
     <a 
href="http://www.top500.org/featured/systems/numerical-wind-tunnel-national-aerospace-laboratory-of-japan/" class="url" ><span 
class="pcrr7t-x-x-120">http://www.top500.org/featured/systems/numerical-wind-tunnel-national-aerospace-laboratory-of-japan/</span></a>,
     2014.
     </p>
     <p class="bibitem" ><span class="biblabel">
 [77]<span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span><a 
 id="XFacebook13Mouse"></a>Lisa Vaas.  Facebook mulls silently tracking users&#8217;
     cursor movements to see which ads we like best, 2013.
     </p>
     <p class="bibitem" ><span class="biblabel">
 [78]<span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span><a 
 id="XVagateWilfong2014"></a>Pamela  Vagata  and  Kevin  Wilfong.     Scaling
     the    facebook    data    warehouse    to    300    PB.
     <a 
href="https://code.facebook.com/posts/229861827208629/scaling-the-facebook-data-warehouse-to-300-pb/" class="url" ><span 
class="pcrr7t-x-x-120">https://code.facebook.com/posts/229861827208629/scaling-the-facebook-data-warehouse-to-300-pb/</span></a>,
     April 2014.
                                                          
                                                          
     </p>
     <p class="bibitem" ><span class="biblabel">
 [79]<span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span><a 
 id="XSmallFiles"></a>Tom    White.         The    small    files    problem.
     <a 
href="http://blog.cloudera.com/blog/2009/02/the-small-files-problem/" class="url" ><span 
class="pcrr7t-x-x-120">http://blog.cloudera.com/blog/2009/02/the-small-files-problem/</span></a>,
     2009.
     </p>
     <p class="bibitem" ><span class="biblabel">
 [80]<span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span><a 
 id="XNvidia2014"></a>Wikipedia.                           Nvidia          tesla.
     <a 
href="http://en.wikipedia.org/wiki/Nvidia_Tesla" class="url" ><span 
class="pcrr7t-x-x-120">http://en.wikipedia.org/wiki/Nvidia_Tesla</span></a>,
     2014.
     </p>
     <p class="bibitem" ><span class="biblabel">
 [81]<span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span>
     <a 
 id="XZaharia:2012:RDD"></a>Matei Zaharia, Mosharaf Chowdhury, Tathagata Das,
     Ankur Dave, Justin Ma, Murphy McCauley, Michael&#x00A0;J.
     Franklin,  Scott  Shenker,  and  Ion  Stoica.    Resilient
     distributed  datasets:  A  fault-tolerant  abstraction  for
     in-memory cluster computing.   In <span 
class="cmti-12">Proceedings of the</span>
     <span 
class="cmti-12">9th USENIX Conference on Networked Systems Design</span>
     <span 
class="cmti-12">and Implementation</span>, NSDI&#8217;12, 2012.
     </p>
     <p class="bibitem" ><span class="biblabel">
 [82]<span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span><a 
 id="XZaharia:2010:SCC"></a>Matei  Zaharia,  Mosharaf  Chowdhury,  Michael&#x00A0;J.
     Franklin, Scott Shenker, and Ion Stoica. Spark: Cluster
     computing with working sets.   In <span 
class="cmti-12">Proceedings of the</span>
     <span 
class="cmti-12">2Nd  USENIX  Conference  on  Hot  Topics  in  Cloud</span>
     <span 
class="cmti-12">Computing</span>, HotCloud&#8217;10, 2010.
</p>
                                                          
                                                          
     </div>
    
</body></html> 

                                                          


